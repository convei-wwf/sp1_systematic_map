---
title: "Examine TF-IDF on documents by topic"
author: "O'Hara"
format: 
  html:
    code-fold: true
    code-summary: "Show me the code"
execute:
  echo: true
  warning: false
  message: false
editor: source
---

```{r setup}
library(tidyverse)
library(here)
library(tidytext)

source(here('common_fxns.R'))
```

Here we will iterate topic-by-topic to identify, using the TF-IDF concept, words and bigrams that are particularly important for a small number of articles, as well as those that come up frequently across all articles in the topic.

* The TF-IDF analysis identifies words that are uncommon across the articles, but show up frequently in one or two articles.  This may show odd terms that indicate articles that came up in the search but are focused on something else entirely.
* A term frequency analysis can highlight terms that show up commonly across the corpus, perhaps revealing commonalities.  This will be useful when looking at "value" related terms and bigrams in particular.

## Load search results and LDA results

Load the articles and the results of the LDA on topics.  Examine the LDA results briefly to see if there are any troubling issues.

```{r load articles}
article_df <- load_articles()

topic_df <- read_csv(here('_output/lda_topic_k25.csv'))

top_prob_df <- topic_df %>%
  arrange(doc_id, -prob, topic) %>%
  group_by(doc_id) %>%
  filter(prob == max(prob))

ggplot(top_prob_df, aes(x = prob)) +
  geom_histogram() +
  theme_minimal() +
  labs(title = 'highest probability of single topic')
```

The histogram shows that for many articles, the top topic match (by highest probability) is still only around 50% match.  This means that many articles are indicated as sharing a couple of topics.

```{r}
top2_prob_df <- topic_df %>%
  mutate(topic = str_remove(topic, 'topic  ')) %>%
  group_by(doc_id) %>%
  slice_max(prob, n = 2) %>%
  filter(prob > 0) %>%
  summarize(topic = paste(sort(topic), collapse = '-'),
            prob = sum(prob))

ggplot(top2_prob_df, aes(x = prob)) +
  geom_histogram() +
  theme_minimal() +
  labs(title = 'highest probability of top 2 topics')

topic_pairs <- top2_prob_df %>%
  group_by(topic) %>%
  summarize(n = n())
```

Examine a correlation matrix to see if any topics are closely correlated...

```{r}
topic_mtx <- topic_df %>%
  pivot_wider(names_from = topic, values_from = prob) %>%
  column_to_rownames('doc_id') %>%
  as.matrix()

cor_mtx <- cor(topic_mtx)
ggcorrplot::ggcorrplot(cor_mtx)

cor_vec <- as.vector(cor_mtx) %>%
  .[. != 1]
summary(cor_vec)
```

The correlations all seem quite low.  This probably makes sense since, like a PCA or similar, the LDA is probably coming up with fairly orthogonal definitions of topics.

## Examine term frequency across abstracts

Across all abstracts in each topic, identify most commonly used words (whether keywords or search terms or otherwise), using a $tf-idf$ (term frequency x inverse doc frequency) approach, where 
$$tf_{\text{word}} = \frac{n_{\text{word}}} {n_{\text{all words}}}$$
and 
$$idf = \ln \left(\frac{n_{\text{abstracts}}}{n_{\text{abstracts containing term}}}\right)$$
or in other words,
$$idf = - \ln \left(\frac{n_{\text{abstracts containing terms}}}{n_{\text{abstracts}}}\right)$$

The product $tf \times idf$ indicates words that are relatively important to one abstract (based on frequency of the term within that abstract) within the overall collection of abstracts (based on how infrequently the term shows up in other abstracts).  Terms that show up in all abstracts (e.g., stop words) will have an $idf \rightarrow 0$ equal to or near 0, as the ratio $\rightarrow 1$.

```{r summarize term freq by word}
abstr_df <- article_df %>%
  filter(field == 'abstract') %>%
  select(doc_id, text) %>%
  distinct() %>%
  left_join(top_prob_df, by = c('doc_id'))

abstr_words <- abstr_df %>%
  unnest_tokens(input = text, output = word, token = 'words') %>%
  group_by(doc_id, topic, word) %>%
  summarize(n = n(), .groups = 'drop') %>%
  group_by(doc_id, topic) %>%
  mutate(term_freq = n / sum(n)) %>%
  ungroup()
```

<!-- ### Plot rank vs frequency -->

```{r}

abstr_freq_by_rank <- abstr_words %>%
  group_by(doc_id, topic) %>%
  arrange(desc(term_freq)) %>%
  mutate(rank = 1:n()) %>%
  ungroup() %>%
  anti_join(stop_words, by = c('word')) %>%
  filter(nchar(word) > 2)

rank_subset <- abstr_freq_by_rank %>% 
  filter(rank < 50)

freq_rank_lm <- lm(log10(term_freq) ~ log10(rank), data = rank_subset)
# Coefficients:
# (Intercept)  log10(rank)  
#     -1.1059      -0.7098

ggplot(abstr_freq_by_rank %>% sample_n(2000),
       aes(x = rank, y = term_freq, color = topic)) +
  geom_point(alpha = .3, size = .1, show.legend = FALSE) +
  geom_abline(intercept = freq_rank_lm$coefficients[1],
              slope = freq_rank_lm$coefficients[2],
              linetype = 'dashed') +
  scale_x_log10() +
  scale_y_log10()
```

Zipf's law: Frequency of a word is inversely proportional to rank... log-log plot shows a (basically) constant slope of `r round(freq_rank_lm$coefficients[2], 3)` for terms ranked less than 50.


### Calculate $idf$ and $tf-idf$

```{r calc tf idf for words}
abstr_idf <- abstr_words %>%
  group_by(topic) %>%
  mutate(n_abstr = n_distinct(doc_id)) %>%
  group_by(topic, word) %>%
  summarize(n_abstr_w_term = n_distinct(doc_id),
            n_abstr = first(n_abstr), .groups = 'drop') %>%
  mutate(idf = -log(n_abstr_w_term / n_abstr))

abstr_tf_idf <- abstr_words %>%
  left_join(abstr_idf, by = c('topic', 'word')) %>%
  mutate(tf_idf = term_freq * idf) %>%
  arrange(desc(tf_idf))

top_tf_idf_by_topic <- abstr_tf_idf %>%
  group_by(topic) %>%
  slice_max(tf_idf, n = 5) %>%
  mutate(across(.cols = c(term_freq, idf, tf_idf), .fns = ~round(.x, 4))) %>%
  select(doc_id, topic, word, term_freq, idf, tf_idf)

DT::datatable(top_tf_idf_by_topic)

```

```{r}
wordcloud_df <- abstr_idf %>%
  anti_join(stop_words, by = c('word')) %>%
  filter(nchar(word) > 2) %>%
  filter(!str_detect(word, '[0-9]')) %>%
  mutate(doc_freq = n_abstr_w_term / n_abstr) %>%
  group_by(topic) %>%
  slice_max(order_by = doc_freq, n = 20)

ggplot(wordcloud_df, aes(label = word, size = doc_freq)) +
  ggwordcloud::geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap( ~ topic)
  
```

## Examine "value" bigram frequency across abstracts

Similar to above, except here, let's break the abstracts into bigrams and focus on those that include one of the "value" search terms.

```{r summarize bigrams}
abstr_bigrams_all <- abstr_df %>%
  ### first divide sentences so bigrams don't continue over sentence breaks
  mutate(text = str_split(text, pattern = '[.;]')) %>%
  unnest(text) %>%
  ### now break into bigrams
  unnest_tokens(input = text, output = bigram, token = 'ngrams', n = 2) %>%
  group_by(doc_id, topic, bigram) %>%
  summarize(n = n(), .groups = 'drop') %>%
  group_by(doc_id, topic) %>%
  mutate(bigram_freq = n / sum(n)) %>%
  group_by(topic) %>%
  mutate(n_bigrams = n()) %>%
  ungroup()

value_terms <- 'value|valuation|benefit|utility'

value_bigrams <- abstr_bigrams_all %>%
  ### filter to keep those bigrams with an instance of a value term
  filter(str_detect(bigram, value_terms)) %>%
  ### separate out for dropping stop words
  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>%
  ### drop rows where the initial or final word is a stop word
  anti_join(stop_words, by = c('word1' = 'word')) %>%
  anti_join(stop_words, by = c('word2' = 'word')) %>%
  ### drop rows where any term is less than three characters long
  # filter(nchar(word1) > 2 & nchar(word2) > 2) %>%
  ### reunite the bigram
  unite(col = 'bigram', word1, word2, sep = ' ')
```

### Calculate $idf$ and $tf-idf$

```{r calc tf idf for bigrams}
value_bigram_idf <- value_bigrams %>%
  group_by(topic) %>%
  mutate(n_abstr = n_distinct(doc_id)) %>%
  group_by(topic, bigram) %>%
  summarize(n_abstr_w_term = n_distinct(doc_id),
            n_abstr = first(n_abstr), .groups = 'drop') %>%
  mutate(idf = -log(n_abstr_w_term / n_abstr))

value_bigram_tf_idf <- value_bigrams %>%
  left_join(value_bigram_idf, by = c('topic', 'bigram')) %>%
  mutate(tf_idf = bigram_freq * idf) %>%
  arrange(desc(tf_idf))
  
top_tf_idf_by_topic <- value_bigram_tf_idf %>%
  group_by(topic) %>%
  slice_max(tf_idf, n = 5) %>%
  mutate(across(.cols = c(bigram_freq, idf, tf_idf), .fns = ~round(.x, 4))) %>%
  select(doc_id, topic, bigram, bigram_freq, idf, tf_idf)

DT::datatable(top_tf_idf_by_topic)

```


## Examine "value" trigram frequency across abstracts

Similar to above, except here, let's break the abstracts into _tri_grams and focus on those that include one of the "value" search terms.

```{r summarize trigrams}
abstr_trigrams_all <- abstr_df %>%
  ### first divide sentences so trigrams don't continue over sentence breaks
  mutate(text = str_split(text, pattern = '[.;]')) %>%
  unnest(text) %>%
  ### now break into trigrams
  unnest_tokens(input = text, output = trigram, token = 'trigrams', n = 3) %>%
  group_by(doc_id, topic, trigram) %>%
  summarize(n = n(), .groups = 'drop') %>%
  group_by(doc_id, topic) %>%
  mutate(trigram_freq = n / sum(n)) %>%
  group_by(topic) %>%
  mutate(n_trigrams = n()) %>%
  ungroup()

value_terms <- 'value|valuation|benefit|utility'

value_trigrams <- abstr_trigrams_all %>%
  ### filter to keep those trigrams with an instance of a value term
  filter(str_detect(trigram, value_terms)) %>%
  ### separate out for dropping stop words
  separate(col = trigram, into = c('word1', 'word2', 'word3'), sep = ' ') %>%
  ### drop rows where the initial or final word is a stop word
  anti_join(stop_words, by = c('word1' = 'word')) %>%
  anti_join(stop_words, by = c('word3' = 'word')) %>%
  ### drop rows where any term is less than three characters long
  # filter(nchar(word1) > 2 & nchar(word2) > 2) %>%
  ### reunite the trigram
  unite(col = 'trigram', word1, word2, word3, sep = ' ')


```

### Calculate $idf$ and $tf-idf$

```{r calc tf idf for trigrams}
value_trigram_idf <- value_trigrams %>%
  group_by(topic) %>%
  mutate(n_abstr = n_distinct(doc_id)) %>%
  group_by(topic, trigram) %>%
  summarize(n_abstr_w_term = n_distinct(doc_id),
            n_abstr = first(n_abstr), .groups = 'drop') %>%
  mutate(idf = -log(n_abstr_w_term / n_abstr))

value_trigram_tf_idf <- value_trigrams %>%
  left_join(value_trigram_idf, by = c('topic', 'trigram')) %>%
  mutate(tf_idf = trigram_freq * idf) %>%
  arrange(desc(tf_idf))
  
top_tf_idf_by_topic <- value_trigram_tf_idf %>%
  group_by(topic) %>%
  slice_max(tf_idf, n = 5) %>%
  mutate(across(.cols = c(trigram_freq, idf, tf_idf), .fns = ~round(.x, 4))) %>%
  select(doc_id, topic, trigram, trigram_freq, idf, tf_idf)

DT::datatable(top_tf_idf_by_topic)

```

## 