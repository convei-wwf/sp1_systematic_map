---
title: "Develop screening model based on FAISS outputs"
author: "O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

This script takes the .csv files output by `1_faiss_criteria_query.qmd` and insights from `2_faiss_examine_results.qmd` to develop and train a screening model for the overall citation set.

* For each phrase category (remote sensing, societal benefit, valuation methodology) identify the phrase with the greatest similarity (lowest score) and keep only that.
* Examine how well remaining phrases can predict inclusion vs exclusion on the SBR
    * Try various models: logistic regression, random forest, maybe neural networks
    * Model selection using cross validation in `tidymodels` package
    * Examine success metrics e.g., AUC to identify potential thresholds for exclusion

```{r}
library(tidyverse)
library(ggfortify)   ### for PCA plots
library(here)
library(tidymodels)
library(corrplot)

```

## Load query results

```{r}
faiss_query_fs <- list.files(here('3_sentence_transformer_similarity_ranking', 
                                  'faiss_out'), pattern = 'faiss_min', 
                                  full.names = TRUE)

faiss_q_df <- lapply(faiss_query_fs, FUN = function(f) {
  ## f <- faiss_query_fs[1]
  q <- basename(f) %>% str_remove_all('faiss_min_|.csv')
  df <- read_csv(f, show_col_types = FALSE) %>%
    mutate(query = q) %>%
    select(dist, query, citation)
}) %>%
  bind_rows() %>%
  mutate(criteria = str_extract(query, '^[a-z]'),
         query = str_remove(query, '[a-z]_'))
```


## Load references from Societal Benefits Repository

We can compare the screened results from the SBR to the distance values here, perhaps via PCA, clustering, logistic regression...

Note: `_data/societal_benefits/sbr_screened_results_240429.csv` is the citation screening results; `_output/societal_benefits/fulltext_screened.csv` is the full text screening results (does not include those excluded in the citation screening).

```{r}
cit_scr_df <- read_csv(here('_data/societal_benefits/sbr_screened_results_240429.csv')) %>%
  mutate(title = str_to_title(title))

fulltext_df <- read_csv(here('_output/societal_benefits/fulltext_screened.csv')) %>%
  ### skip author, match by title only?
  select(title, screening_decision, reason_for_exclusion, notes = `Quick notes`) %>%
  mutate(title = str_to_title(title))

screened_df <- cit_scr_df %>%
  full_join(fulltext_df) %>%
  mutate(decision = ifelse(is.na(screening_decision), screening_status, tolower(screening_decision))) %>%
  mutate(decision = str_replace(decision, 'cluded', 'clude'))


rescale <- function(x) {
  1 - (x - min(x)) / (max(x) - min(x))
}

sbr_vs_faiss <- screened_df %>%
  select(author, title, decision, tags, key) %>%
  left_join(faiss_q_df, by = c('key' = 'citation')) %>%
  ### find the lowest distance among queries in the category
  # group_by(author, title, criteria) %>%
  # filter(dist == min(dist)) %>%
  ### convert distance to relevance: low distance = high relevance
  group_by(query) %>%
  mutate(dist_norm = rescale(dist)) %>%
  ungroup()

```

## Examine collinearity

```{r}

sbr_faiss_wide <- sbr_vs_faiss %>%
  filter(!is.na(query)) %>%
  filter(!criteria %in% c('b')) %>% ### drop societal benefits queries?
  select(-dist, -key, -criteria) %>%
  mutate(query = str_sub(query, 1, 10)) %>%
  pivot_wider(names_from = 'query', values_from = 'dist_norm')

sbr_faiss_mtx <- sbr_faiss_wide %>%
  select(-author, -title, -decision, -tags) %>%
  as.matrix()

corr_mtx <- cor(sbr_faiss_mtx)
corrplot.mixed(corr_mtx)
```

The strength of collinearity is pretty low for most variable pairs, with exception of earth observation, remote sensing, and satellite data, which have correlation coefficients of around .67.  Let's see what these look like with a PCA.

## Examine relationships via PCA

```{r}
# set.seed(42)
sbr_faiss_pca <- sbr_faiss_wide %>%
  select(where(is.numeric)) %>%
  scale() %>%
  prcomp()

pca_df <- as.data.frame(sbr_faiss_pca$x) %>%
  mutate(decision = sbr_faiss_wide$decision)

pca_hull_df <- pca_df %>%
  arrange(decision, PC1, PC2) %>%
  group_by(decision) %>%
  slice(chull(PC1, PC2))

pca_rot_df <- data.frame(sbr_faiss_pca$rotation %>% round(4) * 5) %>%
  mutate(query = rownames(.))

p12 <- ggplot(pca_df, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = decision)) +
  geom_segment(data = pca_rot_df, x = 0, y = 0, 
               aes(xend = PC1, yend = PC2), 
               arrow = arrow(length = unit(.1, 'inches'))) +
  geom_text(data = pca_rot_df, aes(label = query)) +
  geom_polygon(data = pca_hull_df, 
               aes(x = PC1, y = PC2, color = decision, fill = decision), alpha = .1)

p12
```

The collinearity at least within the first two principal components is clear in the biplot.  While there is a lot of overlap between the includes and excludes, there's a pretty decent chunk of excluded documents that do not overlap at all.  Identifying thresholds to drop those would save a substantial amount of time when proceeding to a manual citation screening. 

Let's examine between PC 3 and 4:

```{r}
pca_hull_df <- pca_df %>%
  arrange(decision, PC3, PC4) %>%
  group_by(decision) %>%
  slice(chull(PC3, PC4))

pca_rot_df <- data.frame(sbr_faiss_pca$rotation %>% round(4) * 5) %>%
  mutate(query = rownames(.))

p34 <- ggplot(pca_df, aes(x = PC3, y = PC4)) +
  geom_point(aes(color = decision)) +
  geom_segment(data = pca_rot_df, x = 0, y = 0, 
               aes(xend = PC3, yend = PC4), 
               arrow = arrow(length = unit(.1, 'inches'))) +
  geom_text(data = pca_rot_df, aes(label = query)) +
  geom_polygon(data = pca_hull_df, 
               aes(x = PC3, y = PC4, color = decision, fill = decision), alpha = .1)

p34
```

```{r}

scree_df <- data.frame(PC = paste0('PC', 1:length(sbr_faiss_pca$sdev)),
                       pct = (sbr_faiss_pca$sdev)^2 / sum((sbr_faiss_pca$sdev)^2)) %>%
  mutate(PC = fct_inorder(PC),
         lbl = paste0(round(pct * 100, 1), '%'),
         cumsum = cumsum(pct))

ggplot(scree_df, aes(x = PC, y = pct)) +
  geom_col() +
  geom_text(aes(label = lbl), vjust = 0, nudge_y = .01)

knitr::kable(pca_rot_df %>% select(PC1:PC4))

```

The screeplot shows that 75% of the variance is captured in the first four principal components.  We don't necessarily need the dimensionality reduction at this point, but removing the collinearity might be helpful for the binary logistic regression model.


## Let's try some predictive models

Split the data for later comparison of performance of different models.  `sbr_train_df` will be used for model selection.  Drop variables not used in model, and convert outcome variable to factor.  Use the PCA results.

```{r}
set.seed(42)
sbr_split <- initial_split(sbr_faiss_wide %>%
                             mutate(decision = factor(decision)) %>%
                             select(decision, where(is.numeric)),
                           prop = .80, strata = decision)

sbr_test_df <- testing(sbr_split)
sbr_train_df <- training(sbr_split)

sbr_train_folds <- vfold_cv(sbr_train_df, v = 5)
```

### Binary logistic regression

Set up the model, then set up a workflow.  Using this workflow, resample for k-fold cross validation.

```{r}
blr_mdl <- logistic_reg() %>%
  set_engine('glm')

blr_wf <- workflow() %>%
  add_model(blr_mdl) %>%
  add_formula(decision ~ .)

blr_fit_folds <- blr_wf %>%
  fit_resamples(sbr_train_folds)

collect_metrics(blr_fit_folds)

# glm(decision ~ ., data = sbr_train_df, family = 'binomial') %>% summary
```

### Random forest

```{r}
rf_mdl <- rand_forest(trees = 1000) %>%
  set_engine('ranger') %>%
  set_mode('classification')

rf_wf <- workflow() %>%
  add_model(rf_mdl) %>%
  add_formula(decision ~ .)

rf_fit_folds <- rf_wf %>%
  fit_resamples(sbr_train_folds)

collect_metrics(rf_fit_folds)
```

## Compare last fits

```{r}
blr_last <- blr_wf %>%
  last_fit(sbr_split)

rf_last  <- rf_wf %>%
  last_fit(sbr_split)

collect_metrics(blr_last)
collect_metrics(rf_last)
```

Based on this, both models seems to do a good job of prediction.  However, we are more concerned about false negatives (rejecting a document that should be included) than false positives (including a document that should be excluded), and our dataset is a bit unbalanced (~20% inclusion) and the full dataset is likely to be far more unbalanced, so these "success metrics" are likely driven by high true negative rates.  Let's pull info out of the `rf_last`, and plot a ROC and confusion matrix to see if there's a good threshold for probability that drops true negatives but doesn't drop many true positives...

```{r}
mdl_check_df <- rf_last$.predictions[[1]]

mdl_check_df %>% select(.pred_class, decision) %>% table()

mdl_check_df %>% 
  filter(decision == 'include' | .pred_class == 'include') %>% 
  arrange(.pred_include)
```

From this rough cut, one false positive, but a high false negative rate (9 out of 12 "includes" classified as "exclude").  Not great!  The `.pred_include` column shows that false negatives start at only 9.6%, with many more in the 10-20% range.  

```{r}
roc_curve(mdl_check_df, truth = decision, .pred_exclude) %>%
  autoplot()
```

## Which documents are classified as false negatives?

Examine the false negatives, including the queries with the lowest similarity values (these are rescaled distance values, so 0 = highest distance = lowest similarity, 1 = lowest distance = highest similarity).

```{r}
false_neg_indices <- mdl_check_df %>% 
  filter(decision == 'include' & .pred_class == 'exclude') %>% 
  arrange(.pred_include) %>%
  pull(.row)

cites_to_check <- sbr_faiss_wide %>%
  mutate(across(where(is.numeric), ~round(.x, 3))) %>%
  slice(false_neg_indices)

write_csv(cites_to_check, 'cites_to_check.csv')
```

