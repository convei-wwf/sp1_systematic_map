---
title: "Develop screening model based on FAISS outputs"
author: "O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

This script takes the .csv files output by `1_faiss_criteria_query.qmd` and insights from `2_faiss_examine_results.qmd` to develop and train a screening model for the overall citation set.

* Identify candidate phrases to drop due to collinearity
* Examine how well remaining phrases can predict inclusion vs exclusion on the SBR
    * Try various models: logistic regression, random forest, maybe neural networks
    * Model selection using cross validation in `tidymodels` package
    * Examine success metrics e.g., AUC to identify potential thresholds for exclusion

```{r}
library(tidyverse)
library(ggfortify)   ### for PCA plots
library(here)
library(tidymodels)
library(corrplot)

```

## Load query results

```{r}
faiss_query_fs <- list.files('faiss_out', pattern = 'faiss_min', full.names = TRUE)

faiss_q_df <- lapply(faiss_query_fs, FUN = function(f) {
  ## f <- faiss_query_fs[1]
  q <- basename(f) %>% str_remove_all('faiss_min_|.csv')
  df <- read_csv(f, show_col_types = FALSE) %>%
    mutate(query = q) %>%
    select(dist, query, citation)
}) %>%
  bind_rows()
```


## Load references from Societal Benefits Repository

We can compare the screened results from the SBR to the distance values here, perhaps via PCA, clustering, logistic regression...

Note: `_data/societal_benefits/sbr_screened_results_240429.csv` is the citation screening results; `_output/societal_benefits/fulltext_screened.csv` is the full text screening results (does not include those excluded in the citation screening).

```{r}
cit_scr_df <- read_csv(here('_data/societal_benefits/sbr_screened_results_240429.csv')) %>%
  mutate(title = str_to_title(title))

fulltext_df <- read_csv(here('_output/societal_benefits/fulltext_screened.csv')) %>%
  ### skip author, match by title only?
  select(title, screening_decision, reason_for_exclusion, notes = `Quick notes`) %>%
  mutate(title = str_to_title(title))

screened_df <- cit_scr_df %>%
  full_join(fulltext_df) %>%
  mutate(decision = ifelse(is.na(screening_decision), screening_status, tolower(screening_decision))) %>%
  mutate(decision = str_replace(decision, 'cluded', 'clude'))


rescale <- function(x) {
  1 - (x - min(x)) / (max(x) - min(x))
}

sbr_vs_faiss <- screened_df %>%
  select(author, title, decision, tags, key) %>%
  left_join(faiss_q_df, by = c('key' = 'citation')) %>%
  ### convert distance to relevance: low distance = high relevance
  group_by(query) %>%
  mutate(dist_norm = rescale(dist)) %>%
  ungroup()

```

## Identify collinearity to reduce dimensionality

Could also do PCA, but harder to explain axes

```{r}

sbr_faiss_wide <- sbr_vs_faiss %>%
  select(-dist, -key) %>%
  filter(!is.na(query)) %>%
  pivot_wider(names_from = 'query', values_from = 'dist_norm')

sbr_faiss_mtx <- sbr_faiss_wide %>%
  select(-author, -title, -decision, -tags) %>%
  as.matrix()

corr_mtx <- cor(sbr_faiss_mtx)
```

Examine correlation matrix to identify candidate variables to drop.

```{r}
corr_df <- as.data.frame(corr_mtx, col.names = colnames(corr_mtx)) %>% 
  mutate(row = rownames(corr_mtx)) %>%
  pivot_longer(-row, names_to = 'col', values_to = 'corr') %>%
  filter(row != col) %>%
  mutate(pair = ifelse(col > row, paste(col, row, sep = '-'), paste(row, col, sep = '-'))) %>%
  select(-row, -col) %>%
  distinct() %>%
  separate(col = pair, into = c('row', 'col'), sep = '-')

dropcols <- corr_df %>%
  group_by(col) %>%
  mutate(colsum = sum(abs(corr))) %>%
  group_by(row) %>%
  mutate(rowsum = sum(abs(corr))) %>%
  ungroup() %>%
  mutate(drop = ifelse(rowsum > colsum, row, col)) %>%
  filter(abs(corr) > 0.82) %>%
  .$drop

```

## Examine relationships via PCA

```{r}

sbr_faiss_parsim <- sbr_faiss_wide %>%
  ### drop highly correlated axes
  select(-all_of(dropcols)) 

sba_faiss_pca <- sbr_faiss_parsim %>%
  select(-author, -title, -decision, -tags) %>%
  scale() %>%
  prcomp()

sbr_faiss_pca$rotation
   
autoplot(sbr_faiss_pca,
         data = sbr_faiss_wide,
         color = 'decision',
         loadings = TRUE,
         loadings.label = TRUE,
         loadings.color = 'black',
         loadings.label.color = 'black')

screeplot(sbr_faiss_pca)
```

## Let's try some predictive models

Split the data for later comparison of performance of different models.  `sbr_train_df` will be used for model selection.  Drop variables not used in model, and convert outcome variable to factor.

```{r}
sbr_split <- initial_split(sbr_faiss_parsim %>%
                             select(-author, -title, -tags) %>%
                             mutate(decision = factor(decision)),
                           prop = .80, strata = decision)

sbr_test_df <- testing(sbr_split)
sbr_train_df <- training(sbr_split)

sbr_train_folds <- vfold_cv(sbr_train_df, v = 5)
```

### Binary logistic regression

Set up the model, then set up a workflow.  Using this workflow, resample for k-fold cross validation.

```{r}
blr_mdl <- logistic_reg() %>%
  set_engine('glm')

blr_wf <- workflow() %>%
  add_model(blr_mdl) %>%
  add_formula(decision ~ .)

blr_fit_folds <- blr_wf %>%
  fit_resamples(sbr_train_folds)

collect_metrics(blr_fit_folds)
```

### Random forest

```{r}
# set.seed(42)
rf_mdl <- rand_forest(trees = 1000) %>%
  set_engine('ranger') %>%
  set_mode('classification')

rf_wf <- workflow() %>%
  add_model(rf_mdl) %>%
  add_formula(decision ~ .)

rf_fit_folds <- rf_wf %>%
  fit_resamples(sbr_train_folds)

collect_metrics(rf_fit_folds)
```

## Compare last fits

```{r}
blr_last <- blr_wf %>%
  last_fit(sbr_split)

rf_last  <- rf_wf %>%
  last_fit(sbr_split)

collect_metrics(blr_last)
collect_metrics(rf_last)
```

