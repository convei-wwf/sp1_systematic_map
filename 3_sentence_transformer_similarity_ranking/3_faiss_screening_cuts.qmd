---
title: "Develop screening model based on FAISS outputs"
author: "O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

This script takes the .csv files output by `1_faiss_criteria_query.qmd` and insights from `2_faiss_examine_results.qmd` to develop and train a screening model for the overall citation set.

* For each phrase category (remote sensing, societal benefit, valuation methodology) identify the phrase with the greatest similarity (lowest score) and keep only that.
* Examine how well remaining phrases can predict inclusion vs exclusion on the SBR
    * Try various models: logistic regression, random forest, maybe neural networks
    * Model selection using cross validation in `tidymodels` package
    * Examine success metrics e.g., AUC to identify potential thresholds for exclusion

```{r}
library(tidyverse)
library(ggfortify)   ### for PCA plots
library(here)
library(corrplot)
library(cowplot)
library(sf)

```

## Load query results

```{r}
faiss_query_fs <- list.files(here('3_sentence_transformer_similarity_ranking', 
                                  'faiss_out'), pattern = 'faiss_min', 
                                  full.names = TRUE)

faiss_q_df <- lapply(faiss_query_fs, FUN = function(f) {
  ## f <- faiss_query_fs[1]
  q <- basename(f) %>% str_remove_all('faiss_min_|.csv')
  df <- read_csv(f, show_col_types = FALSE) %>%
    mutate(query = q) %>%
    select(dist, query, citation)
}) %>%
  bind_rows() %>%
  mutate(criteria = str_extract(query, '^[a-z]'),
         query = str_remove(query, '[a-z]_')) %>%
  mutate(citation = stringi::stri_trans_general(citation, 'Latin-ASCII'))
  # mutate(citation = str_replace_all(citation, "â€™", "'"))

check <- faiss_q_df %>%
  select(-criteria) %>%
  pivot_wider(values_from = 'dist', names_from = 'query')

### how do citations bin out by year? 
y <- check %>% mutate(y = str_extract(citation, '[0-9]{4}$') %>% as.integer())
x <- y %>% group_by(y) %>% summarize(n = n()) %>% arrange(y) %>% mutate(y_cum = cumsum(n))

# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 1955    2015    2020    2017    2022    2024
### if the median is 2020, a year cutoff is pretty meaningless
```


## Load references from Societal Benefits Repository

We can compare the screened results to the distance values here, perhaps via PCA, clustering, logistic regression... load the `_output/training_set.csv` file.

QUESTION: Since distances are all on the same unitless scale, should we rescale these distance values?  If a query shows a wide range of distances it seems it should play a greater role in differentiating than a query that reveals only a narrow range of distances. 

FOR NOW: no, we will not do rescaling

```{r}
screened_df <- read_csv(here('int/classifier_training_set.csv')) %>%
  mutate(decision = ifelse(include, 'include', 'exclude'))
```


## Examine collinearity

```{r}
screened_faiss <- screened_df %>%
  full_join(faiss_q_df, by = c('key' = 'citation')) %>%
  filter(!is.na(dist))
  ### adding in those climate model papers really fucks the cut rate

faiss_wide <- screened_faiss %>%
  filter(!is.na(query)) %>%
  filter(!criteria %in% c('b')) %>% ### drop societal benefits queries?
  select(key, dist, query, decision, src) %>%
  pivot_wider(names_from = 'query', values_from = 'dist')

faiss_mtx <- faiss_wide %>%
  column_to_rownames(var = 'key') %>%
  select(-decision, -src) %>%
  setNames(str_sub(names(.), 1, 10)) %>%
  as.matrix()

corr_mtx <- cor(faiss_mtx)
corrplot.mixed(corr_mtx)
```

The strength of collinearity is pretty low for most variable pairs, with exception of earth observation, remote sensing, and satellite data, which have higher correlation coefficients with one another and with the full phrase queries.  Let's see what these look like with a PCA.

## Examine relationships via PCA

### PC 1 and 2

QUESTION: Since distances are all on the same unitless scale, should we rescale these distance values?  If a query shows a wide range of distances it seems it should play a greater role in differentiating than a query that reveals only a narrow range of distances.  Here we have disabled rescaling (though have allowed centering for mean zero).

```{r}
# set.seed(42)
faiss_pca <- faiss_wide %>%
  select(where(is.numeric)) %>%
  prcomp(scale. = FALSE)

pca_df <- as.data.frame(faiss_pca$x) %>%
  mutate(src = faiss_wide$src,
         src = ifelse(is.na(src), 'none', src),
         decision = faiss_wide$decision,
         decision = ifelse(is.na(decision), 'not screened', decision),
         decision = factor(decision, levels = c('not screened', 'exclude', 'include'))) %>%
  arrange(decision)

pca_hull_df <- pca_df %>%
  arrange(decision, PC1, PC2) %>%
  group_by(decision) %>%
  slice(chull(PC1, PC2))


scale_factor <- min(max(abs(pca_df$PC1)), max(abs(pca_df$PC2))) / 
                max(abs(faiss_pca$rotation[, 1:2]))
  
### dataframe for loadings; scale up by scaling_factor for plotting
pca_rot_df <- data.frame(faiss_pca$rotation %>% round(4) * scale_factor) %>%
  mutate(query = rownames(.))

p12 <- ggplot(pca_df, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = decision), alpha = .4, size = 2) +
  geom_segment(data = pca_rot_df, x = 0, y = 0, 
               aes(xend = PC1, yend = PC2), 
               arrow = arrow(length = unit(.1, 'inches'))) +
  geom_text(data = pca_rot_df, aes(label = query)) +
  geom_polygon(data = pca_hull_df, 
               aes(x = PC1, y = PC2, color = decision, fill = decision), alpha = .1) +
  stat_ellipse(data = pca_df %>% filter(decision == 'include'), color = 'blue')

```

```{r}
p32 <- ggplot(pca_df, aes(x = PC3, y = PC2)) +
  geom_point(aes(color = decision), alpha = .4, size = 2) +
  geom_segment(data = pca_rot_df, x = 0, y = 0, 
               aes(xend = PC3, yend = PC2), 
               arrow = arrow(length = unit(.1, 'inches'))) +
  geom_text(data = pca_rot_df, aes(label = query)) +
  geom_polygon(data = pca_hull_df, 
               aes(x = PC3, y = PC2, color = decision, fill = decision), alpha = .1) +
  stat_ellipse(data = pca_df %>% filter(decision == 'include'), color = 'blue')
```

```{r}
plot_grid(p12 + theme(legend.position = 'none'), p32 + theme(axis.title.y = element_blank()), rel_widths = c(2, 3))
```

The collinearity at least within the first two principal components is clear in the biplot.  While there is a lot of overlap between the includes and excludes, there's a pretty decent chunk of excluded documents that do not overlap at all, and an even greater cloud of not-yet-screened documents well beyond the group of included documents.  Identifying thresholds to drop those would save a substantial amount of time when proceeding to a manual citation screening. 

Let's examine overlap (in this coordinate system) between ellipses in the first two principal components.
```{r}
p12_build <- ggplot_build(p12)$data
p12_pts <- p12_build[[1]] %>% 
  sf::st_as_sf(coords = c('x', 'y')) %>%
  mutate(id = 1:n())
p12_ell <- p12_build[[5]] %>% sf::st_as_sf(coords = c('x', 'y')) %>% 
  concaveman::concaveman()

p12_in_ell <- sf::st_filter(p12_pts, p12_ell) %>% 
  cbind(st_coordinates(.))
p12_in_ell %>% pull(group) %>% table()

pca_df %>% pull(decision) %>% table()
```

If we keep only observations within the blue ellipse in PC1 and PC 2, we include 86/92 "included" (93% recall), 675/795 "excluded" (not great reject rate), and 10606/12932 yet to be screened.  This reduces the screening burden to 82% of the yet-to-be screened.  NOT a great filter!

## Scree plot

```{r}

scree_df <- data.frame(PC = paste0('PC', 1:length(faiss_pca$sdev)),
                       pct = (faiss_pca$sdev)^2 / sum((faiss_pca$sdev)^2)) %>%
  mutate(PC = fct_inorder(PC),
         lbl = paste0(round(pct * 100, 1), '%'),
         cumsum = cumsum(pct))

ggplot(scree_df, aes(x = PC, y = pct)) +
  geom_col() +
  geom_text(aes(label = lbl), vjust = 0, nudge_y = .01)

knitr::kable(pca_rot_df %>% select(PC1:PC4))

```

The screeplot shows that `r round(scree_df$cumsum[4] * 100)`% of the variance is captured in the first four principal components.  We don't necessarily need the dimensionality reduction at this point, but removing the collinearity might be helpful if we decide to implement a binary logistic regression model.

## Hypervolume method?

```{r}
#| eval: true
#| 
library(ggpubr)
library(hypervolume)

set.seed(42)
sample_df <- pca_df %>%
  group_by(decision) %>%
  # slice_sample(n = 1000) %>%
  ungroup()
### set up matrices for hypervolumes - use first four PCs (85%)
incl_mtx <- sample_df %>%
  filter(decision == 'include') %>%
  select(PC1:PC4) %>%
  as.matrix()
 
excl_mtx <- sample_df %>%
  filter(decision == 'exclude') %>%
  select(PC1:PC4) %>%
  as.matrix()

todo_mtx <- sample_df %>%
  filter(decision == 'not screened') %>%
  select(PC1:PC4) %>%
  as.matrix()
 
### create an "include" hypervolume
incl_hypv <- hypervolume_gaussian(incl_mtx)

### test whether points in the "exclude" matrix fall within the "include"
### hypervolume
excl_test <- hypervolume_inclusion_test(incl_hypv, excl_mtx,
                                        fast.or.accurate = 'accurate',
                                        accurate.method.threshold = quantile(incl_hypv@ValueAtRandomPoints, 0.5))
todo_test <- hypervolume_inclusion_test(incl_hypv, todo_mtx,
                                        fast.or.accurate = 'accurate',
                                        accurate.method.threshold = quantile(incl_hypv@ValueAtRandomPoints, 0.5))

excl_in_vol <- sum(excl_test) / length(excl_test)

todo_in_vol <- sum(todo_test) / length(todo_test)
```

In the hypervolume defined by the "includes", overlaps with `r round(excl_in_vol * 100)`% of "excludes" and `r round(todo_in_vol * 100)`% documents still to be assessed.

```{r}
hv12 <- ggplot(data = sample_df, aes(x = PC1, y = PC2)) +
  geom_point(data = sample_df %>% filter(decision == 'not screened') %>% mutate(test = todo_test),
             aes(color = test), shape = 21, size = 3, alpha = .5, fill = 'yellow') +
  geom_point(data = sample_df %>% filter(decision == 'exclude') %>% mutate(test = excl_test),
             aes(color = test), fill = 'red', shape = 21, size = 3, alpha = .5) +
  geom_point(data = sample_df %>% filter(decision == 'include'), color = 'darkgreen', size = 3) +
  scale_color_manual(values = c('black', 'white'))

hv32 <- ggplot(data = sample_df, aes(x = PC3, y = PC2)) +
  geom_point(data = sample_df %>% filter(decision == 'not screened') %>% mutate(test = todo_test),
             aes(color = test), shape = 21, size = 3, alpha = .5, fill = 'yellow') +
  geom_point(data = sample_df %>% filter(decision == 'exclude') %>% mutate(test = excl_test),
             aes(color = test), fill = 'red', shape = 21, size = 3, alpha = .5) +
  geom_point(data = sample_df %>% filter(decision == 'include'), color = 'darkgreen', size = 3) +
  scale_color_manual(values = c('black', 'white'))

plot_grid(hv12 + theme(legend.position = 'none'), hv32 + theme(axis.title.y = element_blank()), rel_widths = c(4, 5))

```

Green is "includes", red is "excludes", and yellow-filled are still to be screened.