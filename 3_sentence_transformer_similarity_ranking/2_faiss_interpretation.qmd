---
title: "Examine FAISS outputs"
author: "O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

This script takes the .csv files output by `1_faiss_criteria_query.qmd` and examines them:

* Plot distance curves to see whether there are steep breaks
* Run logistic regression against screened documents from the Societal Benefits Repo.

```{r}
library(tidyverse)
library(ggfortify)   ### for PCA plots
library(here)
library(synthesisr)
```

```{r}
faiss_query_fs <- list.files('faiss_out', pattern = 'faiss', full.names = TRUE)

faiss_q_df <- lapply(faiss_query_fs, FUN = function(f) {
  ## f <- faiss_query_fs[1]
  q <- basename(f) %>% str_remove_all('faiss_out_|.csv')
  df <- read_csv(f, show_col_types = FALSE) %>%
    mutate(query = q) %>%
    select(dist, query, citation)
}) %>%
  bind_rows() %>%
  ### dissect citation
  mutate(citation = str_remove(citation, '^\\[.+?\\]') %>% tolower()) %>%  ### remove [ESA] and [the australian thing]
  mutate(author = str_remove(citation, ' \\(.+') %>% str_trim(),
         year   = str_extract(citation, '(?<=\\()[0-9]{4}(?=\\))') %>% as.integer(),
         title  = str_remove(citation, author) %>% str_remove_all('\\([0-9]{4}\\)\\. |\\. n/a.+') %>% str_trim() %>% str_sub(1, 100),
         author = str_extract(author, '[a-z]+'),
         doi    = str_remove(citation, '.+doi: ') %>% str_trim() %>% tolower())
```

## Plot distance per citation for each query

Ideally we'd see a bimodal distribution, with strong representation at low or high values but fewer intermediate values; this would resemble a stepwise function on qq plot.

```{r}
ggplot(faiss_q_df) +
  stat_qq(aes(sample = dist)) +
  facet_wrap(~ query)

ggplot(faiss_q_df) +
  geom_histogram(aes(x = dist)) +
  facet_wrap(~ query)
```

## Load references from Societal Benefits Repository

We can compare the screened results from the SBR to the distance values here, perhaps via PCA, clustering, logistic regression...

```{r}
sbr_screened_df <- read_csv(here('_data/societal_benefits/sbr_screened_results_240429.csv')) %>%
  ### not sure why these citations are messed up...
  mutate(author = ifelse(title == 'The Socio-Economic Value Of Scientific Publications: The Case Of Earth Observation Satellites', 'morretta', author)) %>%
  mutate(title = tolower(title) %>% str_sub(1, 100)) %>%
  filter(!is.na(author)) %>%
  mutate(author = str_remove(author, '\\[.+?\\]') %>% tolower() %>% str_extract('^[a-z]+'))

  
rescale <- function(x) {
  1 - (x - min(x)) / (max(x) - min(x))
}

sbr_vs_faiss <- sbr_screened_df %>%
  select(author, title, screening_status, tags) %>%
  left_join(faiss_q_df, by = c('author', 'title')) %>%
  ### convert distance to relevance: low distance = high relevance
  group_by(query) %>%
  mutate(dist_norm = rescale(dist)) %>%
  ungroup()

sbr_vs_faiss_geom_mean <- sbr_vs_faiss %>%
  ### let d be geometric mean of rescaled relevances: if any axis is low relevance,
  ### citation is of low relevance
  group_by(author, title, screening_status) %>%
  summarize(geom_mean = prod(dist_norm)^(n_distinct(query)), 
            .groups = 'drop') %>%
  right_join(sbr_vs_faiss)

ggplot(sbr_vs_faiss_geom_mean, aes(x = dist_norm)) +
  geom_density(aes(fill = screening_status), alpha = .25) +
  facet_wrap(~ query)

t.test(dist_norm ~ screening_status,
       data = sbr_vs_faiss_geom_mean %>% 
         select(dist_norm, screening_status) %>% 
         distinct())
```

```{r}
ggplot(sbr_vs_faiss_geom_mean) +
  stat_qq(aes(sample = dist_norm, color = screening_status)) +
  facet_wrap(~ query)

ggplot(sbr_vs_faiss_geom_mean) +
  geom_density(aes(x = dist_norm, fill = screening_status), alpha = .25) +
  facet_wrap(~ query, scales = 'free_y')
```

## Let's try some PCA!

```{r}
sbr_faiss_wide <- sbr_vs_faiss %>%
  select(-dist, -citation) %>%
  pivot_wider(names_from = 'query', values_from = 'dist_norm') %>%
  ### post-PCA, identify highly redundant axes
  select(-bayesian_decision_analysis, -remote_earth_observation, -societal_benefits)

sbr_faiss_pca <- sbr_faiss_wide %>%
  select(-author, -title, -screening_status, -year, -doi, -tags) %>%
  scale() %>%
  prcomp()

sbr_faiss_pca$rotation
   
autoplot(sbr_faiss_pca,
         data = sbr_faiss_wide,
         color = 'screening_status',
         loadings = TRUE,
         loadings.label = TRUE,
         loadings.color = 'black',
         loadings.label.color = 'black')

screeplot(sbr_faiss_pca)
```

## Write out normalized distance values

Take a sample of the highest-ranking "excludes" and lowest-ranking "includes" to examine why the distance scoring performs poorly (e.g., maybe missing some key words, maybe some screened articles were classified incorrectly)

```{r}
tmp <- sbr_faiss_wide %>%
  select(-value_of_information, -satellite_data,-socioeconomic_benefits) %>%
  mutate(geom_mean = (decision_analysis * satellite_earth_observation * societal_value * valuation_of_information)^.25) %>%
  left_join(sbr_screened_df %>% select(title, abstract)) %>%
  filter((screening_status == 'excluded' & geom_mean > .5) | (screening_status == 'included' & geom_mean < .35))

write_csv(tmp, 'sbr_screened_dist_mtx.csv')
```

