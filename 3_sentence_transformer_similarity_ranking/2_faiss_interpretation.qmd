---
title: "Examine FAISS outputs"
author: "O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

This script takes the .csv files output by `1_faiss_criteria_query.qmd` and examines them:

* Plot distance curves to see whether there are steep breaks
* Run logistic regression against screened documents from the Societal Benefits Repo.

```{r}
library(tidyverse)
library(ggfortify)   ### for PCA plots
library(here)
library(synthesisr)
```

```{r}
faiss_query_fs <- list.files('faiss_out', pattern = 'faiss_min', full.names = TRUE)

faiss_q_df <- lapply(faiss_query_fs, FUN = function(f) {
  ## f <- faiss_query_fs[1]
  q <- basename(f) %>% str_remove_all('faiss_min_|.csv')
  df <- read_csv(f, show_col_types = FALSE) %>%
    mutate(query = q) %>%
    select(dist, query, citation)
}) %>%
  bind_rows()
```

## Plot distance per citation for each query

Ideally we'd see a bimodal distribution, with strong representation at low or high values but fewer intermediate values; this would resemble a stepwise function on qq plot.

```{r}
ggplot(faiss_q_df) +
  stat_qq(aes(sample = dist)) +
  facet_wrap(~ query)

ggplot(faiss_q_df) +
  geom_histogram(aes(x = dist)) +
  facet_wrap(~ query)
```

## Load references from Societal Benefits Repository

We can compare the screened results from the SBR to the distance values here, perhaps via PCA, clustering, logistic regression...

Note: `_data/societal_benefits/sbr_screened_results_240429.csv` is the citation screening results; `_output/societal_benefits/fulltext_screened.csv` is the full text screening results (does not include those excluded in the citation screening).

```{r}
cit_scr_df <- read_csv(here('_data/societal_benefits/sbr_screened_results_240429.csv')) %>%
  mutate(title = str_to_title(title))

fulltext_df <- read_csv(here('_output/societal_benefits/fulltext_screened.csv')) %>%
  ### skip author, match by title only?
  select(title, screening_decision, reason_for_exclusion, notes = `Quick notes`) %>%
  mutate(title = str_to_title(title))

screened_df <- cit_scr_df %>%
  full_join(fulltext_df) %>%
  mutate(decision = ifelse(is.na(screening_decision), screening_status, tolower(screening_decision))) %>%
  mutate(decision = str_replace(decision, 'cluded', 'clude'))


rescale <- function(x) {
  1 - (x - min(x)) / (max(x) - min(x))
}

sbr_vs_faiss <- screened_df %>%
  select(author, title, decision, tags, key) %>%
  left_join(faiss_q_df, by = c('key' = 'citation')) %>%
  ### convert distance to relevance: low distance = high relevance
  group_by(query) %>%
  mutate(dist_norm = rescale(dist)) %>%
  ungroup()

sbr_vs_faiss_geom_mean <- sbr_vs_faiss %>%
  ### let d be geometric mean of rescaled relevances: if any axis is low relevance,
  ### citation is of low relevance
  group_by(author, title, decision) %>%
  summarize(geom_mean = prod(dist_norm)^(n_distinct(query)), 
            .groups = 'drop') %>%
  right_join(sbr_vs_faiss)

ggplot(sbr_vs_faiss_geom_mean, aes(x = dist_norm)) +
  geom_density(aes(fill = decision), alpha = .25) +
  facet_wrap(~ query)

t.test(dist_norm ~ decision,
       data = sbr_vs_faiss_geom_mean %>% 
         select(dist_norm, decision) %>% 
         distinct())
```

```{r}
ggplot(sbr_vs_faiss_geom_mean) +
  stat_qq(aes(sample = dist_norm, color = decision)) +
  facet_wrap(~ query)

ggplot(sbr_vs_faiss_geom_mean) +
  geom_density(aes(x = dist_norm, fill = decision), alpha = .25) +
  facet_wrap(~ query, scales = 'free_y')
```

## Let's try some PCA!

```{r}
library(corrplot)

sbr_faiss_wide <- sbr_vs_faiss %>%
  select(-dist, -key) %>%
  filter(!is.na(query)) %>%
  pivot_wider(names_from = 'query', values_from = 'dist_norm')

sbr_faiss_mtx <- sbr_faiss_wide %>%
  select(-author, -title, -decision, -tags) %>%
  as.matrix()

corr_mtx <- cor(sbr_faiss_mtx)

### rename rows and cols for easier display
varnames_df <- data.frame(name = colnames(corr_mtx)) %>%
  mutate(code = LETTERS[1:n()])
corr_mtx2 <- corr_mtx
rownames(corr_mtx2) <- colnames(corr_mtx2) <- varnames_df$code
corrplot.mixed(corr_mtx2)
knitr::kable(varnames_df)

```

Examine correlation matrix to identify candidate variables to drop.

```{r}
corr_df <- as.data.frame(corr_mtx, col.names = colnames(corr_mtx)) %>% 
  mutate(row = rownames(corr_mtx)) %>%
  pivot_longer(-row, names_to = 'col', values_to = 'corr') %>%
  filter(row != col) %>%
  mutate(pair = ifelse(col > row, paste(col, row, sep = '-'), paste(row, col, sep = '-'))) %>%
  select(-row, -col) %>%
  distinct() %>%
  separate(col = pair, into = c('row', 'col'), sep = '-')

dropcols <- corr_df %>%
  group_by(col) %>%
  mutate(colsum = sum(abs(corr))) %>%
  group_by(row) %>%
  mutate(rowsum = sum(abs(corr))) %>%
  ungroup() %>%
  mutate(drop = ifelse(rowsum > colsum, row, col)) %>%
  filter(abs(corr) > 0.82) %>%
  .$drop

```

```{r}

sbr_faiss_parsim <- sbr_faiss_wide %>%
  select(-author, -title, -decision, -tags) %>%
  ### drop highly correlated axes
  select(-all_of(dropcols)) 

sba_faiss_pca <- sbr_faiss_parsim %>%
  scale() %>%
  prcomp()

sbr_faiss_pca$rotation
   
autoplot(sbr_faiss_pca,
         data = sbr_faiss_wide,
         color = 'decision',
         loadings = TRUE,
         loadings.label = TRUE,
         loadings.color = 'black',
         loadings.label.color = 'black')

screeplot(sbr_faiss_pca)
```

## Let's try some predictive models



