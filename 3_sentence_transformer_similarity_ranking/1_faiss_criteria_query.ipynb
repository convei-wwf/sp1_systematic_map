{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Test FAISS to assist screening\"\n",
        "author: \"O'Hara\"\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    embed-resources: true\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: false\n",
        "  message: false\n",
        "editor: source\n",
        "---"
      ],
      "id": "2616f501"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script takes the interactive prototype developed by Rich Sharp and modifies it to assist with screening.  That Python script will be used to embed/encode the corpus into a FAISS index.\n",
        "\n",
        "1. The corpus of title/abstracts is embedded and encoded"
      ],
      "id": "0fd4f353"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import chardet\n",
        "# import datetime\n",
        "import glob\n",
        "# import hashlib\n",
        "# import logging\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "import faiss\n",
        "# import spacy\n",
        "# import tiktoken\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "id": "15586197",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# spacy.require_gpu()\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# GPT_MODEL, MAX_TOKENS, MAX_RESPONSE_TOKENS = 'gpt-4o', 20000, 4000\n",
        "# ENCODING = tiktoken.encoding_for_model(GPT_MODEL)"
      ],
      "id": "6fdd4f52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format=(\n",
        "#         '%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n",
        "#         ' [%(funcName)s:%(lineno)d] %(message)s'))\n",
        "#         \n",
        "# logging.getLogger('sentence_transformers').setLevel(logging.WARN)\n",
        "# \n",
        "# LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "# BODY_TAG = 'body'\n",
        "# CITATION_TAG = 'citation'\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model = SentenceTransformer(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
        "\n",
        "### Quarto interprets script location as root; set back one level\n",
        "CACHE_DIR = '../llm_cache'\n",
        "\n",
        "parsed_path = glob.glob(os.path.join(CACHE_DIR, '*.pkl'))\n",
        "faiss_path = glob.glob(os.path.join(CACHE_DIR, '*.faiss'))\n",
        "\n",
        "if len(parsed_path) > 1:\n",
        "    exit('Too many cache files! discard old versions')\n",
        "if len(parsed_path) == 0:\n",
        "    exit('No cache files! run processing script')\n",
        "    \n",
        "### open the pickle file and assign to objects abstract_list and citation_list\n",
        "with open(parsed_path[0], 'rb') as file:\n",
        "    (abstract_list, citation_list) = pickle.load(file)\n",
        "    \n",
        "### read in the document distance index from the faiss file\n",
        "document_distance_index = faiss.read_index(faiss_path[0])"
      ],
      "id": "162e1463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having read in the indexed articles, let's process the distance for each criteria phrase across the entire corpus.  Because each abstract is divided by sentence, take the sentence with the highest quality match and assign its distance score to the entire abstract.  Finally, save out as .csv with article info and distance.\n"
      ],
      "id": "07d4c31e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rank_articles(question):\n",
        "    question_embedding = embedding_model.encode(\n",
        "        question, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "    # Ensure the question_embedding is 2D\n",
        "    if len(question_embedding.shape) == 1:\n",
        "        question_embedding = question_embedding.reshape(1, -1)\n",
        "\n",
        "    ### distances and indices are determined from the search;\n",
        "    ### distances are from low to high, indices are used to retrieve the \n",
        "    ### appropriate citations\n",
        "    n_articles = len(abstract_list)\n",
        "    distances, indices = document_distance_index.search(\n",
        "        question_embedding, n_articles)\n",
        "\n",
        "    retrieved_citations = [\n",
        "        citation_list[idx] for idx in\n",
        "        indices[:, 0:n_articles].flatten()]\n",
        "        \n",
        "    ### Assemble distances and citations into a dataframe\n",
        "    dist_series = pd.Series(distances[0])\n",
        "    cit_series  = pd.Series(retrieved_citations)\n",
        "    \n",
        "    dist_cit_df = pd.DataFrame({'dist': dist_series, 'citation': cit_series})\n",
        "    \n",
        "    return dist_cit_df"
      ],
      "id": "abfd3a05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Queries for each criterion group\n",
        "\n",
        "For each criterion, try a few queries.  When processing, perhaps summarize each criterion group of queries by selecting the minimum value.  For example, \"Earth observation\" might indicate high similarity, even if the paper does not mention satellite data (e.g., mentions Landsat instead) - so base the similarity on the most-similar term in the group.  If a paper is not similar on any metric, then even the most similar will be far afield.\n",
        "\n",
        "### Criterion A: earth observation/satellite data\n",
        "\n",
        "Include a letter indicating criterion group in the file name, for easy access later\n"
      ],
      "id": "25371cc1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "term_list = ['remote sensing', 'earth observation', 'satellite data']\n",
        "\n",
        "OUT_DIR = 'faiss_out'\n",
        "\n",
        "for term in term_list:\n",
        "    ### term = term_list[0]\n",
        "    \n",
        "    ### Apply the rank_articles function for each term in the list\n",
        "    x = rank_articles(term)\n",
        "    \n",
        "    ### use groupby and aggregate to keep the min distance per citation\n",
        "    y = x.groupby('citation').agg({'dist': 'min'})\n",
        "    # z = x.groupby('citation').agg({'dist': 'max'})\n",
        "    \n",
        "    ### create file path and write out y as a csv\n",
        "    f = os.path.join(OUT_DIR, 'faiss_min_a_' + term.replace(' ', '_') + '.csv')\n",
        "    y.to_csv(f)"
      ],
      "id": "6b2cde89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second criterion: societal benefits\n"
      ],
      "id": "dedccb8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# term_list = ['societal benefits', 'capacity building', 'disaster response', 'water resources', 'agriculture', 'climate resilience', 'health and air quality', 'ecological conservation', 'wildland fires']\n",
        "term_list = ['societal benefits', 'sustainable development', 'environmental resilience', 'resource management']\n",
        "OUT_DIR = 'faiss_out'\n",
        "\n",
        "for term in term_list:\n",
        "    ### term = term_list[0]\n",
        "    \n",
        "    ### Apply the rank_articles function for each term in the list\n",
        "    x = rank_articles(term)\n",
        "    \n",
        "    ### use groupby and aggregate to keep the min distance per citation\n",
        "    y = x.groupby('citation').agg({'dist': 'min'})\n",
        "    # z = x.groupby('citation').agg({'dist': 'max'})\n",
        "    \n",
        "    ### create file path and write out y as a csv\n",
        "    f = os.path.join(OUT_DIR, 'faiss_min_b_' + term.replace(' ', '_') + '.csv')\n",
        "    y.to_csv(f)"
      ],
      "id": "f234aea4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Third criterion: valuation methodology\n"
      ],
      "id": "ec48ece9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "term_list = ['valuation of information', 'decision analysis', 'cost benefit analysis']\n",
        "OUT_DIR = 'faiss_out'\n",
        "\n",
        "for term in term_list:\n",
        "    ### term = term_list[0]\n",
        "    \n",
        "    ### Apply the rank_articles function for each term in the list\n",
        "    x = rank_articles(term)\n",
        "    \n",
        "    ### use groupby and aggregate to keep the min distance per citation\n",
        "    y = x.groupby('citation').agg({'dist': 'min'})\n",
        "    # z = x.groupby('citation').agg({'dist': 'max'})\n",
        "    \n",
        "    ### create file path and write out y as a csv\n",
        "    f = os.path.join(OUT_DIR, 'faiss_min_c_' + term.replace(' ', '_') + '.csv')\n",
        "    y.to_csv(f)"
      ],
      "id": "d83d55ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combined criteria?\n"
      ],
      "id": "9906934c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "term_list = ['value of satellite earth observation data to improve societal benefits']\n",
        "OUT_DIR = 'faiss_out'\n",
        "\n",
        "for term in term_list:\n",
        "    ### term = term_list[0]\n",
        "    \n",
        "    ### Apply the rank_articles function for each term in the list\n",
        "    x = rank_articles(term)\n",
        "    \n",
        "    ### use groupby and aggregate to keep the min distance per citation\n",
        "    y = x.groupby('citation').agg({'dist': 'min'})\n",
        "    # z = x.groupby('citation').agg({'dist': 'max'})\n",
        "    \n",
        "    ### create file path and write out y as a csv\n",
        "    f = os.path.join(OUT_DIR, 'faiss_min_d_' + term.replace(' ', '_') + '.csv')\n",
        "    y.to_csv(f)"
      ],
      "id": "8aacd60d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "gee_env",
      "language": "python",
      "display_name": "gee_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}