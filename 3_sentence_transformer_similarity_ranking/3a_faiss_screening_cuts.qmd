---
title: "Develop screening model based on FAISS outputs"
author: "O'Hara"
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

This script takes the .csv files output by `1_faiss_criteria_query.qmd` and insights from `2_faiss_examine_results.qmd` to develop and train a screening model for the overall citation set.

* For each phrase category (remote sensing, societal benefit, valuation methodology) identify the phrase with the greatest similarity (lowest score) and keep only that.
* Examine how well remaining phrases can predict inclusion vs exclusion on the SBR
    * Try various models: logistic regression, random forest, maybe neural networks
    * Model selection using cross validation in `tidymodels` package
    * Examine success metrics e.g., AUC to identify potential thresholds for exclusion

```{r}
library(tidyverse)
library(ggfortify)   ### for PCA plots
library(here)
library(corrplot)
library(sf)

```

## Load query results

```{r}
faiss_query_fs <- list.files(here('3_sentence_transformer_similarity_ranking', 
                                  'faiss_out'), pattern = 'faiss_min', 
                                  full.names = TRUE)

faiss_q_df <- lapply(faiss_query_fs, FUN = function(f) {
  ## f <- faiss_query_fs[1]
  q <- basename(f) %>% str_remove_all('faiss_min_|.csv')
  df <- read_csv(f, show_col_types = FALSE) %>%
    mutate(query = q) %>%
    select(dist, query, citation)
}) %>%
  bind_rows() %>%
  mutate(criteria = str_extract(query, '^[a-z]'),
         query = str_remove(query, '[a-z]_'))

check <- faiss_q_df %>%
  select(-criteria) %>%
  pivot_wider(values_from = 'dist', names_from = 'query')
```


## Load references from Societal Benefits Repository

We can compare the screened results from the SBR to the distance values here, perhaps via PCA, clustering, logistic regression...

Note: `_data/societal_benefits/sbr_screened_results_240429.csv` is the citation screening results; `_output/societal_benefits/fulltext_screened.csv` is the full text screening results (does not include those excluded in the citation screening).

```{r}
cit_scr_df <- read_csv(here('_data/societal_benefits/sbr_screened_results_240429.csv')) %>%
  mutate(title = str_to_title(title))

fulltext_df <- read_csv(here('_output/societal_benefits/fulltext_screened.csv')) %>%
  ### skip author, match by title only?
  select(title, screening_decision, reason_for_exclusion, notes = `Quick notes`) %>%
  mutate(title = str_to_title(title))

screened_df <- cit_scr_df %>%
  full_join(fulltext_df) %>%
  mutate(decision = ifelse(is.na(screening_decision), screening_status, tolower(screening_decision))) %>%
  mutate(decision = str_replace(decision, 'cluded', 'clude')) %>%
  select(author, title, decision, tags, key)

sbr_vs_faiss <- screened_df %>%
  full_join(faiss_q_df, by = c('key' = 'citation')) %>%
  filter(!is.na(dist))

```

## Examine collinearity

```{r}

sbr_faiss_wide <- sbr_vs_faiss %>%
  filter(!is.na(query)) %>%
  filter(!criteria %in% c('b')) %>% ### drop societal benefits queries?
  select(key, dist, query, decision) %>%
  pivot_wider(names_from = 'query', values_from = 'dist')

sbr_faiss_mtx <- sbr_faiss_wide %>%
  column_to_rownames(var = 'key') %>%
  select(-decision) %>%
  setNames(str_sub(names(.), 1, 10)) %>%
  as.matrix()

corr_mtx <- cor(sbr_faiss_mtx)
corrplot.mixed(corr_mtx)
```

The strength of collinearity is pretty low for most variable pairs, with exception of earth observation, remote sensing, and satellite data, which have higher correlation coefficients with one another and with the full phrase queries.  Let's see what these look like with a PCA.

## Examine relationships via PCA

### PC 1 and 2

QUESTION: Since distances are all on the same unitless scale, should we rescale these distance values?  If a query shows a wide range of distances it seems it should play a greater role in differentiating than a query that reveals only a narrow range of distances.  Here we have disabled rescaling (though have allowed centering for mean zero).

```{r}
# set.seed(42)
sbr_faiss_pca <- sbr_faiss_wide %>%
  select(where(is.numeric)) %>%
  prcomp(scale. = FALSE)

pca_df <- as.data.frame(sbr_faiss_pca$x) %>%
  mutate(decision = sbr_faiss_wide$decision,
         decision = ifelse(is.na(decision), 'not screened', decision),
         decision = factor(decision, levels = c('not screened', 'exclude', 'include'))) %>%
  arrange(decision)

pca_hull_df <- pca_df %>%
  arrange(decision, PC1, PC2) %>%
  group_by(decision) %>%
  slice(chull(PC1, PC2))

### dataframe for loadings; scale up by factor of 5 for plotting
pca_rot_df <- data.frame(sbr_faiss_pca$rotation %>% round(4) * 5) %>%
  mutate(query = rownames(.))

p12 <- ggplot(pca_df, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = decision), alpha = .3) +
  geom_segment(data = pca_rot_df, x = 0, y = 0, 
               aes(xend = PC1, yend = PC2), 
               arrow = arrow(length = unit(.1, 'inches'))) +
  geom_text(data = pca_rot_df, aes(label = query)) +
  geom_polygon(data = pca_hull_df, 
               aes(x = PC1, y = PC2, color = decision, fill = decision), alpha = .1) +
  stat_ellipse(data = pca_df %>% filter(decision == 'include'), color = 'blue')

p12
```

The collinearity at least within the first two principal components is clear in the biplot.  While there is a lot of overlap between the includes and excludes, there's a pretty decent chunk of excluded documents that do not overlap at all, and an even greater cloud of not-yet-screened documents well beyond the group of included documents.  Identifying thresholds to drop those would save a substantial amount of time when proceeding to a manual citation screening. 

Let's examine overlap (in this coordinate system) between ellipses in the first two principal components.
```{r}
p12_build <- ggplot_build(p12)$data
p12_pts <- p12_build[[1]] %>% 
  sf::st_as_sf(coords = c('x', 'y')) %>%
  mutate(id = 1:n())
p12_ell <- p12_build[[5]] %>% sf::st_as_sf(coords = c('x', 'y')) %>% 
  concaveman::concaveman()

p12_in_ell <- sf::st_filter(p12_pts, p12_ell) %>% 
  cbind(st_coordinates(.))
p12_in_ell %>% pull(group) %>% table()

pca_df %>% pull(decision) %>% table()
```

If we keep only observations within the blue ellipse, we include 52/56 "included" (93% recall), 145/201 "excluded", and 7262/13562 yet to be screened.  This reduces the screening burden to 54% of the yet-to-be screened.

### PC 1 and 3

Let's examine between PC 1 and 3:

```{r}
pca_hull_df <- pca_df %>%
  arrange(decision, PC1, PC3) %>%
  group_by(decision) %>%
  slice(chull(PC1, PC3))

pca_rot_df <- data.frame(sbr_faiss_pca$rotation %>% round(4) * 5) %>%
  mutate(query = rownames(.))

p13 <- ggplot(pca_df, aes(x = PC1, y = PC3)) +
  geom_point(aes(color = decision)) +
  geom_segment(data = pca_rot_df, x = 0, y = 0, 
               aes(xend = PC1, yend = PC3), 
               arrow = arrow(length = unit(.1, 'inches'))) +
  geom_text(data = pca_rot_df, aes(label = query)) +
  geom_polygon(data = pca_hull_df, 
               aes(x = PC1, y = PC3, color = decision, fill = decision), alpha = .1) +
  stat_ellipse(data = pca_df %>% filter(decision == 'include'), color = 'blue')


p13
```

Let's examine overlap (in this coordinate system) between ellipses in the first and third principal components.

```{r}
p13_build <- ggplot_build(p13)$data
p13_pts <- p13_build[[1]] %>% 
  sf::st_as_sf(coords = c('x', 'y')) %>%
  mutate(id = 1:n())
p13_ell <- p13_build[[5]] %>% sf::st_as_sf(coords = c('x', 'y')) %>% 
  concaveman::concaveman()

p13_in_ell <- sf::st_filter(p13_pts, p13_ell) %>% 
  cbind(st_coordinates(.))
p13_in_ell %>% pull(group) %>% table()

pca_df %>% pull(decision) %>% table()
```

This drops more "includes" than the PC1/PC2 cut, and leaves in more "yet to be screened" docs.  Combining the two cuts, how do we fare?  Verified that the points are in the same order within the `build` object:

```{r}
p123_cut_df <- p12_build[[1]] %>%
  select(x12 = x, y2 = y, group12 = group) %>%
  cbind(p13_build[[1]] %>%
              select(x13 = x, y3 = y, group13 = group)) %>%
  mutate(id = 1:n()) %>%
  # mutate(mismatch = x12 != x13,
  #        mismatch2 = group12 != group13)
  mutate(p12_in_ell = id %in% p12_in_ell$id,
         p13_in_ell = id %in% p13_in_ell$id) %>%
  mutate(both = p12_in_ell & p13_in_ell)

p123_cut_df %>%
  filter(both) %>%
  pull(group12) %>%
  table()
```

This cut leaves 48/56 "includes" (86% accuracy), 134/201 "excludes", and 6456/13562 (or 48%) "to be screened" (compared to 56% for just the PC1/PC2 cut).  

## Scree plot

```{r}

scree_df <- data.frame(PC = paste0('PC', 1:length(sbr_faiss_pca$sdev)),
                       pct = (sbr_faiss_pca$sdev)^2 / sum((sbr_faiss_pca$sdev)^2)) %>%
  mutate(PC = fct_inorder(PC),
         lbl = paste0(round(pct * 100, 1), '%'),
         cumsum = cumsum(pct))

ggplot(scree_df, aes(x = PC, y = pct)) +
  geom_col() +
  geom_text(aes(label = lbl), vjust = 0, nudge_y = .01)

knitr::kable(pca_rot_df %>% select(PC1:PC4))

```

The screeplot shows that 75% of the variance is captured in the first four principal components.  We don't necessarily need the dimensionality reduction at this point, but removing the collinearity might be helpful if we decide to implement a binary logistic regression model.

