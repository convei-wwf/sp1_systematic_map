---
title: "Examine search term frequencies"
author: "O'Hara"
format: 
  html:
    code-fold: true
    code-summary: "Show me the code"
execute:
  echo: true
  warning: false
  message: false
editor: source
---

```{r setup}
library(tidyverse)
library(here)
library(tidytext)
```

## Load search results

```{r load bibtex}
bib_clean_fs <- list.files(here('bibtex_clean'), pattern = 'wos_', full.names = TRUE)
# bib_clean_fs <- here('bibtex_clean/zot_all.bib')
# bib_clean_fs <- here('bibtex_clean/zot_benchmark.bib')

all_fields_df <- lapply(bib_clean_fs, bib2df::bib2df) %>%
  bind_rows() %>%
  janitor::clean_names()

topic_df <- all_fields_df %>%
  select(author, journal, year, title, abstract, keywords, keywords_plus, web_of_science_categories) %>%
  mutate(title_protect = title) %>%
  pivot_longer(cols = c(title, abstract, keywords, keywords_plus), names_to = 'topic', values_to = 'text') %>%
  rename(title = title_protect) %>%
  mutate(text = tolower(text)) %>%
  filter(!is.na(text))
```

## Examine Web of Science category areas

```{r}
wos_cats_df <- topic_df %>%
  mutate(cat = str_split(web_of_science_categories, '; ')) %>%
  unnest(cat) %>%
  mutate(cat = str_squish(cat)) %>%
  filter(!is.na(cat)) %>%
  group_by(cat) %>%
  summarize(n = n()) %>%
  mutate(cat = fct_reorder(cat, n)) %>%
  arrange(-n)

ggplot(wos_cats_df %>% head(10) %>% mutate(cat = fct_drop(cat))) +
  geom_col(aes(y = cat, x = n)) +
  labs(title = 'top 10 Web of Science categories')

ggplot(wos_cats_df %>% filter(n < 10) %>% mutate(cat = fct_drop(cat))) +
  geom_col(aes(y = cat, x = n)) +
  labs(title = 'Categories with 10 or fewer instances')

```


## Examine keywords

```{r}
wos_kws_df <- topic_df %>%
  filter(topic %in% c('keywords')) %>%
  mutate(kw = str_split(text, '; |, ')) %>%
  unnest(kw) %>%
  mutate(kw = str_squish(kw)) %>%
  filter(!is.na(kw)) %>%
  group_by(kw) %>%
  summarize(n = n()) %>%
  mutate(kw = fct_reorder(kw, n)) %>%
  arrange(-n)

ggplot(wos_kws_df %>% head(20) %>% mutate(kw = fct_drop(kw))) +
  geom_col(aes(y = kw, x = n)) +
  labs(title = 'top 10 Web of Science keywords')

# wos_kws_df %>% filter(n == 1) %>% DT::datatable()

```

## Scan topic for Earth Science Information terms

Web of Science search string for this domain are:

"satellite" OR "space-based" OR "remote observation" OR "remote sensing" OR "earth observation" OR "remotely sens*" OR "modis" OR "landsat"

```{r define function to compress keywords}
compress_keywords <- function(kw) {
  kw_compressed <- str_remove_all(kw, '"') %>% ### drop quotation marks
    str_replace_all('[^a-zA-Z *]', '.?') %>%   ### swap out punctuation for zero or one-space regex match
    str_replace_all(' OR ', '|') %>%           ### swap out OR statements with or operator    
    str_replace_all('\\*', '[a-z]*')           ### replace asterisk with regex match for one or more letters
  return(kw_compressed)
}
```


```{r extract esi terms}
esi_key_raw <- '"satellite" OR "space-based" OR "remote observation" OR "remote sensing" OR "earth observation" OR "remotely sens*" OR "modis" OR "landsat"'
esi_key <- compress_keywords(esi_key_raw)
esi_term_df <- topic_df %>%
  mutate(term = str_extract_all(text, esi_key)) %>%
  unnest(term)
```

This documents how many distinct papers (lit search results) note the use of each given term.  Each term is only counted once per document per topic field - i.e., if "satellite" shows up three times in an abstract, it will only be counted once.

```{r function to tally frequency of terms}
tally_results <- function(term_df) {
  term_freq_df <- term_df %>%
    select(author, year, title, topic, term) %>%
    distinct() %>%
    group_by(term, topic) %>%
    mutate(freq_term = n()) %>%
    ungroup() %>%
    mutate(term = fct_reorder(term, freq_term))
  term_count_df <- term_df %>%
    group_by(author, year, title, topic) %>%
    summarize(n_terms = n_distinct(term))
  return(list('freq' = term_freq_df, 'count' = term_count_df))
}
```


```{r plot esi terms}
esi_p_list <- tally_results(esi_term_df)
ggplot(esi_p_list$freq, aes(y = term)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Expanded search term')

ggplot(esi_p_list$count, aes(y = n_terms)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Number of distinct terms')
  
```


## Scan topic for Decision Making Context terms

Web of Science search string for this domain are:

"decision" OR "optimization" OR "risk analysis" OR "management" OR "policy" OR "cost-benefit analysis" OR "benefit-cost analysis" OR "investment" OR "contingent valuation" OR "counterfactual"

```{r extract decision terms}
decision_key_raw <- '"decision" OR "optimization" OR "risk analysis" OR "management" OR "policy" OR "cost-benefit analysis" OR "benefit-cost analysis" OR "investment" OR "contingent valuation" OR "counterfactual"'
decision_key <- compress_keywords(decision_key_raw)
decision_term_df <- topic_df %>%
  mutate(term = str_extract_all(text, decision_key)) %>%
  unnest(term)
```


```{r plot decision terms}
decision_p_list <- tally_results(decision_term_df)
ggplot(decision_p_list$freq, aes(y = term)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Expanded search term')

ggplot(decision_p_list$count, aes(y = n_terms)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Number of distinct terms')
```



## Scan topic for Value terms

Web of Science search string for this domain are:

"value" OR "valuation" OR "benefit" OR "utility"

```{r extract value terms}
value_key_raw <- '"value" OR "valuation" OR "benefit" OR "utility"'
value_key <- compress_keywords(value_key_raw)
value_term_df <- topic_df %>%
  mutate(term = str_extract_all(text, value_key)) %>%
  unnest(term)
```


```{r plot value terms}
value_p_list <- tally_results(value_term_df)
ggplot(value_p_list$freq, aes(y = term)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Expanded search term')

ggplot(value_p_list$count, aes(y = n_terms)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Number of distinct terms')
```



## Scan topic for Societal Context terms

Web of Science search string for this domain are:

"social" OR "societal" OR "cultural" OR "*economic" OR "environmental" OR "ecosystem service" OR "sustainable development goal" OR "protected area" OR "heritage site" OR "non use value"

```{r extract social terms}
societal_key_raw <- '"social" OR "societal" OR "cultural" OR "*economic" OR "environmental" OR "ecosystem service" OR "sustainable development goal" OR "protected area" OR "heritage site" OR "non use value"'
societal_key <- compress_keywords(societal_key_raw)
societal_term_df <- topic_df %>%
  mutate(term = str_extract_all(text, societal_key)) %>%
  unnest(term)
```


```{r plot social terms}
societal_p_list <- tally_results(societal_term_df)
ggplot(societal_p_list$freq, aes(y = term)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Expanded search term')

ggplot(societal_p_list$count, aes(y = n_terms)) +
  geom_bar() +
  facet_wrap(~ topic, scales = 'free_x') +
  theme_minimal() +
  labs(x = 'Search result count', y = 'Number of distinct terms')
```

## Extract bigrams of terms

To look for inappropriate results, examine bigrams of occurrences of search terms.  For example, "satellite" may be used in context of "satellite offices" of hospitals etc.  For now, this will focus on single-word terms, not multi-word terms (e.g., not "cost-benefit analysis") as those are already reasonably specific...

```{r}
all_key <- paste(decision_key, esi_key, value_key, societal_key, sep = '|')
all_terms <- all_key %>%
  str_split('\\|') %>% 
  unlist()

single_terms <- all_terms[!str_detect(all_terms, '[^a-z]')]
single_key <- paste(single_terms, collapse = '|')

bigram_df <- topic_df %>%
  ### break into sentences or clauses based on periods, semicolons, and commas
  mutate(text2 = str_split(text, '[;,.]')) %>%
  unnest(text2) %>%
  mutate(text2 = str_squish(text2)) %>%
  ### break clauses into bigrams
  unnest_tokens(input = text2, output = bigram, token = 'ngrams', n = 2) %>%
  filter(str_detect(bigram, single_key))

bigram_clean_df <- bigram_df %>%
  separate(bigram, into = c('first', 'second'), sep = ' ') %>%
  ### because one of the two terms is a keyword, antijoin on stop words in
  ### each column to drop unhelpful bigrams
  anti_join(stop_words, by = c('first' = 'word')) %>%
  anti_join(stop_words, by = c('second' = 'word')) %>%
  unite(bigram, first, second, sep = ' ')

bigram_summary <- bigram_clean_df %>%
  filter(!str_detect(bigram, '[0-9]')) %>%
  group_by(bigram) %>%
  summarize(n = n()) %>%
  mutate(term = str_extract(bigram, single_key))
```

