---
title: 'Systematic Map: consolidate records, deduplicate, and prescreen'
format: 
  html:
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
editor: source
---

```{r setup}
library(tidyverse)
library(tidytext)
library(stringi)
library(synthesisr)
library(here)
```

# Summary

This script will ingest Bibtex files of records and:

* check for and resolve duplicated records
* from Web of Science, Scopus, the USGS Societal Benefits Repo, and the CONVEI Zotero benchmarks library

# Methods

## Remove duplicates and save out for Colandr ingestion

Pull in all preprocessed citation data.  Identify duplicates by first author and title.  Save only one version (Societal Benefits $\succ$ Web of Science $\succ$ Scopus).

```{r read in the preprocessed WoS citations}
wos_fs <- list.files(here('_data/2_refs_preprocessed'), pattern = 'wos_', full.names = TRUE)
wos_all <- parallel::mclapply(wos_fs, read_refs, mc.cores = 1) %>% 
  bind_rows() %>% 
  mutate(src = 'wos') 

wos_date <- str_extract(basename(wos_fs), '[0-9]{6}') %>% unique()

wos_distinct <- wos_all %>%
  select(source_type = type, src, author, 
         title, journal, year, abstract, doi,
         contains('keywords'),
         research_areas, web_of_science_categories) %>%
  distinct()
```

```{r read in the preprocessed Scopus references}
scopus_fs <- list.files(here('_data/2_refs_preprocessed'), pattern = 'scopus_', full.names = TRUE)

scopus_date <- str_extract(basename(scopus_fs), '[0-9]{6}') %>% unique()

system.time({
  ### running on Windows: set mc.cores = 1 (can't handle mclapply!)
  scopus_all <- parallel::mclapply(scopus_fs, read_refs, mc.cores = 1) %>% 
    bind_rows() %>%
    mutate(src = 'scopus') 
})
#    user  system elapsed 
# 660.872   9.182 144.292

scopus_distinct <- scopus_all %>%
  select(source_type, src, author, title, journal = source, year, abstract, doi,
         contains('keywords')) %>%
  distinct()
```

```{r read in the preprocessed Soc Benefits Repo references}
sbr_fs <- list.files(here('_data/2_refs_preprocessed'), pattern = 'societal_benefits_', full.names = TRUE)

sbr_date <- str_extract(basename(sbr_fs), '[0-9]{6}') %>% unique()

sbr_all <- read_refs(sbr_fs) %>% 
  mutate(src = 'societal benefits repo') 

sbr_distinct <- sbr_all %>%
  select(source_type, src, author, title, journal, year, abstract, doi,
         contains('keywords')) %>%
  distinct()
```


```{r read in benchmark citations}
benchmark_all <- read_refs(here('_data/2_refs_preprocessed/zot_benchmark_a.bib')) %>%
  mutate(src = 'benchmark') %>%
  mutate(title = str_remove_all(title, '\\{\\{|\\}\\}')) %>%
  select(source_type = type, src, author, 
       title, journal, year, abstract, doi,
       contains('keywords'))
```

## Clean up

```{r combine and clean up some messy columns}
all_docs <- bind_rows(scopus_distinct, wos_distinct, sbr_distinct, benchmark_all) %>%
  filter(!is.na(author) & !is.na(title) & !is.na(abstract)) %>%
  mutate(doc_id = 1:n()) 

all_docs_clean <- all_docs %>%
  mutate(first_author = str_remove(tolower(author), ' and .+') %>%
           stri_trans_general('Latin-ASCII') %>% ### drop diacritics
           str_remove('(?<=(, [a-z])).+'),
         title = str_to_title(title) %>%
           str_remove_all('\\<.+?\\>'), ### drop HTML tags
         title_short = tolower(title) %>% 
           stri_trans_general('Latin-ASCII') %>% ### drop diacritics
           str_remove_all('[^a-z0-9 ]+') %>% 
           str_squish() %>% 
           str_sub(1, 100),
         abstr_short = tolower(abstract) %>% 
           stri_trans_general('Latin-ASCII') %>% ### drop diacritics
           str_remove_all('[^a-z0-9 ]+') %>% 
           str_squish() %>% 
           str_sub(1, 100),
         journal = str_to_title(journal),
         year = str_extract(year, '[0-9]{4}') %>% as.numeric()) %>%
  mutate(doi = tolower(doi),
         source_type = tolower(source_type))
```

## Check benchmark papers

```{r}
bench_check <- all_docs_clean %>%
  group_by(first_author, title, year) %>%
  filter('benchmark' %in% src) %>%
  summarize(n = n(), src = paste(src, collapse = ';'), .groups = 'drop')
  

DT::datatable(bench_check)
```

As of Feb. 9, 2024, 13 of 18 papers show up in both WoS and Scopus.  One shows up only in Scopus, but is known to not be available in WoS with direct search.  Four do not show up in either dataset, all known to not be available in either source with direct search.  Therefore, 100% retrieval rate of all papers known to be in each source.  Nine of the papers show up in the Societal Benefits repository, including one white/grey lit that did not show up in either Scopus or WoS.

## Filter out duplicates

Here we discard duplicated references, keeping the most recent version (if multiple years) that also contains a DOI (if any).

* Drop any observations of conference proceedings and listings where no DOI is present.
* Identify citations with multiple DOIs and try to disambiguate.
* Papers found in the Benchmark set are tagged, then benchmark observations removed (keeping observations from other sources).
* If a citation is found in multiple (non-benchmark) sources, keeping Societal Benefits Repo $\succ$ Web of Science $\succ$ Scopus.  

### Examine multi-DOI occurrences

```{r}
all_docs_distinct <- all_docs_clean %>%
  filter(!source_type %in% c('conf', 'inproceedings')) %>%
  group_by(first_author, title_short, abstr_short) %>%
  ### flag any author/title combos that show up in the benchmark set,
  ### then drop those benchmark observations (keeping obs in other sources)
  mutate(benchmark = any(src == 'benchmark')) %>%
  filter(src != 'benchmark') %>%
  mutate(n = n(),
         n_doi = sum(!is.na(doi)),
         n_dist_doi = n_distinct(doi[!is.na(doi)])) %>%
  ungroup()


multi_doi <- all_docs_distinct %>%
  filter(n_dist_doi > 1)
```

* Some of these multi-DOI articles are due to published articles being included in a collection (with a different DOI).  In which case, make sure there is at least one `jour` or `article` observation and then drop the `incollection` observations.
* Other apparent multi-DOI observations are due to extras in the DOI column: non-DOI information (thanks Italy?), or `https://` information.  In which case, strip down to just the DOI number.  From [https://support.datacite.org/docs/doi-basics](https://support.datacite.org/docs/doi-basics):
    * All DOIs have a prefix starting with `10.` followed by a number, then a slash, then additional info.
    * Other characters in the suffix can be letters, numbers, and punctuation (and are not case sensitive).
    * Reserved characters not allowed in the suffix: `;`, `/`, `?`, `:`, `@`, `&`, `=`, `+`, `$`, `,`.
    * Some DOIs have multi-part suffixes separated by a slash (despite the above).

```{r test doi cleaning code}
#| eval: false

multi_doi_check <- multi_doi %>%
  select(source_type, src, first_author, title_short, abstr_short, year, journal, doi, n_dist_doi) %>%
  group_by(first_author, title_short, abstr_short) %>%
  ### test the first case
  mutate(drop_collection = any(source_type %in% c('jour', 'article')) & source_type == 'incollection') %>%
  ### three instances...
  ungroup() %>%
  ### test the second case: prefix, and any suffix up to a space
  mutate(doi = str_remove(doi, 'https?://([a-z]+\\.)?doi.org/'),
         doi_clean = str_extract(doi, '10.[0-9]+/.+') %>% str_remove(' .+')) %>%
  mutate(doi_match = doi == doi_clean)

x <-  multi_doi_check %>%
  filter(!drop_collection) %>%
  filter(!doi_match)
### all reasonable!

y <- multi_doi_check %>%
  filter(!drop_collection) %>%
  group_by(first_author, title_short, abstr_short) %>%
  mutate(n_dist_doi = n_distinct(doi_clean)) %>%
  ungroup()
### still some bad characters: backslashes, curly braces
z <- y %>%
  mutate(doi_clean = str_remove_all(doi_clean, '\\\\|\\{|\\}')) %>%
  group_by(first_author, title_short, abstr_short) %>%
  mutate(n_dist_doi = n_distinct(doi_clean)) %>%
  ungroup()
### At this point things look difficult to resolve automatically
```

After addressing the above issues, a few articles still have apparent differences in DOI, some small, some major.  At this point we will simply choose one and hope for the best.

```{r}
single_doi <- all_docs_distinct %>%
  filter(n_doi > 0) %>%       ### drop citations with NA DOI across all observations
  filter(n_dist_doi == 1) %>% ### keep only citations with a single (non-NA) DOI (or URL)
  filter(!is.na(doi))         ### then drop the remaining NA observations (which have a valid DOI elsewhere)

multi_doi_resolved <- multi_doi %>%
  arrange(title_short, abstr_short, desc(year), desc(src)) %>%
  group_by(first_author, title_short, abstr_short) %>%
  ### drop the first case, incollection with article elsewhere:
  filter(!(any(source_type %in% c('jour', 'article')) & source_type == 'incollection')) %>%
  ### remove second case, doi cruft:
  mutate(doi = str_remove(doi, 'https?://([a-z]+\\.)?doi.org/'), ### drop url prefix stuff
         doi = str_extract(doi, '10.[0-9]+/.+') %>%              ### extract DOI plus trailing
           str_remove_all(' .+|\\\\|\\{|\\}')) %>%               ### drop trailing info and \, {, }
  mutate(n_dist_doi = n_distinct(doi)) %>%
  ungroup()
  
resolved_auth_title_abstr_doi <- multi_doi_resolved %>% 
  bind_rows(single_doi) %>%
  ### Factor the src column according to preference relation
  mutate(src = factor(src, levels = c('societal benefits repo', 'wos', 'scopus'))) %>%
  ### arrange by descending year (prefer recent articles) and ascending categorical
  ### source (i.e., prefer soc benefits to wos to scopus)
  group_by(first_author, title_short, abstr_short) %>%
  arrange(title_short, abstr_short, desc(year), src) %>%
  ### then take just the top observation for this author/title/abstract
  slice(1) %>%
  ungroup() %>%
  select(-n, -n_doi, -n_dist_doi) %>%
  select(-first_author, -title_short, -abstr_short)
```

At this point we've disambiguated near DOI matches (by looking at author, title, abstract) and dropped repeats.  Then we have removed instances where an article showed up in multiple sources: there are 5,606 articles common to Web of Science and Scopus.  Even still, there remain duplicated DOIs, that weren't caught due to mismatches in short title, short abstract, or author order.

```{r}
doi_still_dupes <- resolved_auth_title_abstr_doi %>%
  filter(!is.na(doi)) %>%
  janitor::get_dupes(doi) 

doi_dupes_to_keep <- doi_still_dupes %>%
  group_by(doi) %>%
  slice(1) %>%
  ungroup() %>%
  select(-dupe_count)

resolved_remaining_dupes <- resolved_auth_title_abstr_doi %>%
  filter(!doi %in% doi_dupes_to_keep$doi | is.na(doi)) %>%
  bind_rows(doi_dupes_to_keep)

```

## Filter out spurious matches

In the beta round of screening, a number of spurious search term matches came up:

* "sentinel": 
    * epidemiology/medicine: study, [lymph] node, site, laboratory, [influenza] surveillance, catalyst, event
    * ecology: species, behavior
* "satellite": account (re: tourism), office, clinic
* "grace": period
* "terra": preta, nova, firme, nullius

Many of these spurious matches relate to cancer or cardiovascular health, unlikely to be related to earth observation, so let's add in 'cancer' and 'cardiac'/'cardiovascular' as potential for spurious matches.

Examine the instances of these in the larger database and drop them unless other valid instances are found (e.g., some cancer studies look at PM2.5 that can potentially be observed using remote sensing).

```{r}
sentinel_terms <- paste('sentinel', 
                        c('study', '(lymph.)?node', 'site', '([a-z]+.)?surveillance', 
                          'species', 'behavior', 'catalyst', 'event'), 
                        sep = '.', collapse = '|')
satellite_terms <- paste('satellite', 
                         c('account', 'office', 'laborator(y|ies)', 'campus',
                           '([a-z]+.)?clinic', '([a-z]+.)?hospital', '([a-z]+.)?cent(er|re)',
                           'lesion', 'nodule', 'mass', 'h(a)?emodialysis'), 
                         sep = '.', collapse = '|')
grace_terms <- 'grace.period'
terra_terms <- paste('terra', c('preta', 'nova', 'firme', 'nullius'), 
                     sep = '.', collapse = '|')
health_terms <- 'cancer|cardiac|cardio'

all_terms <- paste(c(sentinel_terms, satellite_terms, grace_terms, terra_terms, health_terms), collapse = '|')

spurious_match <- resolved_remaining_dupes %>%
  filter(str_detect(tolower(title), all_terms) | str_detect(tolower(abstract), all_terms))

### remove "sentinel" as valid match with these spurious matches
valid_terms <- 'satellite|space.based|remote(ly)? observ[a-z]+|earth observation|remote(ly)?.sens[a-z]+|modis|landsat'

### remove the spurious match terms and see if there are any remaining references to satellites/earth observation
spurious_check <- spurious_match %>%
  select(author, year, journal, title, abstract) %>%
  mutate(title2 = str_remove_all(tolower(title), all_terms),
         abstract2 = str_remove_all(tolower(abstract), all_terms)) %>%
  filter(str_detect(title2, valid_terms) | str_detect(abstract2, valid_terms)) %>%
  mutate(title_match = str_extract(title2, valid_terms), abstr_match = str_extract(abstract2, valid_terms))

# x <- spurious_check %>%
#   unnest_tokens(abstr2, abstract2, 'ngrams', n = 3) %>%
#   filter(str_detect(abstr2, '^satellite'))
# x$abstract[43]
### Many of these are still spurious, but the term "satellite" is not immediately
### connected to many useful, unambigious terms that can uniquely identify a spurious
### match...

spurious_match_clean <- spurious_match %>%
  anti_join(spurious_check)

spurious_dropped <- resolved_remaining_dupes %>%
  anti_join(spurious_match_clean)

```



## Write out by source

Write out as .ris for easier import into Colandr.  Add a column with the key, 

```{r write out cleaned bibliographies}

refs_almost_ready <- spurious_dropped %>%
  select(where(~ any(!is.na(.)))) %>%
  ### drop agency acronyms
  mutate(author = str_remove(author, '\\[.+\\]'))

refs_lookup <- refs_almost_ready %>%
  select(author, title, journal, year, doi, src) %>%
  mutate(auth_short = author %>% 
           stri_trans_general('Latin-ASCII') %>% 
           tolower() %>%
           str_remove(', .+') %>% 
           str_remove_all('[^a-z]')) %>%
  unnest_tokens(output = title_words, input = title, token = 'words', drop = FALSE) %>%
  anti_join(stop_words, by = c('title_words' = 'word')) %>%
  group_by(author, auth_short, title, journal, year, doi, src) %>%
  slice(1:4) %>%
  summarize(title_short = paste(title_words, collapse = ''), .groups = 'drop') %>%
  mutate(key = paste(auth_short, title_short, year, sep = '_')) %>%
  select(key, author, title, journal, year, doi, src) %>%
  group_by(key) %>%
  arrange(src) %>%
  slice(1) %>%
  ungroup()

still_dupes <- janitor::get_dupes(refs_lookup, key)
### these all look like true dupes due to mismatches in title 
  
### Remove spurious dupes by just slicing the first instance and going with it
refs_lookup <- refs_lookup %>%
  group_by(key) %>%
  slice(1) %>%
  ungroup()

refs_ready <- refs_almost_ready %>%
  inner_join(refs_lookup) %>%
  select(-doc_id, notes = key)

clean_scopus <- refs_ready %>%
  filter(src == 'scopus') %>% select(-src) %>%
  ### write_refs can't handle tbl?
  as.data.frame() %>%
  select(where(~ any(!is.na(.))))

clean_wos <- refs_ready %>%
  filter(src == 'wos') %>% select(-src) %>%
  ### write_refs can't handle tbl?
  as.data.frame() %>%
  select(where(~ any(!is.na(.))))

clean_sbr <- refs_ready %>%
  filter(src == 'societal benefits repo') %>% select(-src) %>%
  ### write_refs can't handle tbl?
  as.data.frame() %>%
  select(where(~ any(!is.na(.))))

write_csv(refs_lookup, here('_data/3_refs_clean/ref_key_lookup.csv'))

write_refs(clean_scopus, format = 'ris',
           file = sprintf(here('_data/3_refs_clean/scopus_clean_%s.ris'), 
                          scopus_date))
write_refs(clean_wos,    format = 'ris',  
           file = sprintf(here('_data/3_refs_clean/wos_clean_%s.ris'), 
                          wos_date))
write_refs(clean_sbr,    format = 'ris',  
           file = sprintf(here('_data/3_refs_clean/sbr_clean_%s.ris'), 
                          sbr_date))
``` 
 
## Save out a sample of documents for modeling

This sample excludes benchmarks and societal benefits repo papers.

```{r}
sample_f <- here('_data/3_refs_clean/sample/sample_1000_240708.ris')

if(!file.exists(sample_f)) {
  set.seed(2001)
  
  sample_1000 <- refs_ready %>%
    filter(src != 'societal benefits repo') %>%
    filter(!benchmark) %>%
    slice_sample(n = 1000) %>% 
    select(-src) %>%
    ### write_refs can't handle tbl?
    as.data.frame() %>%
    select(where(~ any(!is.na(.))))
    
  write_refs(sample_1000, format = 'ris',
             file = sample_f)
}
```

