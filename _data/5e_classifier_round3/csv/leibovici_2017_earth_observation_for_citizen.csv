page,p
1,"data article earth observation for citizen science validation, or citizen science for earth observation validation?"
1,the role of quality assurance of volunteered observations didier g.
1,"leibovici 1, * id , jamie williams 2 id , julian f."
1,"rosser 1 , crona hodges 3 , colin chapman 4 , chris higgins 5 and mike j."
1,"jackson 1 1 nottingham geospatial science, university of nottingham, nottingham ng7 2tu, uk; julian.rosser@nottingham.ac.uk (j.f.r.); mike.jackson@nottingham.ac.uk (m.j.j.)"
1,"2 environment systems ltd., aberystwyth sy23 3ah, uk; jamie.williams@envsys.co.uk 3 earth observation group, aberystwyth university penglais, aberystwyth sy23 3jg, uk; crona.hodges@geosmartdecisions.co.uk 4 welsh government, aberystwyth sy23 3ur, uk; colin.chapman@wales.gsi.gov.uk 5 edina, university of edinburgh, edinburgh eh3 9dr, uk; chris.higgins@ed.ac.uk * correspondence: didier.leibovici@nottingham.ac.uk received: 28 august 2017; accepted: 19 october 2017; published: 23 october 2017 abstract: environmental policy involving citizen science (cs) is of growing interest."
1,"in support of this open data stream of information, validation or quality assessment of the cs geo-located data to their appropriate usage for evidence-based policy making needs a flexible and easily adaptable data curation process ensuring transparency."
1,"addressing these needs, this paper describes an approach for automatic quality assurance as proposed by the citizen observatory web (cobweb) fp7 project."
1,"this approach is based upon a workflow composition that combines different quality controls, each belonging to seven categories or “pillars”."
1,each pillar focuses on a specific dimension in the types of reasoning algorithms for cs data qualification.
1,these pillars attribute values to a range of quality elements belonging to three complementary quality models.
1,"additional data from various sources, such as earth observation (eo) data, are often included as part of the inputs of quality controls within the pillars."
1,"however, qualified cs data can also contribute to the validation of eo data."
1,"therefore, the question of validation can be considered as “two sides of the same coin”."
1,"based on an invasive species cs study, concerning fallopia japonica (japanese knotweed), the paper discusses the flexibility and usefulness of qualifying cs data, either when using an eo data product for the validation within the quality assurance process, or validating an eo data product that describes the risk of occurrence of the plant."
1,both validation paths are found to be improved by quality assurance of the cs data.
1,"addressing the reliability of cs open data, issues and limitations of the role of quality assurance for validation, due to the quality of secondary data used within the automatic workflow, are described, e.g., error propagation, paving the route to improvements in the approach."
1,keywords: citizen science; volunteered geographical information; metadata; data quality; quality assurance; scientific workflow; provenance; metaquality; open data 1.
1,"introduction robust and fit-for-purpose evidence is at the heart of environmental policy and decision making in the uk government, as shown by the department for environment, food and rural affairs (defra) in their evidence strategy [1]."
1,"exploring the combined use of innovative technologies, such as earth observation (eo) and citizen science (cs), for supporting various environmental policy areas [1], data 2017, 2, 35; doi:10.3390/data2040035 www.mdpi.com/journal/data"
2,"data 2017, 2, 35 2 of 20 is a consequence of this momentum."
2,"one example of this is detecting, mapping and monitoring the spread of invasive non-native species (inns), such as fallopia japonica (japanese knotweed)."
2,"the total annual cost of the japanese knotweed (jkw hereafter) to the british economy is estimated at £166 million [2], therefore optimizing the use and potential of any data capture methodology is widely encouraged."
2,"cs is not a new phenomenon [3–5], and is not limited to geographical information."
2,"nonetheless, the combined effect of the ubiquity and increasing capabilities of mobile phone technologies with the rise of geospatial applications in everyday life, has propelled cs into an era of geo-savvy people accustomed to map mashups and a myriad of location based services."
2,"sometimes referred to as “geographic” cs [6], and used interchangeably with the term “volunteered geographic information” (vgi) [7,8], this kind of cs is relatively new and is ever increasing in the range of applications, especially in the environmental monitoring space."
2,"there are many cs projects already in action as well as software platforms to create templates and forms that can be used to collect field data with mobile applications or “apps” [9,10]."
2,"it is a growing area of research, with a need for a common standards-based framework for uploading and distributing cs data [11]."
2,"the eu funded framework programme seven (fp7) citizen observatory web (cobweb) project used a co-design approach, engaging with stakeholders at multiple levels (“grass roots” cs practitioners, through to policy makers), to develop a research e-infrastructure that could be used to create, manage, validate and disseminate geospatial information rapidly and in a standardized way [12]."
2,"to better understand the benefits of such an e-infrastructure and the data it curates for the end users (citizens and policy makers alike), further research into the potential of qualified cs data in combination with eo land cover and habitat monitoring data products is required [13]."
2,"examples where cs or crowdsourced data have contributed to applications in eo have largely relied on data that have been sourced from platforms such as mechanical turk, geograph or even openstreetmap [8], or where gamification has attracted large numbers of remote volunteers [14,15]."
2,"however, there are fewer examples in the literature of in situ data gathered by volunteers on the ground being utilized for the validation of eo derived land cover or habitat products [13,16]."
2,"recent research [17] demonstrates that data collected by volunteers for this purpose can be useful, despite general concerns about data quality, and therefore contribute to ecological monitoring and inventory [18,19]."
2,"nonetheless, information of quality is needed for cs data, to give confidence in their re-use, and provide a rich evidence base for policymaking."
2,"for both in situ cs data collection and web-based crowdsourcing, e.g., geo-wiki [20], data quality has long been identified as the crucial challenge for re-use of cs or vgi data [21–26], and is still reportedly a key concern [27,28]."
2,"the mapping of ecological habitat types using satellite imagery is a rich and active research area but arguably this still remains difficult to scale up into fully operational campaigns [29,30], due to the need for in situ data."
2,"eo has provided data suitable for the mapping of inns several years [31], though not without some difficulties [32]."
2,"for example, other vegetation of similar spectral reflectance characteristic can be found surrounding, above or beneath the canopy of the intended target inns."
2,"furthermore, the cost of high resolution imagery required to map certain scales of the stands, and with the need of multiple images over the year to detect the phenological differences in the inns, to that of other vegetation, leads to an accumulation of cost to the end user and becomes nonsensical too."
2,"however, with the recent availability the copernicus space program providing free and open satellite data, with spatial resolutions of 10 m in the optical region, it is likely that an increased number of applications of eo for habitat monitoring, and related policy development, are likely to follow."
2,"the use of eo to explore the extent of inns in europe (also referred to as invasive alien species (ias) in combination with cs based data collection, allow projects to provide validation data to input into distribution models or habitat maps, which is recognized as an exciting new research area [11,33]."
2,"the cobweb project contributed significantly in this field, from demonstrating how the use of a flexible, standards-compliant infrastructure that offers quality assurance data curation processes, was enabling data conflation of multiple data sources, including eo [12]."
3,"data 2017, 2, 35 3 of 20 quality assurance (qa) can be defined as a set of data policies, controls, and tests put in place in order to meet specific requirements, measured from a series of quality metrics."
3,"few directions or methods on how to qualify cs and vgi data have been expressed [22,34–36]."
3,"in the context of cs, the quality controls (qcs) can be related to both the design of data capture tools a priori to a qa procedure, and to (geo)computational operations that output quality values according to particular measures, either during data capture or a posteriori to the data capture (a posteriori qa)."
3,"selection of the qcs, which feed into the qa procedure, is often decided with future data usage in mind (fitness for purpose) [26]."
3,"the process of “verification”, such as that used by the nbn: national biological network, uk, is a common practice, and is primarily used as a definitive way of assessing the data quality."
3,"this involves manual verification by an expert of each observation, e.g., verifying the content of a photo that has been given as evidence of an invasive species occurrence."
3,this is an inefficient and not scalable (ineffective for large numbers of observations) qualifying method for cs.
3,"within the above context, this paper addresses the challenge of validating cs data against eo data, or vice versa, and in each case, demonstrates the choices made and the important role that the qualifying system can perform in increasing the potential of cs data re-use."
3,"this paper goes on to comprehensively describe the purpose of the cobweb e-infrastructure, in specific relation to qa, expanding upon previous work [26,36] (section 2)."
3,"then, applicability of qa is analyzed using an inns jkw case study based in the snowdonia national park, in wales, uk (section 3)."
3,"qa case study background the cs data for this case study were collected during the summer of 2015 in snowdonia national park (snp), as part of a co-design project within the cobweb project."
3,"the specific cobweb survey form used by 34 citizens to report jkw occurrences was generated by the snp cs coordinator, using a web portal, and distributed to the citizens and snp representatives, using a mobile application [12]."
3,"the survey sample, limited to the snp representatives, consisted of 177 points of declared jkw occurrences, with an average positional accuracy of up to 13 m (the positional accuracy measure being the 68% circular error also corresponding to 1 standard deviation)."
3,"for ground truth comparisons, verifications of the photos identified 16 incorrect declarations (<10%) of jkw."
3,"the eo data product considered here is a vector layer product derived from a clustering algorithm including rules using colored infra-red (cir) and light detection and ranging (lidar) images, predicting the likelihood or risk of occurrence of jkw."
3,"each polygon in the eo derived product contains the risk of occurrence of jkw, categorized as “no-risk”, “low risk”, “medium risk” and “high-risk”."
3,"the eo data product consisted of a total of 61,288 features, with 46,356 polygons attributed as “high” and “medium” risk, and 14,932 polygons attributed as “low” or “no” risk zones."
3,"henceforth, when referring to “eo data” within this text, it will refer to the eo data product estimating the risk of occurrence of jkw."
3,"the eo data product was acquired under a commercial contract between environment systems ltd. and the welsh government, with a non-disclosure agreement allowing access for the purposes of this study."
3,requests to environment systems for additional information may be considered on a case-by-case basis.
3,"in terms of estimating the spread of jkw within the snp, both cs and eo data only give partial insight into jkw distribution, due to their known limitations on quality."
3,"these limitations are either due to coverage and potential misjudged records from the citizens, or algorithm performance metrics for the eo data generation."
3,"nonetheless, coherence of these two data sources should enable the validation of one data source with the other, depending on the level of confidence that is accepted."
3,"if the cs data were of very high quality, this ground truth dataset would allow estimations of the accuracy of the eo data (section 4), and if the eo data were highly representative of jkw within the snp, it would alone enable the qualification of the cs data with a high confidence (section 5)."
3,"if both were of high quality, the discrepancy in accuracies would be attributable only to the lag in data acquisition periods, and differential growth of the jkw (section 4.1)."
3,"as within this case study, both datasets cannot be considered of high quality, therefore, the question posed here is, can their combined use be more informative and or offer an increase in quality?"
3,the greater the quality of the cs
4,"data 2017, 2, 35 4 of 20 data 2017, 2, 35 4 of 20 data, the better the eo data accuracy can be evaluated (sections 4.2 and 4.3)."
4,"conversely, the greater the eo data,accuracy, csdata the betterthe eo data thegreater accuracy can confidence be evaluated (metaquality) be given4.2 can(sections toand the 4.3)."
4,"conversely, eo product the within greater the eo data accuracy, the greater confidence (metaquality) can be given the qa to qualify cs data (section 5)."
4,"from iso19157, metaquality information (at dataset level) is to the eo product withinhow describing the the to qualify qa quality wascs data (section obtained 5)."
4,"from iso19157, and eventually its variation across theinformation metaquality dataset. (at dataset level) is describing how the quality was obtained and eventually its variation across the dataset."
4,quality assurance and quality control framework 2.
4,"quality assurance and quality control framework the cobweb project aimed at designing and building a generic interoperable e-infrastructure facilitatingthe collectionproject thecobweb aimed atofdesigning and curation cs data for andfuture buildingusagea generic interoperablemonitoring e-infrastructure in environmental [12]. facilitating the collection and curation of cs as part of this e-infrastructure, qa plays an important role, either during or after data capture."
4,"a [12]. data for future usage in environmental monitoring qa as part of this e-infrastructure, qa plays an important role, either during or after data capture."
4,"a qa procedure is designed using the qa workflow authoring tool (qawat), a web interface of the qa procedure is designed using the qa workflow authoring tool (qawat), a web interface of the qa framework."
4,it is based upon a workflow editor that uses the business process modelling notation framework.
4,"it is based upon a workflow editor that uses the business process modelling notation (bpmn) standard, a standard for graphical notation for specifying business processes (similar to a flow (bpmn) standard, a standard for graphical notation for specifying business processes (similar to a chart) www.bpmn.org."
4,"qawat enables the selection of qc tests by the stakeholder, which are then flow chart) www.bpmn.org."
4,"qawat enables the selection of qc tests by the stakeholder, which are combined and chained to form a qa procedure for their cs survey [37,38]."
4,"to ensure interoperability, then combined and chained to form a qa procedure for their cs survey [37,38]."
4,"to ensure the qcs are implemented interoperability, as web the qcs processing service are implemented as web (wps) processingprocesses."
4,service the (wps) ogc web processing processes.
4,the ogc service standard provides rules for standardizing how inputs and outputs web processing service standard provides rules for standardizing how inputs and outputs for invoking geospatial for processing invokingservices are defined. geospatial qc takes eachservices processing arethe cs data defined.
4,"eachandqc their metadata takes (including the cs data anymetadata and their existing quality valuations) (including anyas and performs input, quality existing processing valuations) as input, andgeoprocessing and on these performs processing data, and producing or geoprocessing on updating the quality metadata of each single observation of the cs data (figure 1). these data, producing or updating the quality metadata of each single observation of the cs data this process may involve other1)."
4,"(figure data process e.g., thissources, may authoritative eo data, data,sources, involve other data social e.g., media. data, eo data, social media. authoritative figurefigure 1."
4,generic 1.
4,"generic pattern pattern a quality of aofquality control(qc), control (qc),seen seenas as processing processing task producingmetadata taskproducing metadata on data on data qualityconceptualized quality conceptualized inin an anatomic-workflow atomic-workflow (bpmn (bpmn diagram)."
4,a workflow diagram). startsstarts at the greyed a workflow at the disk with a green circle and finishes at the black disk with the red circled.
4,processing steps greyed disk with a green circle and finishes at the black disk with the red circled.
4,"processing steps are the rounded rectangles, these tasks may involve inputs and outputs indicated by the data objects are the rounded rectangles, these tasks may involve inputs and outputs indicated by the data objects associated to the task (dotted arrows), and normal flow operates according to the non-dotted black associated to the task (dotted arrows), and normal flow operates according to the non-dotted black arrows."
4,the output data objects are the metadata for the input including the metadata on spatial data arrows.
4,the output data objects are the metadata for the input including the metadata on spatial data quality and potentially the input themselves which have been modified (corrected). quality and potentially the input themselves which have been modified (corrected).
4,"as part of the cobweb platform, the qawat offers flexibility in designing, recording and as part ofathe executing cobweb quality platform, assurance workflow;theitqawat offers flexibility enables communication andindialog designing, betweenrecording and stakeholders executing a quality assurance workflow; it enables communication and dialog and provides metadata on metaquality in a machine-readable format (a bpmn file). between stakeholders and provides metadata figure 1 showson metaquality a qc in a machine-readable conceptualized in a bpmn diagram, as a (a format qabpmn file). with a single task. workflow figure 1 shows a qc conceptualized in a bpmn diagram, as a qa workflow this representation of a single qc and a full workflow, are based upon the same with a singlewhich principle, task."
4,"this representation of a single might use additional qc and bpmn a full workflow, artifacts, are based upon such as conditional flow,the same principle, parallel gatewayswhich might or inclusive"
5,"data 2017, 2, 35 5 of 20 data 2017, 2, 35 5 of 20 use additional bpmn artifacts, such as conditional flow, parallel gateways or inclusive gateways. gateways."
5,"using the bpmn using the bpmn standard, thestandard, the fullfor full workflow qa canfor workflow beqa can be displayed displayed with or without with or without inputs (as in inputs (as in figure 2, and appendix a figure a1)."
5,"this displays all the involved artifacts, figure 2, and appendix a figure a1)."
5,"this displays all the involved artifacts, and the quality the and elements quality elements created and updated during the execution of the workflow, which are shown as created and updated during the execution of the workflow, which are shown as annotations: vol as annotations: vol as volunteer referring to the stakeholder quality model, obs referring to the producer volunteer referring to the stakeholder quality model, obs referring to the producer quality model, quality model, and auth to the consumer model."
5,"these annotations, only used when communicating auth to thebetween and graphically consumer model."
5,"these annotations, only used when communicating graphically stakeholders, are conformant to the bpmn standard. between stakeholders, are conformant to the bpmn standard."
5,figure 2.
5,highlight of one qc within the entire qa workflow (greyed) designed for the fallopia japonica figure 2.
5,highlight of one qc within the entire qa workflow (greyed) designed for the fallopia (japanese (japanese study. knotweed) japonica see figure knotweed) seefor study.
5,"a1 the whole appendix a.1annotated bpmn for the whole each qc where bpmn annotated is labeled where each with qc isnumber, its pillar pillaritsname labeled with pillar and a short number, textual pillar name illustrating the semantics and a short textual of the illustrating process, e.g., the semantics pillar 3 of the process,validation/photo automatic e.g., pillar 3 automaticquality."
5,the annotations validation/photo inthe quality. brown list the in annotations quality brownmetadata output at list the quality step. eachmetadata output at each step.
5,"the the metadata metadata on data on data quality quality belongstotothree belongs differentquality threedifferent quality models models (see appendixb bfor (seeappendix fora a full full description): the producer model, generating the spatial data quality from the description): the producer model, generating the spatial data quality from the iso19157, the consumer iso19157, the consumer model following the principle of the user feedback model (geospatial user feedback model following the principle of the user feedback model (geospatial user feedback standard working standard working group www.opengeospatial.org/projects/groups/gufswg) and the stakeholder group www.opengeospatial.org/projects/groups/gufswg) and the stakeholder quality model [26,36]. quality model [26,36]."
5,"the iso 19157 establishes the principles for describing the quality of iso 19157 data the geographic establishes theeach qualifying principles for describing citizen volunteer, the quality the stakeholder of geographic quality model producesdataquality qualifying citizen volunteer, eachelements the stakeholder quality model produces quality elements that are updated at each new participation in a survey (from running the associated qa) that are updated but new at each participation current in aassociated values are kept survey (from running with the the associated observation qa) by when it is made current butthis citizenvalues are kept volunteer. associated not onlywith can the valueswhen observation the current of the it is made by stakeholder this citizen quality volunteer. model influence, not only factors, can thethe as weighting current quality values of theassessment for the stakeholder quality models, other model quality for as influence, a new data captured weighting bythe factors, thisquality citizen (seen as an for assessment observation the other qualitymade forsensor by the models, datacitizen”), a new“this capturedbut this will bythey evolve also(seen citizen as from the processing an observation madeandby the sensor “this citizen”), but they will also evolve from the processing and rules within each qc."
5,"all three quality models use an encoding that follows the iso19157 schema, with a scope to link “citizen” to the current observation."
6,"data 2017, 2, 35 6 of 20 the qa framework proposes 7 categories of qcs as 7 pillars for quality assessments of cs data (table 1)."
6,this categorization into 7 pillars helps in the development of geoprocesses and in the composition of the workflow.
6,they represent the top of an ontology for the qcs for cs quality assessments.
6,the 7 pillars further extend the classification of quality assessments proposed in previous work by goodchild & li [22]: “the crowdsourcing approach” (validation in reference to the rest of the crowd); “the social approach” (validation using expert peers or trusted peers); and “the geographical approach” (validation involving the geographical context).
6,"several qcs accessible have been developed, and are available within the wps repository (implemented either in java or as r scripts)."
6,"a screenshot and video of the interface can be viewed in the github repository (https://github.com/cobweb-eu/workflow-at) and shows composition of a qa workflow, including choosing a qc from a list of qcs (classified as one of the 7 pillars), populating the necessary input parameters and input data, then continuing to instantiate the workflow."
6,table 1.
6,"the 7 pillars of quality controls in cs [26,36]."
6,"pillar name pillar description location, position and accuracy: focusing on the position of the user and of the targeted feature (if any), pillar 1: positioning local condition or constraints (e.g., authoritative polygon, navigation, routing, etc.)."
6,"erroneous entries, mistakes, malicious entries: erroneous, true mistakes, intentional mistakes, pillar 2: cleaning removals, corrections are checked for the position and for the attributes."
6,feedback mechanism can be an important part of this pillar if the mistakes can be corrected.
6,"simple checks, topology relations and attribute ranges: carries further the cleaning aspects by pillar 3: automatic validating potential good contributions."
6,its aim is towards positive rewarding with more inclusive rules than validation with pillar 2 focusing more on excluding rules.
6,"comparison of submitted observations with authoritative data: either on attributes or position pillar 4: authoritative performs statistical test, (fuzzy) logic rule based test qualifying the data captured or reversely qualifies the data comparison authoritative data."
6,knowledge of the metadata of the authoritative data is paramount.
6,"utilizing statistical and behavioral models: extends pillar 4 testing against modeled data pillar 5: model-based (e.g., physical models, behavioral models) and other user contributed data within the same context."
6,this may validation use intensively fuzzy logics and interactions with the user within a feedback mechanism of interactive surveying.
6,(if some tests will be similar to pillar 4 the outcome in quality elements can be different).
6,data mining techniques and utilizing social media outputs: extends pillar 5 testing to using various pillar 6: linked data social media data or related data sources within a linked data framework.
6,tests are driven by a more correlative analysis paradigm than in previous pillars.
6,conformance enrichment and harmonization in relation to existing ontologies: level of pillar 7: semantic discrepancy of the data captured to existing ontology or crowd agreement is transformed into data quality harmonization information.
6,"in the meantime, data transformation to meet harmonization can take place."
6,the following section illustrates the usage of this qa framework for cs data from the jkw survey in the snp.
6,"designing the japanese knotweed quality assurance the cobweb app [12] was used to record the cs data, with a single observation containing positional information, photos of the reported jkw, along with bearing and tilt angle parameters given by their smartphone, and the citizen’s estimates of the heights of the plants."
6,"the citizens were also asked to report their distance to the declared jkw occurrence (see appendix c), and an approximation of the area covered by the jkw."
6,the diagrammatic qa workflow for the jkw study (figure 2) was designed following initial discussions with the stakeholders.
6,"further engagement led to some modifications to the rules within a qc, for instance “jkw does not grow in the forest or in managed land”, leading to qcs in pillar 4 (authoritative data comparison) using forest and managed land data (termed lpis in the diagram)."
6,the pillar 4 point in polygon qc (pillar4.
6,"pointinpolygon) tests the inclusion of a point, taking into account positional accuracy of both the point and the authoritative data, and concludes first on the usability, topological consistency and domain consistency."
6,"then, depending on the type of"
7,"data 2017, 2, 35 7 of 20 attribute attached (if mentioned), i.e., classification or quantitative or non-quantitative, and of the agreement looked for (“must be in” or “must be not in”), updates the corresponding iso19157 elements, e.g., thematic classification correctness."
7,"although the ordering of the pillars in table 1 is not compulsory when composing any qa workflow, there is often a natural succession of qcs."
7,"this subjective order follows from “topological and location concerns for the observation”, with typical qcs in pillar 1 (positioning) and pillar 2 (cleaning), to “content concerns”, with pillar 3 (automatic validation) for photo quality evaluation."
7,"next follows pillar 5, with the modeled data from eo (model-based validation), attribute range (from pillar 3), and the authoritative data of known verified occurrences (pillar 4, authoritative data comparison) with the authoritative data sourced from the national biological network (nbn), and then excluding rules in pillar 4 for forest and managed land."
7,"following this, “content concerns” comes from the consistency of the answer by comparing this citizen to the other citizen scientists (pillar 5 for reliability distribution and co-occurrence validity)."
7,"then enlarging the comparison to other data sources from the crowd with a qc in pillar 6 (linked data analysis), and finally a normative assessment for the photo annotation from a qc in pillar 7 (semantic harmonization)."
7,the eo data used in pillar 5 proximity suitability score qc (proximitysuitabilityscore) are considered as semantically equivalent to the cs data.
7,observing jkw a citizen also identifies a high-risk zone (without delineation).
7,"the risk is taken as the classification correctness reached by the citizen, 1 if cs is considered to be ground truth, but could be lower depending on the quality and trust of these cs data."
7,"the fact that, if more citizens make the same observation it would increase the local quality of the eo data itself, is not the purpose here."
7,"nonetheless, these co-occurrences made from the cs viewpoint are used in the qa (qualifying the cs data), as increasing the quality of the observation and credibility of the citizen."
7,"where the eo product estimates an area with a high-risk value, one could expect (with a high probability) that a nearby citizen scientist would report a jkw occurrence."
7,"nonetheless, the survey was not set up to allow regular reporting of occurrences or absences of jkw, i.e., the citizen would be prompted to report at regular time intervals or set distances as well as allowing spontaneous reporting."
7,"therefore, the equivalence of cs and eo information is directly valid only for areas of declared occurrences from citizens (true and false positive statistics)."
7,"false negatives were obtained indirectly, using the verification process; however, the sample was considered unbalanced, with only 10% incorrect observations."
7,"this paper does not include a comment on the method of eo data product generation that predicted the risk of occurrence of the jkw, as this is not its focus; only what the eo product represents is required, either when using it in the qa workflow as in figure 2 (see section 5) or when using cs to validate it (see section 4)."
7,"taking the values of the eo data as suitability scores, which are measures of likelihood of occurrence, the pillar 5 proximity suitability score qc (proximitysuitabilityscore), highlighted in figure 2, computes a weighted summary (mean or max) of the values found within the nearby polygons of the eo data product, to the current cs observation."
7,it then assigns these to the classification correctness quality element of that current cs record.
7,"usability and positional accuracy (from accuracies of the polygons) are updated during the process, depending on the score obtained during the comparison to a chosen quality element with simple threshold rules."
7,"for the remainder of the paper, there is a distinction made between the entire qa (as shown in appendix a figure a1), and the qa without the pillar 5 (model-based validation) qc, which uses the eo data (for the risk of jkw)."
7,the latter is highlighted in figure 2.
7,"note, for both situations, the qcs in pillar 6 (linked data analysis) and pillar 7 (semantic harmonization) were not used due to insufficient, relevant additional data but were left in figure 2 as part of the established qa workflow for jkw."
7,running the qa workflow produced quality values for each recorded observation in the cs data.
7,"summaries at dataset level can be made but in the remainder of this paper, cs data quality is considered at individual record level (each citizen’s observations)."
7,"for example, the data quality element classification correctness (dq_classificationcorrectness), for the cs data from this study, is a vector of 177 values, each ranging between 0 and 1."
8,"data 2017, 2, 35 8 of 20 4."
8,"using citizen science for earth observation validation within the approach for validating an eo data product from other data sources, cs data can play an important role for the validation itself."
8,"this can be within a partial opportunistic scheme or providing a training sample as a ground truth, e.g., for a supervised classification algorithm."
8,"although all the data quality elements are potentially important for the future usage of the cs data, the main quality of interest here is the classification correctness (from iso19157)."
8,"each of the 177 cs data records were verified by an expert, who from photo examination, declared 16 as incorrectly identifying a jkw presence."
8,further information on the jkw co-design study can be found on the cobweb website (https://cobwebproject.eu).
8,"without quality assurance of the citizen science data without a means of qa, there is total uncertainty concerning the quality of the cs data, so either the end user would have to blindly trust the data by artificially assigning a classification correctness of 1 (as degree of agreement) for each observation, or they would have to assign 0.5 as degree of agreement (neither correct nor incorrect)."
8,"a value equal to or below 0.5 is not usable for validation, so to use the cs data without qa implies a full degree of trust."
8,"as each cs observation should be associated in the eo data as a high-risk point of observing jkw, the “score” obtained in the pillar 5 qc (proximity suitability score (proximitysuitabilityscore)) alone is a validation measure for the eo, of high risk zones."
8,"when this score is high, the single cs observation is not too far from a polygon of the eo data attributed as high risk of jkw presence."
8,"therefore, this cs observation implicitly declares the eo data as accurate in this zone."
8,"however, if this cs observation has been verified as incorrectly declaring the presence of jkw the eo polygon contributing to a high-risk score can also be considered as incorrect: a commission error (i.e., excess data present in a dataset, measured here as number of excess items in the dataset or sample in relation to the number of items that should have been present (iso19157); this is also the false positive rate)."
8,"when the score obtained for a verified as correct single cs data is low, this infers that the eo data has an omission of high-risk polygons in this zone (i.e., data absent from the dataset, measured here by the number of missing items in the dataset or sample in relation to the number of items that should have been present (iso19157); this is also the false negative rate)."
8,"applying these rules for the whole cs data sample, the findings are: • cs data with no score (25/177) and score <0.2 (20/177) represent an omission for high-risk zones of 45/177 (25%) and correcting for ground truth gives 39/161 (24%)."
8,"• cs data with score >0.5 represent accurate areas, 120/177 (68%), but 10/120 (8%) points wrongly identified jkw so a corrected accuracy of 110/177 (62%) with a commission of 8%."
8,the accuracy measures above (68% and 62%) represent the validation of high or medium risk zones only but these accuracy measures are relative to this very opportunistic sample that the cs data represents.
8,"note, the maximum accuracy one can obtain using this sample of 177 is 91%, due to 16/177 being incorrectly identified jkw observations."
8,"the commission accuracy for the “low” or “no” risk zones identified by the eo data can be calculated with the same algorithm by inverting the risk values, giving 3/177 (2%) or 3/161 (2%) corrected."
8,"obviously, here the sample size of 177 control data points can be considered too small in comparison to the number of segmented zones within the eo data."
8,(note the 16 incorrect observations would also contribute to defining the accuracy of the “no risk” zones but with a very small sample).
8,"however, the nature of cs data is such that optimal sample numbers and spatial distribution are rarely obtained, e.g., areas inaccessible by the citizens."
8,"if this cs sample cannot be used to assess the validation of the eo data, one can nonetheless expect that if the eo data were accurate, with 10% omission for the high-risk zones, this omission value should be observed for any sample coming to challenge this validation."
8,the cs sample (taken as fully accurate) gave an omission of 25% (24% corrected).
9,"data 2017, 2, 35 9 of 20 4.2."
9,"with quality assurance of the citizen science data data 2017, 2, 35 9 of 20 as expressed above, cs data identifying invasive species without quality is taken either as truth trust in full4.2."
9,with(classification quality assurance correctness implicitly of the citizen set to 1) or full uncertainty (classification correctness science data set to 0.5).
9,"as expressedwhen therefore, cs data any above,starting qa, the identifying qualityspecies invasive elements qualityfrom measured without a level is taken eitherofasagreement, truth start with a value of 0.5."
9,"due to the small number of incorrect jkw in the verification (16/177), a qa in full trust (classification correctness implicitly set to 1) or full uncertainty (classification correctness matching the verification set to 0.5)."
9,"therefore, when performance qa, the not starting anyshould change quality elements above accuracy the measured statistics from a level for the eo of agreement, however, data. start it is established with a value of 0.5."
9,due to the expert that validation small number itself isjkw of incorrect notinalways fully accurate.
9,"(16/177),as the verification the qa a qa uponthe matching is based verification external performance assessments, should and a setnot change itthe of rules, above will alsoaccuracy statistics for the theeo be discriminating correct data."
9,"however, observations, it is established and changes will occur thesevalidation thatinexpert itself isbased statistics when upon fully not always accurate. onlyas selecting the quality high qa is cs data. based uponhere note that external the qa assessments, may discard a setaof and(give rules, it will low-quality also for value) discriminating be an accurate jkw theobservation, correct observations, and changes will occur in these statistics when based upon selecting only high quality e.g., the observation is located too close to forest, therefore being “unreliable” as defined in the qa rules."
9,cs data.
9,"note that here the qa may discard (give a low-quality value) for an accurate jkw the qa from figure 2, was then executed without the inclusion of the pillar 5 qc test, which used observation, e.g., the observation is located too close to forest, therefore being “unreliable” as defined the eo data in the qaproduct. rules."
9,"selecting only cs data with usability or classification correctness above the chosen threshold of 0.7, the qa from figure the findings 2, was thenin 4.1 become: executed without the inclusion of the pillar 5 qc test, which • used cs the with data eo data product. no score selecting (10/66) only <0.2 cs data or score withrepresent (9/66) usability or classification omission areascorrectness above of 19/66 (29%). the chosen threshold of 0.7, the findings in 4.1 become: • cs data with score >0.5 represent accurate areas, 41/66 (62%)."
9, cs data with no score (10/66) or score <0.2 (9/66) represent omission areas of 19/66 (29%).
9,"comparing these results with those based upon ground truth, reveals a similar global accuracy  cs data with score >0.5 represent accurate areas, 41/66 (62%). (for the high and medium risk zones), 62%, but a more pessimistic omission rate 29% vs."
9,"the sample comparing these size after of atthose results with selection, leastbased upon 0.7 in groundor usability truth, reveals a similar classification global accuracy correctness, can be put (for the high and medium risk zones), 62%, but a more pessimistic omission rate 29% vs."
9,the into question.
9,"nonetheless, it shows that qualified cs data could be a substitute to verified cs data. sample size after selection, of at least 0.7 in usability or classification correctness, can be put into question."
9,"with nonetheless, line of it shows sight correction in that qualified the qa for thecs data could citizen be data a substitute to verified cs data."
9,science one important 4.3.
9,"with consideration line of sight when correction in the qaderiving the data for the citizen quality science datais that the geoprocessing algorithms are basedone important consideration when deriving the data qualityand upon the location of the citizen (of their smartphone), the actual not the is that position of geoprocessing the observation, which is the aimed point on the ground when taking the photo algorithms are based upon the location of the citizen (of their smartphone), and not the actual position (line of sight (los))."
9,"in the of the qa described observation, which isin thefigure 2, theonfirst aimed point when1taking qc, pillar the ground relative (line line position the photo of(los)). of sight sight qc (relativepositionlineofsight), in the qa described in figure uses 2,thethelos but first qc,aims to investigate pillar the quality 1 relative position line ofofthe topological sight qc (relativepositionlineofsight), uses the los but aims to investigate the quality consistency."
9,"the process uses a digital elevation model (dem), and the device’s bearing, tilt and of the topological consistency. position process the to information uses the compute a digital elevation los [39]."
9,"for this case(dem), model andnatural study, the the device’s bearing,wales tilt and resources lidar position information to compute the los [39]."
9,"for this case study, 2 m resolution digital surface model was used for the dem input."
9,the qc assesses whether the natural resources wales the lidar 2 m resolution digital surface model was used for the dem input.
9,"the qc assesses whether distance of the citizen to the declared observation is too far or not (see the pseudo code given in the distance of the citizen to the declared observation is too far or not (see the pseudo code given in figure 3), and has the option of correcting the position of the observation by replacing it with the line figure 3), and has the option of correcting the position of the observation by replacing it with the line of sight point, basebase of sight under point, certain under conditions."
9,"with conditions. certain optionchosen, this option with this chosen, thethe quality quality values values maymay differdiffer considerably section (see(see considerably 5) during section 5) duringthe restofofthe therest the qa."
9,figure figure 3.
9,pseudo-code 3.
9,pseudo-code anan (using (using r rscript of the scriptstyle) style) of qcusing the qc usinglos: los:pillar 1 relative pillar position 1 relative line line position of of sight qc (relativepositionlineofsight).
9,sight qc (relativepositionlineofsight).
9,"the pseudo-code in figure 3, as does the other qcs given in appendix a table a1, gives some the pseudo-code insight in7figure into how the 3, qc pillars of as does beenother have the qcsand designed given in appendix coded."
9,"according a to table a1,concept, the pillar gives some insight into how the 7 pillars of qc have been designed and coded."
9,"according to the pillar concept,"
10,"data 2017, 2, 35 10 of 20 qc test undertakes some computation based on its input data (for figure 3 this is the los point and its distance to the volunteer), which is then passed onto bespoke rules estimating relevant quality element values."
10,"using a threshold data 2017, 2, 35 distance of 5 m to define the topological consistency (e.g., a swap of 20 8.3 m 10 from using p-below set to 0.6, if the accuracy is similar to the initial average position accuracy of 13 m), qc test the findings undertakes above, some computation still selecting only csbased data on its input with data above usability 0.7,3become: (for figure this is the los point and its distance to the volunteer), which is then passed onto bespoke rules estimating relevant quality • data with cselement no score represents omission areas, 0/72 (0%). values. • withascore cs datausing threshold >0.5distance of 5 accurate represent m to define the topological areas, consistency 72/72 (100%), (e.g., a for correcting swap from 8.3 ground m gives truth using p-below set to 0.6, if the accuracy is similar to the initial average position accuracy of 13 m), the 67/72 (93%). findings above, still selecting only cs data with usability above 0.7, become: • cs data with score <0.2 (which was the low risk value in the eo data) represents an omission rate  cs data with no score represents omission areas, 0/72 (0%). of 0/72 (0%)."
10," cs data with score >0.5 represent accurate areas, 72/72 (100%), correcting for ground truth gives the los correction 67/72 (93%). affected 38 out of 177 cs data records, i.e., where the los gave a result."
10," cs data with no estimation is returned when score (which <0.2either was the theoflow line value riskdoes sight in reach not the eothe data) demrepresents withinan omission a 1000 m distance, rate of 0/72 (0%). or the tilt angle is inadequate, or when the dem has an issue (no data value encountered, due to the los holes, etc.)"
10,"[39]. of the 38affected outcorrection points,3826 of 177 csadata outobtained records, i.e., topological where the los consistency a result. gave0.6, below no citizen i.e., the estimation is returned when either the line of sight does not reach the dem within was identified as being too far from the observation."
10,"forcing a replacement, for example by setting a 1000 m distance, or the tilt angle is inadequate, or when the dem has an issue (no data value encountered, due to holes p-below to 0.99, could introduce some error propagation effects, as the uncertainties of the tilt and etc.)"
10,"out of the 38 points, 26 obtained a topological consistency below 0.6, i.e., the citizen was can be potentially bearingidentified far from the(see as being too important table a2 observation. in thea discussion forcing for exampleoverall, replacement, section). by settingcomparing p- results in sections below to 0.99, 4.2could 4.3, the los and introduce somebase point error correction propagation improved effects, the eo accuracies. as the uncertainties of the tilt and bearing can be potentially important (see table 3 in the discussion section)."
10,"overall, comparing earthinobservation 5."
10,"usingresults to4.3, sections 4.2 and improve citizen the los base pointscience data correction quality improved the eo accuracies."
10,section in 5.
10,"4.2, the using earth qa fromtofigure observation improve2citizen was used without science the inclusion of the pillar 1 proximity data quality suitability score qc (proximitysuitabilityscore) as highlighted in figure 2, which uses the eo data in section 4.2, the qa from figure 2 was used without the inclusion of the pillar 1 proximity comparing product.suitability the quality element obtained with and without running this particular qc gives score qc (proximitysuitabilityscore) as highlighted in figure 2, which uses the eo data some indications of the usefulness product."
10,"comparing an eo data of suchobtained the quality element product, with and withoutand whether running or not this this particular can be used qc gives in environmental directlysome indications of thepolicy making, usefulness even of such if the an eo dataeo product product, anditself is not whether considered this can beas or not “perfect”. used the plotsdirectly in environmental in figure policy 4 are density making, plots evenquality of the if the eoelement product itself valuesis not considered derived afterasrunning “perfect”.the qa (entirelythe plots in figure or without 4 are and that qc), density plots were of the quality obtained from theelement values derived r function afterthe density(): running y-axisthe qa density is the (entirely or without that qc), and were obtained from the r function density(): the y-axis is the density value as defined by this function."
10,"raw observations have also been overlaid for each group, displaying value as defined by this function."
10,"raw observations have also been overlaid for each group, the observed distributions displaying the observed values (no of ofdistributions y-axis valuesfor(nothese). y-axis for these)."
10,qa without pillar 5 qc (eo) qa without pillar 5 qc (eo) 0.5 0.5 / 16 incorrect jkw / 16 incorrect jkw 4 4 | 161 correct jkw | 161 correct jkw 3 3 density density 2 2 1 1 // / // / // / //// / / / / |||| ||| |||| ||| || ||| | ||| || ||||| ||||| |||| |||||| | ||| | ||| || ||||| 0 0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 dq_classificationcorrectness dq_usability entire qa entire qa 0.5 0.5 2.5 2.5 / 16 incorrect jkw / 16 incorrect jkw | 161 correct jkw | 161 correct jkw 2.0 2.0 1.5 1.5 density density 1.0 1.0 0.5 0.5 // / / // / // / / / / / / / // / / // / // / / / / / / / 0.0 0.0 | | ||| |||||||||||| | |||| ||||| |||||||||||||||||||||| | |||| |||||||||||||||||| | | |||| ||||||||||| |||||| ||||| | ||||||||| |||||||||||| || | |||| ||||||||||||||||||| 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 dq_classificationcorrectness dq_usability figure 4.
10,quality values and densities for classification correctness and usability for two different qa figure 4.
10,"quality values and densities for classification correctness and usability for two different qa processes, and for the two groups of observations correctly identifying jkw or not (raw observations processes, and for the two groups of observations correctly identifying jkw or not (raw observations are also overlaid). are also overlaid)."
11,"data 2017, 2, 35 11 of 20 5.1."
11,"cs data validation the first row of plots in figure 4 corresponds to the values used in section 4.2, i.e., without the qc in pillar 5 involving the eo data."
11,"being able to test the performance of the qa with verified cs data, also completes the eo to cs data validation."
11,"figure 4 shows that, without the pillar 5 qc involving the eo data product, the data resulted in too many correct jkw observations with low quality values."
11,"the densities are also very similar between the two groups, even though the densities of very low quality values are higher for incorrectly identified jkw observations."
11,"however, one must be cautious, as the sample size difference between the two groups introduces a bias when looking at the densities."
11,"running the entire qa corrected these two aspects to some degree, as the densities of correctly identified jkw observations have a mode towards high qualities (plots labeled “entire qa”)."
11,"the distributions also appear more in agreement with correct and incorrect jkw observations, as uncertainties (0.5) remain higher for incorrect jkw observations, and the density of high qualities is higher for correct jkw observations."
11,"nonetheless, low quality values are still given to a considerable number of correct jkw observations."
11,"this could be an indication for adding more tests into the qa workflow, or adjusting some parameters used in these qcs."
11,"beside a calibration issue, there may also be a sensitivity issue due, for example, to position uncertainty or the quality of the authoritative data used in the different qcs."
11,"note that a simple t-test for classification correctness resulted in a mean comparison of 0.65 for correctly identified jkw, and of 0.57 for incorrectly identified jkw (p value = 0.09)."
11,"for the qa without los correction, the same t-test gives: correct jkw, 0.65 incorrect jkw 0.58 (p-value = 0.10)."
11,this solely demonstrates the usefulness of the qa in discriminating data quality.
11,"moreover, the same t-test for the quality results obtained without the qc in pillar 5 (model-based validation), involving the eo data, is not significant (p value = 0.3), therefore highlighting the importance of this pillar 5 qc and supporting eo data product, in the qa process."
11,"iterative paradigm as seen in section 5.1, despite the indication that the eo data quality is not high (section 4), the cs data quality will only improve when used in the qa."
11,"provided the eo data does not have high commission (false positive rate) and omission (false negative rate) errors, this would generally work, otherwise to some extent, the errors will propagate into the cs qualification."
11,"here, this could be the cause of high quality values for some incorrect jkw observations (false positives), as well as low quality for some correct jkw observations."
11,"note that the differences in commission for high and low risk categories could be challenging, therefore, only using the extreme risk categories within eo data for a jkw risk product, would be a better choice for qa."
11,the eo algorithm could also utilize the qualified cs data as additional evidence to support the algorithm (as a training sample).
11,"from the iterative process that could result, the eo data zones where the cs data have been validated would then conflate appropriately to the cs data, ensuring better agreement of cs with eo data."
11,"providing the qa makes use of the additional “qualifying rules” related to eo data product, this iterative process would be alternating (i) the eo supervised algorithm using the cs of high quality, and (ii) the qa for the cs data including the eo data product for jkw risk obtained in (iii), where a threshold of high quality is fixed after the first iteration."
11,"once validated, the new cs data collection, used as supplementary training samples, could also generate some benchmarking requirements when new eo imagery data would be needed, i.e., using the goodness of fit of the eo algorithm."
11,"within an environmental monitoring scheme, eo and cs would then be complementary."
11,"discussion with the aim of providing relevant metadata for cs data along with the provenance of the metadata on data quality, i.e., the metaquality encapsulated in the qa workflow, the results shown in the preceding sections are promising."
11,they show also the benefit of using qualified cs data to validate
12,"data 2017, 2, 35 12 of 20 the eo data."
12,"however, there are some important and notable limitations."
12,the eo data validation must be seen more as a confirmation rather than an assessment of the validation because the cs data are an opportunistic sample not controlled for eo validation.
12,"nonetheless, for the user’s accuracy, and whether the sampling is controlled or not, as long as no evidence of introducing a bias in omission or commission can be established, the values obtained reflect a level of quality present in the eo data product."
12,"if the sample size of the verified data (ground truth) were large enough and balanced in potential presence or absence of the inns, assessing the omissions and commissions of the eo data (as in section 4), using qualified cs data instead of only the verified data has nonetheless, proven to be useful."
12,"moreover, correcting the position of the occurrences using the los improved the results in eo confirmatory validation (section 4.3, even though the dem accuracy as well as tilt and bearing accuracy may induce a propagation of errors."
12,the distance to the observation as declared by the volunteers can have variable quality and its comparison to the los distance showed large differences (see appendix c table a5).
12,"if, overall, the qa results discriminated the correct and incorrect jkw occurrences well (true positive versus false positive), the current qa performance did not seem able to adequately identify a single observation as correct, or not, from its resulting qualities."
12,"the number of qcs used, their characteristics, their order in the qa workflow, the positional accuracy sensitivity for both the observed data and the external data used, along with the parameters chosen in the qc (such as threshold distances) are contributing to the qa outcome, with potential issues due to calibration and error propagated through the defined qa workflow."
12,one could expect that increasing the number of qcs would diminish its sensitivity.
12,"cancelling out the assessment for a given qc, if the quality metadata linked to the external data do note reach a certain level, would diminish error propagation while highlighting inadequacy."
12,down weighting the impact of new quality values when updating can also be a less drastic solution to low quality external data used in the qa.
12,"note, the concern was only on qualifying the declaration of occurrence of the jkw plant; however, combining evidence across other available information can lead to better qualifying."
12,"for instance, the extent of the jkw at a declared occurrence and its potential agreement with the eo data product may be useful, i.e., it can be reasonably thought that the confidence in the risk given by the eo data product is increased as the extent detected by the eo algorithm becomes larger."
12,this also raises concerns about the use of eo to bring evidence of the jkw or another inns without appropriate spatial and temporal resolution for the detection of early spread [40].
12,"a citizen scientist may detect a very early spread of jkw, observing a very small extent that eo would be unable to achieve."
12,"these considerations have impacts on both, eo for cs or cs for eo validation paths."
12,"a further limitation is the number of volunteers used (34) and the number of observations (177), and reflected the relatively small-scale co-design type project."
12,"one way of increasing the number of observations would have been to regularly ask the citizen scientist to identify, at their current location, the occurrence or absence data for jkw."
12,"this would also have resulted in a better estimation of commission in the eo data, along with a better calibration of the citizen’s qualities as a sensor, i.e., trust, reliability."
12,"conclusions as an alternative to a costly and not necessarily reliable “verification process” often used in biodiversity citizen science (cs) surveys, this paper investigated the role and usefulness of an automated quality assurance workflow process based on 7 pillars of quality controls, and the three quality models introduced by the cobweb project [12]."
12,"besides the flexibility this approach offers for curation of cs data, a case study and results investigating citizen reporting of an invasive species (japanese knotweed) are reported."
12,"specifically, using informative earth observation data within the quality assurance workflow, in relation to the occurrences of japanese knotweed, even when of low quality, improves the cs quality assessment, i.e., it enables better discrimination between correct and incorrect jkw occurrences."
12,"conversely, using qualified cs instead of raw cs data resulted in"
13,"data 2017, 2, 35 13 of 20 increased user’s accuracy for the earth observation data."
13,"in groom et al. [41], the authors advocate an open data approach concerning the invasive non-native species (inns), in order to facilitate policy and management."
13,"as the cs data are unlikely to be of high quality, and verification from experts is not always viable (or not possible for all cs data), the automatic quality assurance presented in this paper could be beneficial."
13,"several limitations have been highlighted, however, and indications for potential remedies to the sensitivity of the quality assurance process were given."
13,"multiple criteria selection on the quality elements, to validate cs data, could compensate for the weaknesses of the qa workflow used."
13,"trustworthiness is used in many cs data collection systems, such as ispot [42] and coralwatch [27] as proxies for verification or in combination with other qa approaches to highlight potential re-use."
13,"addressing several dimensions related to trustworthiness, the stakeholder quality model plays a role in qualifying the volunteer’s observations."
13,a qc can use those quality values as weights for updating the other quality elements.
13,"it is expected that the flexibility provided by this qa framework, along with its principles, would guide the development of new qcs within the given pillars system as in the composition of workflows from stakeholders."
13,"in order to achieve robustness of the qualifying process, performing a pilot study in order to test the composed workflow qa on a verified sample is recommended as a necessary step before being used for a complete cs study."
13,the qa framework presented is applicable in other environmental monitoring contexts for which eo data and cs data can be complementary.
13,a typical example can be land use mapping with an emphasis on crop identification for agricultural land.
13,"being able to appropriately combine multiple source of information with eo data, including cs data is becoming a high priority in initiatives such as the copernicus program directed by european commission [43]."
13,"citizen observatories, where citizen science data are empowering directly the citizens, for example in reaction to the eo available, play an important role in this “copernicus chain” (from eo data to usable information) [43]."
13,it is acknowledged that a successful implementation of this “copernicus chain” would require attention to data quality.
13,the qa framework presented in this paper could contribute to this success.
13,acknowledgments: this research has received funding from the european union seventh framework programme (fp7/2007–2013) under grant agreement number 308513.
13,"author contributions: c.c., cr.h. and ch.h. co-designed the jkw experiment."
13,"d.g.l., j.f.r. and m.j.j. designed the qa framework."
13,d.g.l. and j.w. designed the jkw.
13,j.w. provided the eo data related to jkw risk.
13,d.g.l. and j.f.r. implemented the qa.
13,"d.g.l. structured the paper, performed the analysis and wrote the paper."
13,"d.g.l, cr.h., j.w. and j.f.r finalized the paper."
13,conflicts of interest: the authors declare no conflict of interest.
13,abbreviations the following abbreviations are used in this manuscript: cir colored infra-red cobweb citizen observatory web cs citizen science dem digital elevation model eo earth observation fp7 framework program 7 ias invasive alien species jkw japanese knotweed inns invasive non-native species lidar light detection and ranging los line of sight qa quality assurance qawat quality assurance workflow authoring tool qc quality controls snp snowdonia national park vgi volunteered geographic information
14,"data 2017, 2, 35 14 of 20 data 2017, 2, 35 14 of 20 appendix a appendix a figure a1."
14,entire qa workflow of figure 2: annotated bpmn where each qc is labeled with its pillar figure a1.
14,"entire qa workflow of figure 2: annotated bpmn where each qc is labeled with its pillar number, pillar name, and a short textual illustrating the semantic of the process."
14,"the annotations number, pillar name, and a short textual illustrating the semantic of the process."
14,the annotations in in brown list the quality metadata output at each step. (see www.bpmn.org for bpmn artifact brown list the quality metadata output at each step. (see www.bpmn.org for bpmn artifact descriptions). descriptions).
15,"data 2017, 2, 35 15 of 20 table data a1."
15,"2017, pseudo-code of the qcs used in figure 2 with a short description including input and output. 2, 35 15 of 20 pseudo code quality elements metadata created or updated: short description dq iso19157 producer model gvq geoviqua’s feedback model (simplified) csq cobweb’s stakeholder model pillar1."
15,"locationbasedserviceposition.relativepositionlineofsight #start input(""citizen position, dem, tilt and bearing of the phone "") lospt=computethelospoint(position, dem, tilt, bearing) input(""input dem uncertainty and position uncertainty"") d=distance(position,lospoint) sd= uncertaintyofdistance(d,dem_accuracy, dq_absoluteexternalpositionalaccuracy) input(""choosing threshold distance"", ""optionswap=true or false"", p-below=0.6) dq_topologicalconsistency=probability(gaussiandistribution(d,sd^2)<=threshold) if(optionswap & dq_topologicalconsistency < p-below) swaptolos(position,lospoint) update(dq_topologicalconsistency, dq_usablility) update(dq_absoluteexternalpositionalaccuracy) #end compute the line of sight, dq_usabilityelement aiming point and distance to dq_topologicalconsistency it."
15,swap to los point under certain conditions -los.point / dq_absoluteexternalpositionalaccuracy dem with scope defined as “los” .
15,thresholddistance p-below optionswap pillar2.
15,"cleaning.locationquality #start input(""input citizen position, dqlevelthreshold, posunthreshold, methods"") for each observation { for each method in methods { dq_usability=evaluaterule(dq_absoluteexternalpositionalaccuracy, dq_usability, dq_topologicalconsistency, dq_conceptualconsistency) } } update(csqs) #end according to the study dq_usabilityelement requirement check if position csq_vagueness is correct and/or can be csq_judgement corrected csq_reliability methods: csq_validity “pillar1."
15,"withinpoly”, ”pillar1."
15,"lineofsight”, “pillar1,containspoly”, “pillar1."
15,"getspatialaccuracy”, ”pillar4.or pillar1."
15,distanceto” dqlevelthreshold posuncertaintythreshold pillar3.
15,"automaticvalidation.photoquality #start input(""input citizen captured photo, blurthreshold,"" ) edgeimage=laplacetrasnform(photo) usable=evaluaterule(edgeimage, blurthreshold) update(dq_usability, dq_domainconsistency, csq_judgement, csq_trust) message(usable) #end test on sharpness of the image dq_usabilityelement using the method of edge dq_domainconsistency detection from laplace csq_reliability transform."
15,csq_trust -dynamic: blurthreshold <message> “can you take another picture?”
16,"data 2017, 2, 35 16 of 20 data 2017, 2, 35 table a1."
16,cont. 16 of 20 pseudo code quality elements metadata created or updated: short description dq iso19157 producer model gvq geoviqua’s feedback model (simplified) csq cobweb’s stakeholder model pillar5.
16,model-basedvalidation.
16,"proximitysuitabilityscore #start input(""input citizen position, buffersize, attribute"") proximpol=findnearby(buffersize) score=summarymeasure(calculweightdistance(proximpol),values(proximpol, attribute)) updaterules(dqs,csqs,gvqs, score) #end deriving the likelihood of the dq_usabilityelement observed occurrence (citizen dq_thematicclassificationcorrectness captured data) from polygon dq_absoluteexternalpositionalaccuracy proximity to a model-estimate dq_relativeinternalpositionalaccuracy one (e.g. suitability gvq_positivefeedback likelihood) gvq_negativefeedback csq_judgement modelled data csq_reliability attribute csq_validity buffersize csq_trust pillar4."
16,"authoritativedatacomparison.proximitysuitabilityscore #start input(""input citizen position, buffersize, attribute"") proximpol=findnearby(buffersize) score=summarymeasure(calculweightdistance(proximpol) updaterules(dqs,csqs,gvqs, score) #end deriving the likelihood of the dq_usabilityelement observed occurrence (citizen dq_domainconsistency captured data) from polygon dq_thematicclassificationcorrectness proximity to a given dq_nonquantitativeattributecorrectness authoritative data (e.g."
16,dq_absoluteexternalpositionalaccuracy existing observed occurrences) dq_relativeinternalpositionalaccuracy gvq_positivefeedback authoritative data gvq_negativefeedback buffersize csq_judgement note: comparing to pillar5 csq_reliability attribute value is 1 as csq_validity authoritative data.
16,csq_trust pillar4.
16,"authoritativedatacomparison.pointinpolygon #start input(""citizen observation, thematicagreement, authdata, buffersize"") inpol=evaluatein(observation.dq_absolutepositioninternalpositionalaccuracy, authdata.dq_absolutepositioninternalpositionalaccuracy, authdata, buffersize) iniscore=evaluateoverlapareas(inpol, observation, uncertainties) updaterules(dqs,csqs,gvqs, iniscore) if(not inpol) { proxpol=evaluatenear(obs.dq_absolutepositioninternalpositionalaccuracy, auth.dq_absolutepositioninternalpositionalaccuracy) proxscore=evaluate(proxpol, obs, uncertainties) updaterules(dqs,csqs,gvqs, proxscore) } #end given the position and its dq_usabilityelement uncertainty checking if a point dq_thematicclassificationcorrectness belongs to a polygon then dq_nonquantitativeattributecorrectness concluding on relative dq_quantitativeattributeaccuracy position accuracy and dq_absoluteexternalpositionalaccuracy 'relative' semantic therefore dq_griddeddatapositionalaccuracy usability and attribute dq_relativeinternalpositionalaccuracy accuracies gvq_positivefeedback gvq_negativefeedback authoritative data csq_judgement thematicagreement csq_reliability csq_validity csq_trust"
17,"data 2017, 2, 35 17 of 20 appendix b the three quality models used in qawat are given tables a2–a4."
17,table a2.
17,"producer quality model (iso19157); all the elements of the standard can potentially be used, even though they were initially designed for authoritative data."
17,dq quality element dq_ definition extracted from the iso19157 01 usabilityelement degree of adherence to as specific set of data quality requirements.
17,02 completenesscommission excess data present in a dataset.
17,03 completenessomission absence of data in a dataset.
17,"comparison of the classes assigned to features or their attributes to 04 thematicclassificationcorrectness a universe of discourse (e.g., ground truth or reference data)."
17,05 nonquantitativeattributecorrectness whether a non-quantitative attribute is correct or incorrect.
17,closeness of the value of a quantitative attribute to a value accepted 06 quantitativeattributeaccuracy as or known to be true.
17,07 conceptualconsistency adherence to rules of the conceptual schema.
17,08 domainconsistency adherence of values to the value domains.
17,degree to which data is stored in accordance with the physical 09 formatconsistency structure of the dataset.
17,correctness of the explicitly encoded topological characteristics of 10 topologicalconsistency a database.
17,closeness of reported time measurements to values accepted as or 11 accuracyofatimemeasurement known to be true.
17,12 temporalconsistency correctness of the order of events.
17,13 temporalvalidity validity of data with respect to time.
17,closeness of reported coordinate values to values accepted as or 14 absoluteexternalpositionalaccuracy being true.
17,closeness of gridded data spatial position values to values accepted 15 griddeddatapositionalaccuracy as or being true.
17,closeness of the relative positions of features in a dataset to their 16 relativeinternalpositionalaccuracy respective relative positions accepted as or being true.
17,table a3.
17,simplified consumer quality model (geoviqua www.geoviqua.org): for cobweb the simple concept of positive and negative feedback is kept but as automatically generated by the qcs.
17,gvq_ quality element gvq_ definition 01 positivefeedback number of positive feedbacks to the used data 02 negativefeedback number of negative feedbacks to the used data table a4.
17,stakeholder quality model (cobweb): qualifies the citizen volunteer in order to influence further confidence in a particular citizen’s observations when deriving the producer quality model values.
17,"csq_ quality element csq_ definition 01 vagueness inability to make a clear-cut choice (i.e., lack of classifying capability)."
17,"incompatibility of the choices or descriptions made (i.e., lack of 02 ambiguity understanding, of clarity)."
17,"accuracy of choice or decision in a relation to something known to be 03 judgment true (i.e., perception capability and interpretation)."
17,"04 reliability consistency in choices / decisions (i.e., testing against itself)."
17,"05 validity coherence with other people’s choices (i.e., against other knowledge)."
17,"confidence accumulated over other criterion concerning data captured 06 trust previously (linked to reliability, validity and reputability)."
17,07 nbcontrols total number of controls over all contributions of this volunteer.
18,"data 2017, 2, 35 18 of 20 appendix c table a5."
18,comparison of the citizen’s declared versus estimated from los distances (for 38 pts) of the citizen to declared invasive species.
18,"declared distance n (total 177) los distance (38/177) n = 10 out of * 73, close (<1 m) 73 min. 1st qu."
18,median mean 3rd qu.
18,"1.5 3.1 6.5 12.7 15.0 59.3 n = 9 out of * 30, nearby (1 m–3 m) 30 min. 1st qu."
18,median mean 3rd qu.
18,"3.5 5.8 14.8 213.5 516.9 687.8 n = 13 out of * 39, far (3 m–10 m) 39 min. 1st qu."
18,median mean 3rd qu.
18,"1.5 2.0 16.5 125.0 175.7 642.3 n = 6 out of * 34, very far (>10 m) 34 min. 1st qu."
18,median mean 3rd qu.
18,1.5 2.4 5.0 11.3 6.9 47.1 missing 1 - * not all estimations gave a result (see section 4.3).
18,references 1.
18,defra.
18,uk biodiversity indicators in your pocket.
18,available online: http://jncc.defra.gov.uk/page- 4229#download (accessed on 30 july 2016).
18,"williams, f."
18,"; eschen, r."
18,"; harris, a."
18,"; djeddour, d."
18,"; pratt, c."
18,"; shaw, r.s."
18,"; varia, s."
18,"; lamontagne-godwin, j."
18,"; thomas, s.e."
18,"; murphy, s.t."
18,"the economic cost of invasive non-native species on great britain; cabi report; cabi: wallingford, uk, 2010; p."
18,"silvertown, j."
18,a new dawn for citizen science.
18,trends ecol.
18,"2009, 24, 467–471."
18,[crossref] [pubmed] 4.
18,"butcher, g.s."
18,audubon christmas bird counts; biological report 90(1); u.s.
18,"fish wildlife service: washington, dc, usa, 1990."
18,"bonney, r."
18,"; cooper, c.b."
18,"; dickinson, j."
18,"; kelling, s."
18,"; phillips, t."
18,"; rosenberg, k.v."
18,"; shirk, j."
18,citizen science: a developing tool for expanding science knowledge and scientific literacy.
18,"bioscience 2009, 59, 977–984."
18,[crossref] 6.
18,"haklay, m."
18,citizen science and volunteered geographic information—overview and typology of participation.
18,"in crowdsourcing geographic knowledge: volunteered geographic information (vgi) in theory and practice, 1st ed."
18,"; sui, d.z., elwood, s., goodchild, m.f., eds."
18,"; springer: berlin, germany, 2013; pp."
18,105–122.
18,"goodchild, m.f."
18,citizens as sensors: the world of volunteered geography.
18,"geojournal 2007, 69, 211–221."
18,[crossref] 8.
18,"fonte, c.c."
18,"; bastin, l."
18,"; see, l."
18,"; foody, g."
18,"; lupia, f."
18,usability of vgi for validation of land cover maps.
18,geogr.
18,"2015, 29, 1269–1291."
18,[crossref] 9.
18,"franzoni, c."
18,"; sauermann, h."
18,crowd science: the organisation of scientific research in open collaborative projects.
18,"policy 2014, 43, 1–20."
18,[crossref] 10.
18,"roy, h.e."
18,"; pocock, m.j.o."
18,"; preston, c.d."
18,"; roy, d.b."
18,"; savage, j."
18,"; tweddle, j.c."
18,"; robinson, l.d."
18,"understanding citizen science & environmental monitoring: final report on behalf of uk-eof; nerc centre for ecology & hydrology and natural history museum: bailrigg, uk, 2012."
18,"adriaens, t."
18,"; suttoncroft, m."
18,"; owen, k."
18,"; brosens, d."
18,"; van valkenburg, j."
18,"; kilbey, d."
18,"; groom, q."
18,"; ehmig, c."
18,"; thurkow, f."
18,"; van hende, p.; et al."
18,trying to engage the crowd in recording invasive alien species in europe: experienced from two smartphone applications in northwest europe.
18,manag.
18,"invasions 2015, 6, 215–225."
18,[crossref]
19,"data 2017, 2, 35 19 of 20 12."
19,"higgins, c.i."
19,"; williams, j."
19,"; leibovici, d.g."
19,"; simonis, i."
19,"; davis, m.j."
19,"; muldoon, c."
19,"; van gneuchten, p."
19,"; o’hare, g."
19,citizen observatory web (cobweb): a generic infrastructure platform to facilitate the collection of citizen science data for environmental monitoring.
19,data infrastruct.
19,"2016, 11, 20–48."
19,[crossref] 13.
19,"kotovirta, v."
19,"; toivanen, t."
19,"; tergujeff, r."
19,"; häme, t."
19,"; molinier, m."
19,citizen science for earth observation: applications in environmental monitoring and disaster response.
19,photogramm.
19,remote sens.
19,"2015, 40, 1221."
19,[crossref] 14.
19,"see, l."
19,"; sturn, t."
19,"; perger, c."
19,"; fritz, s."
19,"; mccallum, i."
19,"; salk, c."
19,cropland capture: a gaming approach to improve global land cover.
19,"in proceedings of the agile 2014 international conference on geographic information science, castellon, spain, 3–6 june 2014."
19,"sturn, t."
19,"; pangerl, d."
19,"; see, l."
19,"; fritz, s."
19,"; wimmer, m."
19,landspotting: a serious ipad game for improving global land cover.
19,"in proceedings of the gi-forum 2013, salzburg, austria, 2–5 july 2013."
19,"sparks, k."
19,"; klippel, a."
19,"; wallgrün, j.o."
19,"; mark, d."
19,citizen science land cover classification based on ground and aerial imagery.
19,"in spatial information theory (cosit 2015), lecture notes in computer science; fabrikant, s.i., raubal, m., bertolotto, m., davies, c., freundschuh, z., bell, s., eds."
19,"; springer: cham, switzerland, 2015; pp."
19,289–305.
19,"kinley, l.r."
19,exploring the use of crowd generated geospatial content in improving the quality of ecological feature mapping.
19,"master’s thesis, the university of nottingham, nottingham, uk, october 2015."
19,"rossiter, d.g."
19,"; liu, j."
19,"; carlisle, s."
19,"; zhu, a."
19,can citizen science assist digital soil mapping?
19,"geoderma 2015, 259–260, 71–80."
19,[crossref] 19.
19,"walker, d."
19,"; forsythe, n."
19,"; parkin, g."
19,"; gowing, j."
19,filling the observational void: scientific value and a quantitative validation of hydrometeorological data from a community-based monitoring programme.
19,hydrol.
19,"2016, 538, 713–725."
19,[crossref] 20.
19,"fritz, s."
19,"; mccallum, i."
19,"; schill, c."
19,"; perger, c."
19,"; grillmayer, r."
19,"; achard, f."
19,"; kraxner, f."
19,"; obersteiner, m."
19,geo-wiki.org: the use of crowdsourcing to improve global land cover.
19,remote sens.
19,"2009, 1, 345–354."
19,[crossref] 21.
19,"flanagin, a.j."
19,"; metzger, m.j."
19,the credibility of volunteered geographic information.
19,"geojournal 2008, 72, 137–148."
19,[crossref] 22.
19,"goodchild, m.f."
19,"; li, l."
19,assuring the quality of volunteered geographic information.
19,"2012, 1, 110–120."
19,[crossref] 23.
19,"fowler, a."
19,"; whyatt, j.d."
19,"; davies, g."
19,"; ellis, r."
19,how reliable are citizen-derived scientific data?
19,assessing the quality of contrail observations made by the general public.
19,trans.
19,"gis 2013, 17, 488–506."
19,[crossref] 24.
19,"foody, g.m."
19,"; see, l."
19,"; fritz, s.; van der velde, m."
19,"; perger, c."
19,"; schill, c."
19,"; boyd, d.s."
19,"; comber, a."
19,accurate attribute mapping from volunteered geographic information: issues of volunteer quantity and quality.
19,cartogr.
19,"2014, 52, 336–344."
19,[crossref] 25.
19,"comber, a."
19,"; see, l."
19,"; fritz, s.; van der velde, m."
19,"; perger, c."
19,"; foody, g.m."
19,using control data to determine the reliability of volunteered geographic information about land cover.
19,earth obs.
19,geoinf.
19,"2013, 23, 37–48."
19,[crossref] 26.
19,"leibovici, d.g."
19,"; evans, b."
19,"; hodges, c."
19,"; wiemann, s."
19,"; meek, s."
19,"; rosser, j."
19,"; jackson, m."
19,on data quality assurance and conflation entanglement in crowdsourcing for environmental studies.
19,isprs int.
19,geo-inf.
19,"2017, 6, 78."
19,[crossref] 27.
19,"hunter, j."
19,"; alabri, a."
19,"; van ingen, c."
19,assessing the quality and trustworthiness of citizen science data.
19,concurr.
19,comput.
19,pract.
19,"2013, 25, 454–466."
19,[crossref] 28.
19,"craglia, m."
19,"; shanley, l."
19,data democracy–increased supply of geospatial information and expanded participatory processes in the production of data.
19,digit.
19,"earth 2015, 8, 1–15."
19,[crossref] 29.
19,"aplin, p."
19,remote sensing: ecology.
19,geogr.
19,"2005, 29, 104–113."
19,[crossref] 30.
19,"zlinsky, a."
19,"; heilmeier, h."
19,"; baltzter, h."
19,"; czucz, b."
19,"; pfeifer, n."
19,remote sensing and gis for habitat quality monitoring: new approaches and future research.
19,remote sens.
19,"2015, 7, 7987–7994."
19,[crossref] 31.
19,"jones, d."
19,"; pike, s."
19,"; thomas, m."
19,"; murphy, d."
19,object-based image analysis for detection of japanese knotweed s.l. taxa (polygonaceae) in wales (uk).
19,remote sens.
19,"2011, 3, 319–342."
19,[crossref] 32.
19,"viana, h."
19,"; aranha, j.t.m."
19,mapping invasive species (acacia dealbata link) using aster/terra and landsat 7 etm+ imagery.
19,"in proceedings of the conference of iufro landscape ecology working group, bragança, portugal, 21–27 september 2010."
20,"data 2017, 2, 35 20 of 20 33."
20,"tulloch, a.i.t."
20,"; possingham, h.p."
20,"; joseph, l.n."
20,"; szabo, j."
20,"; martin, t.g."
20,realising the full potential of citizen science monitoring programs.
20,conserv.
20,"2013, 165, 128–138."
20,[crossref] 34.
20,"alabri, a."
20,"; hunter, j."
20,enhancing the quality and trust of citizen science data.
20,"in proceedings of the 2010 ieee sixth international conference on e-science (e-science), brisbane, australia, 7–10 december 2010; pp."
20,81–88.
20,"bordogna, g."
20,"; carrara, p."
20,"; criscuolo, l."
20,"; pepe, m."
20,"; rampini, a."
20,a linguistic decision making approach to assess the quality of volunteer geographic information for citizen science.
20,"2014, 258, 312–327."
20,[crossref] 36.
20,"meek, s."
20,"; jackson, m."
20,"; leibovici, d.g."
20,a flexible framework for assessing the quality of crowdsourced data.
20,"in proceedings of the 17th agile conference, castellon, spain, 3–6 june 2014; available online: https: //agile-online.org/index.php/conference/proceedings/proceedings-2014 (accessed on 20 october 2017)."
20,"meek, s."
20,"; jackson, m."
20,"; leibovici, d.g."
20,a bpmn solution for chaining ogc services to quality assure location-based crowdsourced data.
20,comput.
20,geosci.
20,"2016, 87, 76–83."
20,[crossref] 38.
20,"rosser, j."
20,"; pourabdolllah, a."
20,"; brackin, r."
20,"; jackson, m.j."
20,"; leibovici, d.g."
20,full meta objects for flexible geoprocessing workflows: profiling wps or bpmn?
20,"in proceedings of the 19th agile conference, helsinki, finland, 14–17 june 2016; available online: https://agile-online.org/index.php/conference/proceedings/ proceedings-2016 (accessed on 20 october 2017)."
20,"meek, s."
20,"; goulding, j."
20,"; priestnall, g."
20,the influence of digital surface model choice on visibility-based mobile geospatial applications.
20,trans.
20,"gis 2013, 17, 526–543."
20,[crossref] 40.
20,"kimothi, m.m."
20,"; dasari, a."
20,methodology to map the spread of an invasive plant (lantana camara l.) in forest ecosystems using indian remote sensing satellite data.
20,remote sens.
20,"2010, 31, 3273–3289."
20,[crossref] 41.
20,"groom, q.j."
20,"; desmet, p."
20,"; vanderhoeven, s."
20,"; adriaens, t."
20,"the importance of open data for invasive alien species research, policy and management."
20,manag.
20,"invasions 2015, 6, 119–125."
20,[crossref] 42.
20,"dodd, m. ispot data quality, metadata and visualization."
20,"in proceedings of the 1st european citizen science association conference, berlin, germany, 19–21 may 2016; available online: http://www.ecsa2016.eu/ (accessed on 20 october 2017)."
20,"grainger, a."
20,citizen observatories and the new earth observation science.
20,remote sens.
20,"2017, 9, 153."
20,[crossref] © 2017 by the authors.
20,"licensee mdpi, basel, switzerland."
20,this article is an open access article distributed under the terms and conditions of the creative commons attribution (cc by) license (http://creativecommons.org/licenses/by/4.0/).
