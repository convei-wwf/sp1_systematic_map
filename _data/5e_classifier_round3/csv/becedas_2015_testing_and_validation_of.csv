page,p
1,"international journal of remote sensing issn: 0143-1161 (print) 1366-5901 (online) journal homepage: www.tandfonline.com/journals/tres20 testing and validation of cloud infrastructures for earth observation services with satellite constellations jonathan becedas, rubén pérez & gerardo gonzález to cite this article: jonathan becedas, rubén pérez & gerardo gonzález (2015) testing and validation of cloud infrastructures for earth observation services with satellite constellations, international journal of remote sensing, 36:19-20, 5289-5307, doi: 10.1080/01431161.2015.1070324 to link to this article: https://doi.org/10.1080/01431161.2015.1070324 published online: 27 jul 2015."
1,submit your article to this journal article views: 138 view related articles view crossmark data full terms & conditions of access and use can be found at https://www.tandfonline.com/action/journalinformation?
1,journalcode=tres20
2,"international journal of remote sensing, 2015 vol."
2,"36, nos. 19–20, 5289–5307, http://dx.doi.org/10.1080/01431161.2015.1070324 testing and validation of cloud infrastructures for earth observation services with satellite constellations jonathan becedas*, rubén pérez, and gerardo gonzález satellite systems, elecnor deimos, 13500 puertollano, spain (received 3 december 2014; accepted 13 may 2015) earth observation (eo) with satellites has been applied in different fields such as environment monitoring, natural disasters response, emergencies management, civil security, intelligence, maritime surveillance, and many others."
2,some of the application fields are very demanding in terms of system revisit time and product delivery time.
2,this is the case of responses to natural disasters.
2,"however eo still presents critical challenges to overcome in order to cover the actual demand of services: (i) high revisit time, (ii) high response time, and (iii) easy and instant access to eo products."
2,"to increase the revisit time and broaden the applications of the remote sensing, new space concepts such as constellations and formations of satellites have been developed."
2,"however, the traditional ground infra- structures, which are required to process and store data, are expensive."
2,"moreover, their limited scalability as well as their limited flexibility to manage large and variable amounts of imagery data shall also be considered."
2,"along this work, we propose a cloud infrastructure for data management to be validated with a constellation of 17 satellites acquiring the earth’s surface on a daily basis in order to offer high added value services for highly demanding applications."
2,the satellites download the raw data images in a network of 12 ground stations distributed around the world to provide global coverage.
2,the cloud system is based on previous works carried out by the research group.
2,"thus the cloud infrastructure is tested and evaluated for its use in the eo sector and applied to different realistic scenarios, including an intensive comparison with a traditional system responding to the lorca’s earthquake, which occurred in spain in 2011."
2,introduction earth observation (eo) commercial data sales present a positive trend during the last years.
2,"in 2000, the commercial data sales turnover was $200 million; at the end of 2009, it increased to $1.1 billion; and the forecast for 2019 is $4 billion (keith 2011; euroconsult 2010; euroconsult 2014)."
2,the number of instances of satellite launch is also increasing.
2,"it is estimated that at the end of the decade, 353 eo satellites will be launched, almost the double that during the first decade, i.e."
2,149 (euroconsult 2014).
2,"the value chain is experiencing a relevant growth regarding manufacturing, launch, data management, and services, allowing eo to become a new field of opportunities and work."
2,the enhancement of in-orbit eo satellites recording images of the world’s surface increases the amount of generated data to be managed by existing infrastructures.
2,the process of recording data from eos provided by an increasing number of satellites *corresponding author.
2,email: jonathan.becedas@elecnor-deimos.es © 2015 taylor & francis
3,5290 j.
3,becedas et al. generates massive amounts of spatio-temporal geospatial information that has to be intensively processed.
3,this is a major challenge (yang et al. 2011) and a complex problem because the recorded data increase over time.
3,"on-site traditional infrastructures or data centres are not designed to manage variable and increasing amounts of eo data, because they were designed and sized to operate a defined number of satellites and, for instance, to manage a certain data volume."
3,they are then really limited in terms of flexibility and scalability (farres 2012).
3,"two main drawbacks are extracted: (i) traditional infrastructures are not flexible to operate an increasing number of satellites and (ii) there is a risk of oversizing/ undersizing the infrastructure to offer services when highly variable demand exists, such as crisis management, natural disasters monitoring, and civil security, among others (deren 2007)."
3,"thus, the services requested by these users have to be more flexible, allowing instantaneous and easy web access."
3,"in this work, future internet technologies provided by the fed4fire infrastructure were used to experiment a cloud-based system capable of covering the demand of highly variable eo services through the geo-cloud experiment, which is part of the fed4fire fp7 project (becedas 2014)."
3,fed4fire is an integrating project under the european union’s seventh framework programme (fp7) in the topic future internet research and experimentation.
3,the project is performed by a consortium of 29 partners from different countries.
3,the fed4fire project establishes a common and heterogeneous federation framework for experimenters in which a large number of european facilities are integrated.
3,"the fed4fire’s test beds used in the geo-cloud experiment are bonfire cloud, virtual wall platform, as well as planetlab europe and central nodes."
3,"bonfire is a multi-cloud test bed based on an infrastructure as a service (iaas) delivery model with guidelines, policies, and best practices for experimenting (kavoussanakis et al."
3,2013; hume et al.
3,2012).
3,"currently, bonfire is composed of seven geographically distributed test beds that offer heterogeneous cloud services, com- pute resources, and storage resources, though only three are federated with fed4fire: epcc, inria, and ibbt vw."
3,they were used in geo-cloud.
3,"the virtual wall test bed provides an emulation environment for experimenting with advances and customizable networks, distributed software, and service evaluation (kavoussanakis et al."
3,2013).
3,"it allows the experimenters to create any type of network topology, e.g. emulating a large multi-hop topology and client-server topologies, among others."
3,"planetlab offers around 1000 servers distributed worldwide that can be used by the experimenters to perform and check distributed systems, network protocols, and peer-to- peer and network measurements, among other applications."
3,"the system tested in geo-cloud was constituted of (i) a space system simulator implemented in virtual wall reproducing the behaviour of a constellation of 17 satellites and 12 ground stations, and generating the imagery data to be ingested in the cloud, and (ii) a data centre implemented in the bonfire multi-cloud system, which ingested, processed, archived, catalogued, and distributed the images to the end users on demand (pérez et al."
3,2014).
3,the system is based on the recently published project carried out by the research group (becedas et al.
3,2014).
3,"in this document, the same architecture is presented but widely tested and validated in different realistic scenarios:"
4,"international journal of remote sensing 5291 ● scenario 1 – emergencies: disaster monitoring and management of the lorca’s earthquake in spain, which occurred in 2011."
4,● scenario 2 – infrastructure monitoring: affection in railway infrastructures by sand movement in desert areas (united arab emirates).
4,● scenario 3 – land management: land cover and land-cover changes in south west england.
4,"in the previous scenarios, the constellation of satellites acquired the images of the area of interest (aoi)."
4,those images were downloaded into the ground station which the satellites had visibility with.
4,"that ground station receiving the imagery data depends on its own location, the location of the aoi, and on the orbital path of the satellite that acquired the image."
4,"once any image is available at the ground station, it is ingested in the cloud so as to be processed, archived, catalogued, and distributed to end users through a web service."
4,"the whole process was carried out automatically, managed by an orchestrator."
4,this process is in detail described in section 2.2.
4,"the main results of the system, performing under the conditions of the previous scenarios, are analysed."
4,"the processing time of the processors, the required time to deliver a final product to end users from the image acquisition, and real-time scalability of the cloud are included."
4,the article is structured into five sections: section 2 describes the architecture of the data centre implemented in the bonfire multi-cloud system; section 3 is devoted to explain the scenarios tested; section 4 shows the main results of the experiment; and section 5 summarizes the main conclusions of the work.
4,system architecture the entire system was composed of the following subsystems.
4,● the space system simulator: this subsystem reproduces the behaviour of a con- stellation of 17 satellites imaging the earth’s surface and a network of 12 ground stations in which the satellites downloaded the acquisitions.
4,● the geo-images management system: it is the data centre computed in the cloud infrastructure.
4,"it is in charge of the ingestion, processing, and cataloguing of the acquired images from the satellites."
4,the cloud infrastructure used was bonfire.
4,this provides different instance types under the fed4fire infrastructure: ● lite: 0.5 cpu core and 256 mb of memory. ● small: 1 cpu core and 1 gb of memory. ● medium: 2 cpu cores and 2 gb of memory. ● large: 2 cpu cores and 4 gb of memory. ● xlarge: 4 cpu cores and 8 gb of memory with higher cpu clock speed (over 3 ghz).
4,● custom: the number of cores and the memory are user defined.
4,more information about the instance types can be found in the bonfire user documentation document (http://doc.bonfire-project.eu/r4.1/bonfire.pdf).
4,"the instance types used in bonfire to carry out the experiment were xlarge instances with four virtual cpus and 8 gb ram, and medium instances with two virtual cpus and 2 gb ram."
4,the architecture of the data centre is depicted in figure 1.
5,5292 j.
5,becedas et al.
5,figure 1.
5,geo-images management architecture.
5,space system simulator the space system simulator was implemented in virtual wall.
5,this subsystem simulated a constellation of 17 satellites acquiring images of the earth’s surface that were downloaded into a network of 12 ground stations.
5,"the satellite system simulator executed 17 satellite simulators, each of them reproducing the individual behaviour of a specific satellite."
5,"the characteristics of each satellite were the following: resolution 6.7 m, swath 160 km, five bands (four multispectral (rgb and nir) and one panchromatic)."
5,every satellite down- loaded the imagery data at 160 mbps transfer rate.
5,"the ground station system simulator aggregated 12 ground station simulators, each of them reproducing the individual behaviour of a ground station (pérez et al."
5,2014).
5,"the network links of each ground station implemented in virtual wall and bonfire were customized by manually configuring the bandwidth, the loss-rate, and the delay of the network."
5,they were previously measured in an emulated network implemented in planetlab (gonzález et al.
5,2014).
5,these values are summarized in table 1.
5,"data centre architecture implemented in the cloud the architecture was constituted of the orchestrator, the processing chains, and the archive and catalogue modules."
5,the orchestrator performed over the bonfire infrastructure and controlled the interactions between all the components (see figure 2).
5,it developed the following functions: (i) to identify the outputs to be generated by the processors; (ii) to generate the job orders containing all the information needed to make the product processors perform correctly; and (iii) to manage the ingestion of raw data from the satellites.
5,"when a ground station simulator created a new raw data file, the orchestrator down- loaded it through an ftp connection."
5,in case of several raw data sets pending to be
6,international journal of remote sensing 5293 table 1.
6,impairments measured in the network implemented in planetlab..
6,ground station average bandwidth (mbps) average latency (ms) loss rate (%) gs_irkutsk 2.21 242.29 0.024 gs_puertollano 15.59 27.19 0.005 gs_svalbard 7.32 59.23 0.006 gs_troll 1.42 340.90 0.204 gs_chetumal 3.36 154.20 0.010 gs_córdoba 0.20 302.68 0.240 gs_dubai 2.29 70.08 0.001 gs_kourou 1.56 317.20 0.076 gs_krugersdorp 2.29 207.36 0.010 gs_malaysia 2.23 201.51 0.063 gs_prince_albert 3.39 166.63 0.008 gs_sidney 1.33 375.31 0.004 figure 2.
6,"orchestrator interactions. downloaded, a thread for downloading them was created. (iv) to control the proces- sing chains by managing the product processors. (v) to manage the archive and catalogue."
6,the orchestrator was implemented in an xlarge instance in bonfire.
6,it had ftp interfaces with the ground stations and ssh interfaces with the processing chains and with the archive and catalogue.
6,it shall be observed that both ftp and ssh protocols are encapsulated in a tcp message.
6,every processing chain was in charge of processing the raw data from the satellites to produce image products.
6,"the operations that the processing chain performed in different steps (see figure 3) were as follows: (1) l0 level processing: this level decodes the ingested raw data and processes raw data producing unprocessed images, in digital count numbers."
6,(2) l0r level processing: the decoded l0 images were packed into square scenes.
6,they are still unprocessed images.
7,5294 j.
7,becedas et al.
7,figure 3.
7,processing chain diagram.
7,"(3) l1a: once the raw data were converted into square scenes, they were calibrated."
7,the calibration consisted of converting pixel elements from instrument digital count into radiance units.
7,"(4) l1b: the processing in this level consists of (i) doing geometric corrections to eliminate distortions due to misalignments of the sensors in the focal plane geometry and (ii) a geolocation, to compute the geodetic coordinates of the input pixels."
7,(5) l1c level processing: this level orthorectifies the image with vertical projection and free of distortions.
7,precisely geolocation is obtained by using ground control points.
7,the previous product processors are part of the gs4eo commercial suit owned by elecnor deimos; further information of this product can be found in monge et al.
7,(2013).
7,the orchestrator component manages the execution of the processing chains indicat- ing which processing step has to be performed and in which order.
7,"concerning system implementation, several processing chains were requested and instantiated."
7,"when there were new incoming raw data, the orchestrator selected"
8,international journal of remote sensing 5295 one of the processing chains by using a simple load balancing algorithm which started processing the data.
8,"thus, the product processors from l0 to l1c were dynamically and sequentially executed avoiding queues by taking advantage of the flexibility of resources of the cloud platform."
8,"otherwise, not dynamical use of the processing chain would be done and then all the raw data would have been processed by only one instance; i.e. new incoming raw data would be stacked forming a queue and waiting to be processed by a single instance."
8,"with the dynamic processing, the raw data processing can be parallelized in as many processing instances as required."
8,"as a consequence, queuing is not possible in the system since it is able to process all the incoming data from the moment it is received."
8,"in the archive and catalogue, the processed images were stored and catalogued with the following features: the catalogue (i) stored an inventory database with the metadata of archive files and (ii) provided a csw standard interface in the web service."
8,"regarding the storage, two implementations were done."
8,"first by using datablocks (the local storage of the processing chain instances), as shown in figure 2."
8,second by using a shared storage provided by the ibbt test bed in bonfire (see figure 4).
8,the datablock storage consisted of logical disk units locally mounted in a selected compute resource by using a network file system (nfs).
8,only epcc and inria test beds support datablocks in bonfire.
8,"the shared storage consisted of logical disk units, also using nfs protocol, provided by the ibbt test bed only."
8,"furthermore, this test bed only provides shared storage resources, not the compute resources."
8,figure 4.
8,architecture of layer 1 with nfs.
9,5296 j.
9,becedas et al.
9,scenarios description 3.1.
9,"scenario 1 – emergencies on 11 may 2011, an earthquake took place in lorca, in the region of murcia (spain) at 18:47 local time."
9,"it had a moderate magnitude 5.1, in moment magnitude scale (m), and was preceded by a 4.5m foreshock at 17:05 local time."
9,"the seism also affected some other spanish regions, namely almería, albacete, granada, jaén, málaga, alicante, ciudad real, and madrid (instituto geográfico nacional 2011)."
9,"when this type of natural disaster occurs, emergency units and humanitarian organizations demand accurate imagery of the affected area after the shock."
9,"this imagery helps to obtain a preliminary damage assessment and to achieve situational awareness, which enables the identification of possible focus areas where victims can be concentrated, as well as to analyse accesses and evacuation routes."
9,"the hypothesis is the following: after the earthquake, there was a request to acquire an image of the region of murcia in spain."
9,"the area to record was 11,311 km2."
9,the first satellite passing over the aoi was the geo-cloud_011 at 12:10:38 local time.
9,"due to the wide swath of the satellite, the aoi was acquired in a single pass."
9,"since the satellite was inside the visibility area of puertollano ground station (gs_puertollano), the geo-cloud_011 simultaneously downloaded the recorded data there."
9,figure 5 shows an image of the scenario.
9,"it is remarkable that at the same time, the other satellites were acquiring, in parallel, images of other relevant areas and downloading them into other ground stations of the network."
9,"when the raw data were available in the ground station, the orchestrator ingested it into the cloud infrastructure, specifically in the epcc (bonfire) test bed."
9,"then, the raw data were processed in the processing chain module in epcc."
9,"in this module, the raw data passed through the different processing levels: l0 to decode the raw data; l0r to create scenes; l1a to execute a radiometric calibration; l1b to effectuate geometric corrections, to eliminate distortions in the image, and to compute geodetic coordinates in the image; and finally l1c to orthorectify the image."
9,figure 5.
9,image of the lorca’s earthquake scenario.
10,"international journal of remote sensing 5297 in every processing level, the different products l0–l1c were produced and trans- ferred to the archive and catalogue located in the inria test bed."
10,"once there, the images were directly accessible from a web service also provided by the inria test bed."
10,the previous steps from the ingestion of raw data images in the cloud to their distribution through a web service were repeated for scenarios 2 and 3.
10,"scenario 2 – infrastructures monitoring new high-performance railway lines at ambient conditions are affected by the presence of wind, sand dunes, sand in the air, and high temperatures."
10,innovative technologies are required to minimize the impact of these phenomena on different infrastructure elements during the operation.
10,it is the case of the railway infrastructure to join medina and mecca in saudi arabia.
10,"in this use case, the satellite constellation was used to monitor the area at different time intervals (considering monthly intervals) to detect changes in the land, for example, the sand dunes movements, and to provide data to assist in the deployment, operation, and maintenance of the railway tracks."
10,the length to cover with images between the two cities was 440 km.
10,"as the constellation repeated the world coverage on a daily basis, a simulation of the acquisition of one day was enough to evaluate the system."
10,"to record the route between the two cities, three satellites were utilized to cover the aoi in consecutive passes."
10,"the scenario started with the satellite geo-cloud_004 acquiring the first image at 10:31:00 local time, then the satellite geo-cloud_003 acquiring the second image at 10:36:30 local time, and finally the satellite geo-cloud_002 acquiring the last image at 10:42:10; a total of 76,800 km2."
10,all the acquired images were downloaded into the dubai ground station (gs_dubai) which the satellites had visibility during the scenario with.
10,"once downloaded the raw data in the ground station, it was ingested in the cloud by the orchestrator to start the image processing and distribution of the final products as explained in scenario 1."
10,"it is remarkable that at the same time, the other satellites were acquiring, in parallel, images of other relevant areas and downloading them into other ground stations."
10,the other relevant areas are not described to simplify the explanation and to focus on the scenario.
10,figure 6 shows an image of the scenario.
10,"scenario 3 – land management there is a need to characterize landscape and crops in south west england, uk."
10,"besides, some users demand a multi-temporal imagery product to analyse and detect changes in the land."
10,"in order to carry out this task, multi-temporal classification and regression techniques are used to provide products exploiting the high revisit time, high spatial resolution, high swath, and large number of bands in the system."
10,0the primary focus of the service was landscape management linked to the use of the land for agricultural purposes versus conservation.
10,multi-temporal land-cover classification and change detection services were needed.
10,land-cover and land analysis products will be generated from the acquired images weekly.
10,"so as to develop this task, a scenario was executed in one day as an example of a typical acquisition day."
10,"the aoi had a size that required five satellites consecutively acquiring images of the land: geo-cloud_015, geo-cloud_014, geo- cloud_013, geo-cloud_012, and geo-cloud_011."
10,the geo-cloud_015 satellite sets the start of the scenario with the beginning of the first acquisition at 10:51:20 local time.
10,"during the acquisition of the aoi, all the satellites had visibility with puertollano ground station (gs_puertollano)."
10,the end of the acquisition scenario
11,5298 j.
11,becedas et al.
11,figure 6.
11,"image of the arabia saudi’s scenario. was 11:14:25 local time, when the satellite geo-cloud_011 finished downloading the last image of the aoi."
11,the area to be acquired was 23.837 km2.
11,figure 7 depicts an image of the scenario.
11,"again, when a set of raw data was available in the ground station, it was transferred to the cloud to be processed and distributed."
11,the performance of the system computed in cloud for the three previous scenarios is analysed along the results description within the next section.
11,figure 7.
11,image of south west england’s scenario.
12,international journal of remote sensing 5299 4.
12,results 4.1.
12,"architectures testing and validation the processing chains were tested in the architecture with local storage (datablocks), shown in figure 1, and in the architecture with shared storage, shown in figure 4."
12,"to validate the architecture, the processing time of scenario 1 (emergencies) was used."
12,the results of the processing chain working in the two architectures proposed are shown in figure 8.
12,there was a high difference in the processing time when the architecture with datablocks and shared storage were compared in bonfire.
12,this fact can be noticed in the accumulated diagram.
12,"the processing time of the whole chain using shared storage was almost 16.93 hours (16 hours, 55 min), while using datablocks was 1.432 hours (1 hours, 25 min, 54 s)."
12,that difference was produced because the processing chain was continu- ously consulting the stored images to compare pixels and metadata.
12,"since the shared storage was located in the ibbt test bed (ghent, belgium) and the processing chain in the epcc test bed (edinburgh, uk), the processing time increased."
12,all the i/o operations such as readings and writing were performed using the internet network.
12,"the delays on the communications, the congestion of the network, and the packets distribution algo- rithms of the network caused more delays than when datablocks located in the epcc figure 8."
12,products’ processing time. (a) per stage. (b) accumulated.
13,5300 j.
13,becedas et al.
13,table 2.
13,imagery data products and sizes.
13,"data type raw l0 l0 r l1a l1br l1bg l1 c size (mb) 289 409 723 1198 2785 4730 3061 nodes were used, as the transfer time between the memory and the processors was highly reduced."
13,"to corroborate this hypothesis, performance tests were done in the shared and in the datablocks local storages."
13,these tests consisted of executing the dd software copying information in both storage configurations.
13,"with the shared storage, 2.7 gb of informa- tion data were copied in 1174.6 s (at 2.3 mb s–1) and in the local storage (datablock), 2.7 gb were copied in 50.97 s (52.7 mb s–1)."
13,"consequently, it can be concluded that the implementation of the architecture with shared storage produced a bottleneck in the system performance."
13,"thus, the architecture with datablocks was selected as a baseline for the experimentation."
13,the generated products with a raw data of 289 mb size are described in table 2.
13,the sum of all the products generated with that raw data was 12.60 gb.
13,"to obtain a representative value of the processing time of each level, several tests were carried out for l0, l0r, l1bg, and l1br 1146 tests with each level; for l1a, 1145; and for l1c, 571."
13,"from the data set, the mean and the standard deviation values were obtained."
13,table 3 shows the values for every processing level.
13,the orthorectification process was the most demanding one in terms of processing time.
13,"thus, we can affirm that the whole processing chain finished generating imagery products after 1 hour 51 min and 3 s (mean)."
13,"in table 4, the ingestion time of 289 mb raw data in the cloud from each ground station is shown."
13,the mean time required to archive and catalogue the products was 44.05 s with a standard deviation of 5.10 s.
13,this result was obtained after analysing 1146 samples.
13,"to test the transfer network between the simulated ground stations in virtual wall and the bonfire cloud, the impairments described in table 1 (obtained after experimentation with real nodes in planetlab europe and central) were implemented."
13,"although the impairments were measured in an emulated network using planetlab nodes near the real ground stations and not in the real ground stations, a value to test the system in real time is provided to perturb it and to observe how it performed."
13,the ingestion time of 289 mb raw data in the cloud from each simulated ground stations is depicted in table 4.
13,"it shall be noticed that between virtual wall and bonfire, a bandwidth of 235 mbps was available for the experiment, which was also shared with other experimenters."
13,this table 3.
13,processing time per stage using the architecture with datablocks.
13,processing level mean processing time standard deviation samples l0 0:01:39 0:00:17 1146 l0 r 0:01:19 0:00:12 1146 l1a 0:01:29 0:00:22 1145 l1bg 0:14:02 0:02:28 1146 l1br 0:37:41 0:24:51 1146 l1 c 0:54:52 00:11:18 571
14,international journal of remote sensing 5301 table 4.
14,ingestion times in the cloud from the ground stations.
14,ground station average ingestion time to the cloud standard deviation samples gs_chetumal 1:03:13 0:01:21 1221 gs_cordoba 0:43:50 0:14:13 1221 gs_dubai 0:47:53 0:05:27 1221 gs_irkutsk 0:54:14 0:07:25 1221 gs_kourou 0:42:47 0:15:22 1221 gs_krugersdorp 0:55:02 0:11:30 1221 gs_malaysia 0:52:58 0:08:51 1221 gs_prince_albert 0:48:30 0:09:07 1221 gs_puertollano 0:35:36 0:21:48 1221 gs_svalbard 0:35:13 0:19:09 1221 gs_sydney 1:02:38 0:10:41 1221 gs_troll 1:00:29 0:12:37 1221 means that a transfer of a large number of images to the epcc bonfire test bed in edinburgh could saturate all the available bandwidth.
14,"anyway, such ingestion time would be highly reduced if the cloud had nodes near the real ground stations."
14,"scenario 1 – emergencies results the experiment started when an image of the city of lorca was requested, at 20:00 local time on 11 may 2014."
14,the first satellite passing over the city with optimal illumination conditions was the geo-cloud_011 at 12:10:36 next morning.
14,it entered the aoi and started the image recording and the simultaneous download into the gs_puertollano ground station.
14,"after 23.4 s (12:10:59), the geo-cloud_011 finished downloading the image into the ground station."
14,"during that time, the satellites geo-cloud_005, geo- cloud_006, geo-cloud_008, and geo-cloud_013 downloaded four images recorded out of the aoi."
14,"the geo-cloud_005 and geo-cloud_006 satellites downloaded the images into the simulated gs_troll ground station, while the geo-cloud_008 in gs_krugersdorp and the geo-cloud_013 in gs_svalbard."
14,"once a ground station receives new available data, the orchestrator ingests it into the bonfire cloud."
14,"in this scenario, 23.4 s after the start of the acquisition, the 302 mb of raw data were ingested from the gs_puertollano ground station at a transfer rate of 15.59 mbps (see table 1)."
14,"after 12 s (12:10:59), the image reached the cloud and the orchestrator launched a processing chain to process the raw data."
14,"once the processing chain was activated, the raw data were processed to produce l0, l0r, l1a, l1b, l1br, and l1c products."
14,"the first deliverable image was the l1a product, a geo-referenced image with radio- metric calibration, which was processed in 4 min and 41 s after the ingestion (12:15:52)."
14,"after processing, the resultant image was archived and catalogued."
14,archiving started 36 s after the image was sent from the processing chain.
14,cataloguing was instantaneously done without any delay.
14,the l1a product was available in the online web catalogue at 12:16:24.
14,"an orthorectified image would have been available 1 hour 21 min and 17 s later, i.e. at 13:37:41."
14,"in table 5, a summary with the description of the scenario is depicted."
14,"in order of appearance (in columns, from left to right) in the table, the following information is provided: ● the satellite that takes the image of the scenario."
14,● the ground station in which the data are downloaded.
15,5302 j.
15,becedas et al.
15,table 5.
15,summary description of scenario 1.
15,downloading ingestion l1 c ground acquisition end time in no. of time in the product catalogue satellite station start time gs images cloud time time g-c_011 gs_puertollano 12:10:36 12:10:59 1 12:11:11 13:37:05 13:37:41 ● the local time when the satellite starts recording.
15,● the time when the data are fully downloaded into the ground station.
15,● the number of images acquired and transferred to the cloud by the specific satellite.
15,● the time when the data are fully ingested in the cloud.
15,● the time when the last product is obtained: the l1c.
15,● the time when the image is ready to be accessible from the catalogue.
15,"this process can be compared to a typical use case: an eo satellite that takes an image and downloads it into a ground station in less than one orbital period (around 90 min in low earth orbits); after the ingestion, it is processed and ready to be delivered."
15,"then, the first image would have been available 8 hours later considering the typical case, at 20:00 (see figure 9)."
15,"the response time of the typical case has been adapted from révillon (2012), setting the same time to start acquiring the aoi."
15,this means that the solution proposed in geo-cloud has enough potential to reduce the response time in emergen- cies 7 hours and 44 min with respect to conventional solutions.
15,"this time reduction is achieved due to the following reasons: first, the system and the network of ground stations distribution around the world enabled a simultaneous acquisition and a down- load of images; second, the total time from ingestion of raw data to the distribution of the final product is improved because of the automatic execution of the system, in which no human operation is required; third, the availability of elastic resources facilitates the figure 9."
15,comparison of geo-cloud emergencies response with typical use case.
16,international journal of remote sensing 5303 table 6.
16,summary description of scenario 2.
16,"downloading ingestion l1 c ground acquisition end time in no. of time in the product catalogue satellite station start time gs images cloud time time g-c_004 gs_dubai 10:31:00 10:31:23 1 11:28:45 13:08:00 13:08:52 g-c_003 gs_dubai 10:36:30 10:36:53 1 11:33:09 13:10:48 13:11:37 g-c_002 gs_dubai 10:42:10 10:42:33 1 11:39:36 13:45:13 13:46:52 parallelization of the image processing in different processing chains, which eliminates any possible queue in the system and, subsequently, any delay derived from the waiting time required to dispatch imagery."
16,scenario 2 – infrastructures monitoring results the time when the satellites acquire the images and finish their download in the gs_dubai are depicted in table 6.
16,it shall be noticed that in 11 min and 10 s the whole aoi was acquired.
16,"during that time, 295 additional images were acquired by the rest of the satellites."
16,"when the gs_dubai finished downloading the data from the satellite, the orchestrator, which is pooling over the gs, detected that there were new data to be ingested in the cloud and started the data transfer."
16,"when the new image in raw data was ingested, a processing chain was executed to produce l0, l0r, l1a, l1b, l1br, and l1c products."
16,"in table 6, the local time when each image was processed at l1c level and catalogued is depicted."
16,"the imagery products of the whole scenario were available at 13:46:52 from the acquisition, which started at 10:31:00."
16,"the computing resources used in this scenario to generate the products of the aoi were the following: the orchestrator (xlarge), three processing chains (xlarge), and the archive and catalogue (medium)."
16,the total amount of generated data of the scenario was 37.8 gb.
16,"since this scenario was daily repeated, it was easy to extrapolate the results to the infrastructure monitoring scenario with the sample time desired."
16,"we considered monthly intervals; thus, every month at 13:46:52 a new set of products with 37.8 gb size of the aoi (three sets of imagery data) was obtained similarly to the above-mentioned process."
16,scenario 3 – land management results every satellite involved in the scenario during the acquisition was in contact with the ground station gs_puertollano.
16,the time when each satellite acquired the images and finished downloading them in the ground station is depicted in table 7.
16,it shall be observed that in 11 min and 10 s the whole aoi was acquired.
16,"during that time, 610 additional images were acquired by the rest of the satellites."
16,"when the orchestrator detected new data in the gs_puertollano, it ingested the data into the cloud."
16,"then, a processing chain started processing the new data to generate all the products."
16,"in table 7, the local time when each image was processed at l1c level and catalogued is depicted."
17,5304 j.
17,becedas et al.
17,table 7.
17,summary description of scenario 3.
17,"downloading ingestion l1 c ground acquisition end time in no. of time in product catalogue satellite station start time gs images the cloud time time g-c_015 gs_puertollano 10:51:20 10:51:43 1 12:06:23 13:46:54 13:47:41 g-c_014 gs_puertollano 10:57:01 10:57:24 1 12:11:30 13:57:36 13:58:26 g-c_013 gs_puertollano 11:02:41 11:03:04 1 11:50:43 13:32:10 13:33:01 g-c_012 gs_puertollano 11:08:21 11:08:44 1 12:22:06 13:42:31 13:43:32 g-c_011 gs_puertollano 11:14:02 11:14:25 1 11:57:48 14:02:55 14:03:40 the images of the whole aoi, whose acquisition started at 10:51:20, were available at 14:03:40."
17,"the computing resources used in this scenario were the following: the orches- trator (xlarge), five processing chains (xlarge), and the archive and catalogue (medium)."
17,the total amount of generated data of the scenario was 63 gb.
17,"as in the previous case, this scenario was daily repeated so it was easy to extrapolate the results to a monitoring time frame."
17,"in this case, a week interval is considered."
17,"taking into account four weeks a month, 252 gb (20 sets of imagery data) of data were generated every month for this use case."
17,"conclusions the architecture implemented in the bonfire multi-cloud system, which is described in this article, is able of processing and distributing satellite imagery on demand."
17,it was demonstrated that using local storage for image processing was faster than using the nfs shared storage: the processing time of the whole processing chain was 1 hour and 51 min using local storage versus 16 hours 55 min using shared storage.
17,this was motivated because the processing chain required accessing to the system and to the stored images and metadata to process the images.
17,having a distributed system of files for storage delayed the image processing.
17,"it is worth to mention that the automation of the whole process from acquisition until having the images in the catalogue highly reduced the delivery times: l1a products were available after 5 min from the raw data ingestion in the cloud, and l1c after 1 hour and 51 min and 47 s if urgency was required (mean time)."
17,"thus, cloud computing for eo image applications can reduce the time to deliver the imagery products to the end user since scalability allows parallel processing and automatic archiving and cataloguing of imagery products."
17,"in addition, the effect of transferring satellite data to the bonfire cloud from several ground stations distributed around the world was also evaluated."
17,"as data needed to be transferred through the internet, delays reduced the response time of the system."
17,this situation can be mitigated by using dedicated communication channels.
17,"however, trans- ferring imagery data from all the ground stations to nodes in europe through dedicated channels with high bandwidth would have a high economic cost."
17,"in this case, the cloud has a benefit since several public clouds offer nodes distributed around the world."
17,"this way, to send satellite data from the ground stations to a cloud node near the ground station would widely reduce the costs and the transfer delays."
17,"consequently, cloud computing facilitates the transference of data from the ground stations distributed around the world to commercial cloud nodes."
18,"international journal of remote sensing 5305 furthermore, the cloud offers a clear solution when processing massive amounts of data."
18,"as demonstrated in the scenarios presented in this work, a large land area in south west england (23.837 km2) can be acquired in 23 min, and different imagery products, including orthorectified images, would be available in 1 hour 51 min and 47 s mean time (processing time of the whole chain 1:51:03 plus cataloguing 44 s), thanks to the elasticity and on-demand characteristics of the cloud."
18,"even a larger area, for example, the surface of argentina (2.780.400 km2), would be acquired in 1 hour and 49 min with the constella- tion, and it would also be available 1 hour 51 min and 47 s later."
18,"the previous affirmation means that if one image is ingested in the cloud, 1 hour and 51 min later an orthorectified product is obtained."
18,"but it also means that if several images are ingested at the same time, let’s say tenths or hundreds, the time to process all the images is the same: 1 hour and 51 min."
18,this achievement reduces the time to deliver the product to the user since any set of images can be processed in parallel.
18,"another issue to take into account is the fact that the constellation downloaded 1.64 tb of raw data after compression daily, which was transferred to the cloud."
18,this means that 73.24 tb of imagery products were generated every day.
18,"with cloud computing, this amount of data can be managed on demand, using an already deployed infra- structure."
18,"if a private infrastructure should have to be deployed, this would require time to appropriately design the infrastructure and to carry out its deployment, testing, and maintenance."
18,these are time and cost saved by using a cloud infrastructure.
18,"consequently cloud computing for eo applications reduces the total cost of ownership and eliminates the costs and effort of deploying, dimensioning, testing, and maintain- ing a traditional infrastructure."
18,it is also concluded that the virtualization of the instances affected the processing time of the imagery data.
18,fully dedicated resources would provide more stable processing time.
18,"however, cloud infrastructures servers are located in different sites, not only in different countries, as bonfire, but also in different continents."
18,"this can be either an advantage, as explained before, or a drawback provided that the control of the files storage location is required due to legal aspects of some applications or countries."
18,cloud computing also contributes to globally distribute the imagery products obtained.
18,any user in any part of the world can access a web service and can visualize or download the imagery products.
18,"furthermore, if many users are accessing the products at the same time, auto-scaling of the cloud resources contributes to offer on-demand services."
18,"if a private infrastructure is used, this cannot be done."
18,"in that case, the communications would be saturated and the experience of the user would not be pleasant."
18,one of the main objectives in the project was the evaluation of the social and economic viability of using cloud computing for eo services.
18,"economic issues arise as an important handicap concerning storage capability, though the scalability of the system and the reduced cost of on-demand processing and distribution (which is possible thanks to the cloud computing performances) are positive reasons to consider an economic advantage versus traditional on-premises infrastructures."
18,social aspects that affect the comparison between cloud computing and conventional eo infrastructures are not easily identifiable.
18,"intrinsic performances as security in cloud shall be considered; it is an important subject to be solved in the next years, in which numerous companies are currently working on."
18,"another social benefit of the use of cloud computing is the robustness and efficiency of the geo-information thanks to the reduction of the processing, storage, communication, and distribution costs that facilitate the access to remote-sensing technologies to more users."
19,5306 j.
19,becedas et al.
19,"the impact of using the technology described in this article can be summarized as follows: ● cloud computing provides the externalization of the infrastructure to the eo business, so companies in the eo field can fully dedicate their resources to their real business."
19,"● furthermore, cloud computing offers the possibility to implement different archi- tectures for eo processing and distribution of products."
19,it has to be taken into account that a specific architecture could reduce the performance of the system because of the distribution and virtualization of nodes.
19,"however, cloud reduces data transfer rates, which highly increases the performance and reduces the delivery time to user."
19,"● another issue that will be considered is cloud computing offers on-demand ser- vices, so the costs related to the use of resources are variable and can be highly reduced since no consume means no cost."
19,"however, in the case of eo, satellite imagery has to be stored during years."
19,"this aspect increases the costs, so a cost optimization exercise should be done before transferring the business totally to the cloud."
19,"however, the elasticity and scalability characteristic of the system can be an asset in cost reduction since on-demand processing can also be considered."
19,"● in addition, cloud computing distribution in nodes reduces the transfer time and costs between the satellite antennas and the data centre."
19,"in a problem as the one considered in geo-cloud, with a globally distributed network of antennas to guarantee global coverage, it is a very interesting solution."
19,some clouds have distributed nodes around the world.
19,it is very interesting to connect the antennas to the nearest node in the cloud.
19,"this action highly reduces the costs between the antennas and a traditional data centre, which requires dedicated communication lines between the antennas and the on-premises infrastructure."
19,it is interesting to mention that cloud computing opens the door to the appearance of new applications in remote sensing.
19,"in geo-cloud, we tested several scenarios, one of them a natural disaster."
19,it was demonstrated that in less than 5 min an image in l1a processing level of the aoi was ready to be visualized in the web service.
19,"thus, cloud computing can generate new opportunities, jobs, and applications in the eo field."
19,disclosure statement this article does not necessarily reflect the views of the european commission.
19,the european commission is not liable for any use that may be made of the information contained herein..
19,"funding this work was carried out with the support of the fp7 fed4fire-project (‘federation for fire’), an integrated project funded by the european union’s seventh framework programme (fp7) for research, technological development, and demonstration [grant agreement number 318389]."
19,"references becedas, j."
19,“the geo-cloud experiment: global earth observation system computed in cloud.”
19,"the 8th international conference on innovative mobile and internet services in ubiquitous computing, birmingham, july 2–4."
20,"international journal of remote sensing 5307 becedas, j., r."
20,"pérez, g."
20,"gonzález, f."
20,"pedrera, and m."
20,latorre.
20,“validation of an experimental cloud infrastructure for earth observation services.”
20,"raqrs2014, 4th international symposium recent advances in quantitative remote sensing torrent, valencia, september 22–26."
20,"deren, l."
20,“remote sensing can help monitoring and predication natural disasters.”
20,science & technology review 25 (6): 3.
20,euroconsult.
20,satellite-based earth observation: market prospects to 2019.
20,paris: euroconsult.
20,euroconsult.
20,satellite-based earth observation: market prospects to 2023.
20,paris: euroconsult.
20,"farres, j."
20,“cloud computing and content delivery network use within earth observation ground segments: experiences and lessons learnt.”
20,"accessed june 6. http://earth.esa.int/ gscb/papers/2012/11-cloud_computing_content_delivery.pdf gonzález, g., r."
20,"pérez, j."
20,"becedas, m."
20,"latorre, and f."
20,pedrera.
20,"“measurement and modelling of planetlab network impairments for fed4fire’s geo-cloud experiment.” 26th ieee international teletraffic congress, karlskrona, september 9–11, 1–4."
20,"hume, a."
20,"c., y."
20,"al-hazmi, b."
20,"belter, k."
20,"campowsky, l."
20,"carril, g."
20,"carrozzo, and g."
20,van seghbroeck.
20,“bonfire: a multi-cloud test facility for internet of services experimentation.”
20,in testbeds and research infrastructure.
20,"development of networks and communities, edited by t."
20,"korakis, m."
20,"zink, and m."
20,"ott, 81–96."
20,berlin: springer.
20,"instituto geográfico nacional, universidad complutense de madrid, universidad politécnica de madrid, instituto geológico y minero de españa, and asociación española de ingeniería sísmica."
20,informe del seismo de lorca del 11 de mayo de 2011.
20,madrid: instituto geográfico nacional.
20,"kavoussanakis, k., a."
20,"hume, j."
20,"martrat, c."
20,"ragusa, m."
20,"gienger, k."
20,"campowsky, g."
20,"seghbroeck, c."
20,"vazquez, c."
20,"velayos, f."
20,"gittler, p."
20,"inglesant, g."
20,"carella, v."
20,"engen, m."
20,"giertych, g."
20,"landi, and d."
20,margery.
20,"“bonfire: the clouds and services testbed”. 5th ieee international conference on cloud computing technology and science, bristol, december 2–5."
20,"keith, a."
20,"emerging markets, partnerships set to fuel global growth, 1–3."
20,"denver, co: earthwide communications llc."
20,"monge, a., s."
20,"negrin, j."
20,"gonzález, a."
20,"ortiz, o."
20,"gonzález, and d."
20,lozano.
20,"“gs4eo: a new ground segment for earth observation missions.” 6th international astronautical congress, pasadena, ca, september 23–27."
20,"pérez, r., g."
20,"gonzález, j."
20,"becedas, f."
20,"pedrera, and m."
20,latorre.
20,"“testing cloud computing for massive space data processing, storage and distribution with open-source geo-software.”"
20,"foss4g-e2014, osgeo’s european conference on free and open source software for geospatial bremen, germany."
20,"révillon, p."
20,"“earth observation market revolution and drivers.” e-geos international conference, rome, may, 24."
20,"accessed november. http://www.e-geos.it/news/meeting12/presen tations/24-re%81lvillon-euroconsult.pdf yang, c., m."
20,"goodchild, q."
20,"huang, d."
20,"nebert, r."
20,"raskin, y."
20,"xu, m."
20,"bambacus, and d."
20,“spatial cloud computing: how can the geospatial sciences use and help shape cloud computing?”
20,international journal of digital earth 4 (4): 305–329. doi:10.1080/ 17538947.2011.587547.
