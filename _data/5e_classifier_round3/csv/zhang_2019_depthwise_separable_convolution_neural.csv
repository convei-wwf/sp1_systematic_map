page,p
1,"remote sensing article depthwise separable convolution neural network for high-speed sar ship detection tianwen zhang *, xiaoling zhang, jun shi and shunjun wei school of information and communication engineering, university of electronic science and technology of china, chengdu 611731, china; xlzhang@uestc.edu.cn (x.z.); shijun@uestc.edu.cn (j.s.); weishunjun@uestc.edu.cn (s.w.)"
1,"* correspondence: twzhang@std.uestc.edu.cn; tel.: +86-17381580825  received: 19 september 2019; accepted: 17 october 2019; published: 24 october 2019  abstract: as an active microwave imaging sensor for the high-resolution earth observation, synthetic aperture radar (sar) has been extensively applied in military, agriculture, geology, ecology, oceanography, etc., due to its prominent advantages of all-weather and all-time working capacity."
1,"especially, in the marine field, sar can provide numerous high-quality services for fishery management, traffic control, sea-ice monitoring, marine environmental protection, etc."
1,"among them, ship detection in sar images has attracted more and more attention on account of the urgent requirements of maritime rescue and military strategy formulation."
1,"nowadays, most researches are focusing on improving the ship detection accuracy, while the detection speed is frequently neglected, regardless of traditional feature extraction methods or modern deep learning (dl) methods."
1,"however, the high-speed sar ship detection is of great practical value, because it can provide real-time maritime disaster rescue and emergency military planning."
1,"therefore, in order to address this problem, we proposed a novel high-speed sar ship detection approach by mainly using depthwise separable convolution neural network (ds-cnn)."
1,"in this approach, we integrated multi-scale detection mechanism, concatenation mechanism and anchor box mechanism to establish a brand-new light-weight network architecture for the high-speed sar ship detection."
1,"we used ds-cnn, which consists of a depthwise convolution (d-conv2d) and a pointwise convolution (p-conv2d), to substitute for the conventional convolution neural network (c-cnn)."
1,"in this way, the number of network parameters gets obviously decreased, and the ship detection speed gets dramatically improved."
1,we experimented on an open sar ship detection dataset (ssdd) to validate the correctness and feasibility of the proposed method.
1,"to verify the strong migration capacity of our method, we also carried out actual ship detection on a wide-region large-size sentinel-1 sar image."
1,"ultimately, under the same hardware platform with nvidia rtx2080ti gpu, the experimental results indicated that the ship detection speed of our proposed method is faster than other methods, meanwhile the detection accuracy is only lightly sacrificed compared with the state-of-art object detectors."
1,our method has great application value in real-time maritime disaster rescue and emergency military planning.
1,keywords: synthetic aperture radar (sar); ship detection; high-speed; convolution neural network (cnn); depthwise separable convolution neural network (ds-cnn); depthwise convolution (d-conv2d); pointwise convolution (p-conv2d) 1.
1,"introduction synthetic aperture radar (sar), an active microwave remote sensing imaging radar capable of observing the earth’s surface all-day and all-weather, has a wide range of applications in military, agriculture, geology, ecology, oceanography, etc."
1,"in particular, since the united states launched the remote sens. 2019, 11, 2483; doi:10.3390/rs11212483 www.mdpi.com/journal/remotesensing"
2,"remote sens. 2019, 11, 2483 2 of 37 first civil sar satellite to carry out ocean exploration in 1978 [1], sar has begun to be constantly used in the marine field, such as fishery management [2], traffic control [3], sea-ice monitoring [4], marine environmental protection [5], ship surveillance [6,7], etc."
2,"among them, in recent years, ship detection in sar images has become a research hotspot for its broad application prospects [8,9]."
2,"on the military side, it is conducive for tactical deployment and ocean defense early warning."
2,"on the civil side, it is also beneficial for maritime transport management and maritime distress rescue."
2,"however, despite of the wide practical value in the military and civil side, up to now, sar ship detection technology is still lagging behind optical images due to their different imaging mechanisms [10]."
2,"therefore, many constructive methods are emerging to solve this problem, which promote the development of sar interpretation technology persistently."
2,"according to our investigation, these multifarious methods can be mainly divided into two categories: (1) traditional feature extraction methods; (2) modern deep learning (dl) methods."
2,the most noteworthy characteristic of the traditional feature extraction methods is the manual feature extraction.
2,"in these ways, ships can be distinguished from ports, islands, etc., through gray level, texture, contrast ratio, geometric size, scattering characteristics, the scale-invariant feature transform (sift) [11], haar-like (haar) [12], histogram of oriented gradient (hog) [13], etc."
2,"among them, the constant false alarm rate (cfar) is one of the typical algorithms."
2,"when performing ship detection, cfar detectors provide a threshold, which needs to avoid noise background clutter and interference as far as possible, to detect the existence of ships."
2,"therefore, it is essential to establish an accurate clutter statistical model for cfar detectors."
2,"among them, some frequently-used clutter statistical models are based on gauss-distribution [14], rayleigh-distribution [15], k-distribution [16], and weibull-distribution [17]."
2,"however, for these cfar detectors, there are still two drawbacks hindering its development and application."
2,"on the one hand, it is bitterly challenging to build an accurate clutter model."
2,"on the other hand, these established models are inevitably vulnerable to sea clutter and ocean currents."
2,"therefore, the application scenarios are limited and the migration capacity is also weak."
2,"even worse, in the practical applications, it takes a lot of time to solve lots of parameters of the above distribution equations, leading to a slower detection speed."
2,"another common traditional method is the template-based detection [18,19]."
2,"this method considers both the characteristics of specific targets and that of backgrounds, which has achieved good detection performance."
2,"however, these detection templates are frequently dependent on the expert experience and deficient in mature theories, which may lead to their poor migration ability."
2,"in addition, the template matching in wide-region sar images is also time-consuming, which undoubtedly declines the detection speed."
2,"in summary, these traditional feature extraction methods are complex in computation, weak in generalization, and troublesome in manual feature extraction."
2,"most importantly, the detection speed of these methods is too slow to satisfy real-time ship detection in the practical application occasion."
2,the most noteworthy characteristic of the modern dl methods is the automatic feature extraction [20].
2,"by this means, it is unnecessary to design features via manual participation, simply and efficiently."
2,"as long as given a labeled dataset, computers can train and learn like human brains to accomplish accurate object detection tasks."
2,"for this reason, nowadays, more and more scholars are following the path of artificial intelligence (ai) to achieve sar ship detection."
2,"generally, there are two main types of object detectors in the dl field [21]: (1) two-stage detectors; (2) one-stage detectors."
2,two-stage detectors usually assign detection tasks into two stages: acquisition of region of interest (roi) and classification of roi.
2,region-convolutional neural network (r-cnn) [22] is the first one to realize object detection via dl methods.
2,"r-cnn used the selective search [23] to generate rois, then extracted the features of rois by cnns, and finally, classified rois by support vector machine (svm) [24]."
2,it made the mean average precision (map) on the pascal voc dataset [25] a great increment compared with previous traditional methods.
2,"however, due to its huge amount of calculation, the detection speed of r-cnn is notoriously slow, leading to impracticability in industry."
2,"to improve the detection speed, a fast region-convolutional neural network (fast r-cnn) was proposed by girshick et al. [26], which draws some experience from spatial pyramid pooling"
3,"remote sens. 2019, 11, 2483 3 of 37 network (spp-net) [27]."
3,"fast r-cnn added an roi pooling layer and realized feature sharing, as a result, its speed is 213 times faster than r-cnn [26]."
3,"however, the roi extraction process still takes up most of the detection time, reducing its detection efficiency."
3,"therefore, faster region-convolutional neural network (faster r-cnn) [28] was proposed to simplify this process by a region proposal network (rpn), as a result, its speed is 10 times faster than fast r-cnn [28]."
3,"so far, faster r-cnn is still one of the mainstream object detection methods in the dl field [29]."
3,"from these emerged methods, we can summarize that the development tendency of two-stage detectors is to improve the detection speed."
3,"nevertheless, two-stage detectors inherently need to obtain region recommendation boxes in advance, leading to their heavy computation."
3,"therefore, there are some intrinsic technical bottlenecks in improving detection speed, so they scarcely meet the real-time requirement."
3,"hence, one-stage detectors emerged to simplify the detection process."
3,one-stage detectors achieve detection tasks directly based on position regression.
3,"redmon et al. [30] proposed the first one-stage detector, called you only look once (yolo), which processes the input image only once."
3,"in this way, the amount of calculation gets decreased further and the detection speed gets improved further, compared with two-stage detectors."
3,"however, its detection performance on neighboring targets and small ones is not ideal, because each grid is responsible for predicting only one target."
3,"therefore, they also proposed an improved version called yolov2 [31] based on anchor box mechanism and feature fusion."
3,"in yolov2, a more robust backbone network darknet-19 [31] was proposed to improve the detection accuracy."
3,"however, the accuracy of yolov2 is still far inferior to two-stage detectors."
3,"finally, yolov3 [32] was proposed to increase the detection accuracy further, by using darknet-53 [32] and multiscale mechanism."
3,"in addition, another two common one-stage detectors are single shot multi-box detector (ssd) proposed by liu et al. [33] and retinanet proposed by lin et al."
3,ssd combined the advantages of yolo and faster r-cnn to achieve a balance between accuracy and speed.
3,"retinanet proposed a focal loss to solve the extreme imbalance between positive and negative sample regions, which can improve accuracy."
3,"however, their detection speed is inferior to yolo."
3,"from these emerged methods, we can summarize that the development tendency of one-stage detectors is to improve the detection accuracy."
3,"from the comparison of two-stage detectors and one-stage detectors, both accuracy and speed are extremely significant for object detection."
3,"however, nowadays, in the sar ship detection field, many types of researches are focusing on improving the ship detection accuracy."
3,"unfortunately, there are few studies on improving the ship detection speed."
3,"however, it definitely cannot be ignored as high-speed sar ship detection is of great practical value, especially in the cases of maritime distress rescue and war emergency scenarios."
3,"therefore, in order to address this problem, in this paper, a novel high-speed sar ship detection approach was proposed by mainly using depthwise separable convolution neural network (ds-cnn)."
3,"a ds-cnn is composed of a depthwise convolution (d-conv2d) and a pointwise convolution (p-conv2d), which substitute for the conventional convolution neural network (c-cnn), by which the number of network parameters get greatly decreased."
3,"consequently, the detection speed gets dramatically improved."
3,"particularly, a brand-new light-weight network architecture, which integrates multi-scale detection mechanism, concatenation mechanism and anchor box mechanism, was exclusively established for the high-speed sar ship detection."
3,we experimented on an open sar ship detection dataset (ssdd) [35] to verify the correctness and feasibility of the proposed method.
3,"in addition, we also carried out actual ship detection on a wide-region large-size sentinel-1 sar image to verify the strong migration ability of the proposed method."
3,"finally, the experimental results indicated that our method can achieve high-speed sar ship detection, authentically."
3,"on the ssdd dataset, it only takes 9.03 ms per sar image for ship detection (111 images can be detected per second)."
3,"the detection speed of the proposed method is faster than other methods, such as faster r-cnn, retinanet, ssd, and yolo, under the same hardware platform with nvidia rtx2080ti gpu."
3,our method accelerated sar ship detection by many times with only a slight accuracy sacrifice
4,"remote sens. 2019, 11, 2483 4 of 37 compared with the state-of-art object detectors, which is of great application value in real-time maritime disaster rescue and emergency military planning."
4,"to be clear, in this paper, we did not consider the time of sar imaging process (most often, decades of minutes up to hours for a wide region) because sar images to be detected have been acquired previously."
4,"therefore, only the time of sar image interpretation, namely ship detection, was considered."
4,"another point to note is that the adjective “high-speed” is used to describe the detection speed by a detector, instead of the moving speed of ship."
4,the main contributions of our work are as follows: 1.
4,a brand-new light-weight network architecture for the high-speed sar ship detection was established by mainly using ds-cnn; 2.
4,"multi-scale detection mechanism, concatenation mechanism and anchor box mechanism were integrated into our method to improve the detection accuracy; 3."
4,"the ship detection speed of our proposed method is faster than the current other object detectors, with only a slight accuracy sacrifice compared with the state-of-art object detectors."
4,the rest of this paper is organized as follows.
4,section 2 introduces c-cnn and ds-cnn.
4,section 3 introduces our network architecture.
4,section 4 introduces our ship detection model.
4,section 5 introduces our experiments.
4,section 6 presents the results of sar ship detection.
4,"finally, a summary is made in section 7."
4,"cnn in this section, firstly, we will introduce the basic structures of c-cnn and ds-cnn."
4,"afterwards, we will make a comparison of theoretical computational complexity between c-cnn and ds-cnn to show that ds-cnn has lower computational cost."
4,"c-cnn convolution neural network (cnn) was first proposed by lecun et al. in 1998 [36], which has been successfully applied in speech recognition [37], face recognition [38], natural language processing [39], etc."
4,"especially, in the computer vision (cv) field, cnn has largely surpassed the traditional object detection algorithms in the imagenet large-scale visual recognition challenge (ilsvrc) [40]."
4,"in general, two main factors are contributing to the success of cnn."
4,"on the one hand, it can improve the recognition rate for its receptive field [41] similar to human visual cells."
4,"on the other hand, it can also effectively reduce the number of network parameters and can alleviate over-fitting by local connection and weight sharing, compared with a fully connected deep neural network."
4,"therefore, in the dl object detection field, cnn has become an extremely important research direction."
4,"in this paper, we called the above cnn as conventional cnn (c-cnn)."
4,figure 1a is the basic structure of c-cnn.
4,"from figure 1a, each convolution kernel (k1 or k2 or k3 or k4 ) needs to convolute all four input channels separately (i1 , i2 , i3 , i4 ), then add up the above four convolution operation results to obtain the output of one channel (o1 or o2 or o3 or o4 ), and finally these four outputs from four different kernels are connected into a whole (o1 o2 o3 o4 )."
4,"for example, the rough process for the kernel k1 can be expressed as follows: o1 = k1 ⊗ i1 + k1 ⊗ i2 + k1 ⊗ i3 + k1 ⊗ i4 (1) where i1 , i2 , i3 , and i4 are the network inputs, o1 is one of the network outputs, k1 is one of the network kernels, and ⊗ denotes the convolution operation."
4,"thus far, many excellent c-cnns have emerged, such as alexnet [38], vgg [42], googlenet [43], darknet [31], etc., which are used by most of the object detectors to extract features."
4,"however, these c-cnns simultaneously consider both channels and regions [44], leading to a huge number of"
5,"remote sens. 2019, 11, 2483 5 of 37 network remote 2019, 11, x foras sens.parameters. a result, peer review the detection speed will be inevitably decreased."
5,"therefore, aiming 5 of 38 at this problem, ds-cnn emerged. (a) k1 k2 k3 k4 o1 i1 o2 i2 o3 i3 o4 i4 input kernel output (b) k1 o1 i1 k2 o2 i2 o3 k3 i3 o4 i4 k4 input kernel output figure 1.1."
5,convolutional figure convolutionalneural neuralnetwork. (a)(a) network.
5,conventional convolution conventional neural convolution network neural (c-cnn); network (b) (c-cnn); depthwise separable (b) depthwise convolution separable neural convolution network neural (ds-cnn). network (ds-cnn).
5,184 2.2.
5,"ds-cnn for example, the rough process for the kernel k1 can be expressed as follows: 185 ds-cnn was first proposed o1 = kby + k1 in l.i 1sifre i 2 + k[45],  2014  iwhich + k1  hasi 4 been successfully applied(1) to 1 1 3 xception [46] and mobilenet [44]."
5,"especially, ds-cnn has a tremendous application prospect in mobile 186 communication where devices i1, i2, i3, and i4 areandtheembedded networkdevices inputs,for one ofnetworks o1itsislighter the networkand faster speed. outputs, k1 in is the onedloffield, the 187 ordinarily, network and kernels convolution kernels, denotes canthebeconvolution regarded as 3d filters (width, height, and channel) for convolution operation. 188 operations, thus far, andmany excellent convolution conventional c-cnns have emerged,are operations joint such asmappings alexnet [38], of channel correlations vgg [42], googlenet and 189 spatial correlations [46]."
5,"then, it will be beneficial for the reduction [43], darknet [31], etc., which are used by most of the object detectors to extract features."
5,"however,of computational complexity if we 190 decouple these c-cnnschannel correlations and simultaneously spatial consider both correlations channels and for this,[44], [46].regions ds-cnnleadingsuccessfully decouples to a huge number of 191 channel correlations and spatial correlations to reduce computational network parameters."
5,"as a result, the detection speed will be inevitably decreased."
5,"therefore, aiming complexity, meanwhile it does 192 not at make this ds-cnnsacrifice great accuracy problem, emerged. because c-cnn inherently has some redundancy [45]."
5,figure 1b is the basic structure of ds-cnn.
5,"from figure 1b, each convolution kernel (k1 or k2 or 193 k3 or 2.2."
5,"k4 ) needs to convolute only one input channel (i1 or i2 or i3 or i4 ), then obtain the output of one ds-cnn channel (o1 or o2 or o3 or o4 ) without summation process, and finally these four outputs from four 194 ds-cnn was first proposed by l."
5,"sifre in 2014 [45], which has been successfully applied to different kernels are directly connected into a whole (o1 o2 o3 o4 )."
5,195 xception [46] and mobilenet [44].
5,"especially, ds-cnn has a tremendous application prospect in for example, the rough process for the kernel k1 can be expressed as follows: 196 mobile communication devices and embedded devices for its lighter networks and faster speed."
5,"in 197 the dl field, ordinarily, convolution kernels o1 = cank be regarded as 3d filters (width, height, and 1 ⊗ i1 (2) 198 channel) for convolution operations, and conventional convolution operations are joint mappings of 199 channel from correlations equations and (1) andspatial correlations (2), c-cnn [46].convolution has four then, it will be beneficial operations for the reduction and ds-cnn has only one of 200 computational complexity convolution operation, if we decouple therefore, ds-cnn channel correlations simplifies traditionalandconvolution spatial correlations operation,[46].which for this, can 201 ds-cnn successfully decouples channel correlations and obviously reduce the amount of calculation."
5,"to be clear, equations (1) and (2) here are only rough spatial correlations to reduce 202 expressions andcomplexity, computational more rigorous meanwhile expressions it does will not make greatin accuracy be introduced section 2.3. sacrifice because c-cnn 203 figurehas inherently 2 issome redundancy the detailed internal [45]. operation flow of a ds-cnn."
5,"from figure 2, a ds-cnn consists of 204 figure 1b is the basic structure a d-conv2d and a p-conv2d."
5,for d-conv2d of ds-cnn.
5,"from figure in figure 1b, each convolution 2a, it performs kernel (k1 or convolution operations 2 or onkonly 205 3 orchannel, kone to convolute k4) needsrespectively, only and one input generates results (iwithout fourchannel 1 or i2 or i3 or i4), then summing them. obtain then,the foroutput p-conv2dof one in 206 channel (o 1 or o 2 or o 3 or o figure 2b, it uses 1 × 1 kernel to perform conventional convolution operations on all results generated 4 ) without summation process, and finally these four outputs from four 207 before tokernels different ensureare thedirectly same outputconnected shape into whole (o as ac-cnn, 1o2o then 3o4sum the ). is used to generate an output of 208 for example, one channel, and the rough finally process for all outputs fromthethekernel above k1 1can× 1be expressed kernels as follows:into a whole as the are connected final output."
5,"through the above two steps, ds-cnn successfully decouples channel correlation and 209 o1 = k1  i 1 (2) spatial correlation."
5,"210 from equations (1) and (2), c-cnn has four convolution operations and ds-cnn has only one 211 convolution operation, therefore, ds-cnn simplifies traditional convolution operation, which can 212 obviously reduce the amount of calculation."
5,"to be clear, equations (1) and (2) here are only rough 213 expressions and more rigorous expressions will be introduced in section 2.3."
6,"217 215 ofp-conv2d a d-conv2d in figure 2b, it uses 1for and a p-conv2d."
6,"× 1d-conv2d in figureconventional kernel to perform convolution 2a, it performs convolution operations operationsonon all 218 216 results only onegenerated channel, before andthe to ensure respectively, same output generates shape as four results c-cnn, without then thethem. summing sum then, is usedforto 219 217 generate an p-conv2d inoutput figureof one 2b, channel, it uses 1 × 1and finally kernel to all outputs perform from the above conventional 1 × 1 kernels convolution are connected operations on all 220 218 into a generated results whole as the ensurethrough finaltooutput. before the samethe two steps, aboveshape output ds-cnn as c-cnn, thensuccessfully the sum isdecouples used to 221 219 channel an generate output ofand correlation onespatial correlation. channel, and finally all outputs from the above 1 × 1 kernels are connected remote sens. 2019, 11, 2483 6 of 37 220 into a whole as the (a) final output."
6,"through the above (b)1×1two steps, ds-cnn successfully decouples 221 channel correlation and spatialkcorrelation."
6,4 k3 1×1 (a) (b)1×1 k4 k2 1×1 k3 1×1 k1 1×1 k2 1×1 k1 1×1 d-conv2d p-conv2d figure 2.
6,detailed internal operation flow of a ds-cnn. (a) depthwise convolution (d-conv2d); (b) d-conv2d p-conv2d pointwise convolution (p-conv2d).
6,figure 2.
6,figure detailedinternal 2.
6,detailed internaloperation operationflow of of flow a ds-cnn.
6,depthwise (a) (a) a ds-cnn. convolution depthwise (d-conv2d); convolution (b) (d-conv2d); pointwise convolution (p-conv2d). (b) pointwise convolution (p-conv2d).
6,222 2.3.
6,computational complexity of c-cnn and ds-cnn 2.3.
6,"computational complexity of c-cnn and ds-cnn 223 currently, the mainstream target detectors commonly use c-cnns to build networks, such as 222 224 2.3."
6,"computational complexity of c-cnn and ds-cnn faster r-cnn, the currently, retinanet, mainstreamssd,target yolo, etc."
6,"however, detectors in this commonly usepaper, we used c-cnns ds-cnns to build to build networks, suchour as 225 223 network."
6,"faster to currently,verify the lightweight the mainstream r-cnn, retinanet, ssd, target of ds-cnn, yolo,detectors commonly etc."
6,"however,we compared use in this the c-cnns paper, computational to build we used complexity networks, ds-cnns asof suchour to build 226 224 c-cnn faster network. andverify r-cnn, to ds-cnn."
6,"retinanet, figure 3of shows ssd, yolo, the lightweight ds-cnn, thewecomparison etc."
6,"however, in thisthe compared diagrammatic paper, we used sketch computational ds-cnnsof c-cnn to of build complexity and our c-cnn 227 225 ds-cnn. network. and to verify ds-cnn."
6,"figurethe lightweight 3 shows of ds-cnn, the comparison we compared diagrammatic sketchthe of computational complexity of c-cnn and ds-cnn."
6,226 c-cnn and ds-cnn.
6,figure 3 shows the comparison diagrammatic sketch of c-cnn and 227 ds-cnn. (a) cin n conv (a) cinn conv n conv cconv (b) n conv 1 cconv cin n conv 1 (b) 1 n conv d-conv2d cin1 p-conv2d n conv cin cconv 1 n conv d-conv2d 1 p-conv2d figure complexity.
6,figure 3.
6,computational c (a) c-cnn; 3.
6,computational (b) ds-cnn. complexity. (a) c-cnn;c (b) ds-cnn. in conv 228 assume assume figure that thesize thatthe sizeof 3.
6,computational ofimages complexity.
6,"×l, ll×c-cnn; imagesisis(a) l,the thesize sizeds-cnn. (b) of theconvolution ofthe kernelisisnnconv convolutionkernel ×n conv×n ×cin×, c conv conv in , and 229 and the number the number convc is cis . conv ."
6,"230 228 then, then, the thecomputational computational assume that the size of imagescostof cost c-cnn ofisc-cnn in infigure l × l, the size of3a figure 3a theis: is: convolution kernel is nconv×nconv×cin, and 229 the number is cconv."
6,"231 230 then, the computational computation computation c−cnn cost of c-cnn =in=lfigure c −cnn l· l l· nnconv 3a · n convis: nconv  c  cconv conv · cinin · cconv (3) (3) 232 231 ififwe useds-cnn, weuse ds-cnn,the thecomputation computationalcost computational = lofof cost  n  n inin  ld-conv2d d-conv2d  cfigure  c 3b figure 3bis: is: (3) c −cnn conv conv in conv 233 232 computation if we use ds-cnn, thecomputation computational cost of==d-conv2d d−conv2d d-conv2d nnconv  n convin conv · nconv c cinin l· l · figure  l· 3b l is: (4) (4) 234 233 and and the the computational computational cost ofcost of p-conv2d computation p-conv2d in in=figure figure n 3b is: c  l  l is: n 3bconv (4) d-conv2d conv in 235 234 ofcomputation and the computational costcomputation =c = c3b  ccconv  l l (5) p-conv2d in figure p-conv2d p−conv2d in · is: in conv · l · l (5) 235 therefore, the computationalcomputation cost of ds-cnn = c  cconv  l  l p-conv2d is: in (5) computationds−cnn = nconv · nconv · cin · l · l + cin · cconv · l · l (6) then, their ratio is: computationds−cnn 1 1 ratio = = + 2 (7) computationc−cnn cconv nconv"
7,"remote sens. 2019, 11, 2483 7 of 37 where cconv >> 1, and nconv > 1."
7,"finally, ratio << 1 (8) therefore, from formula (8), ds-cnn can effectively reduce computational cost, which can improve the detection speed."
7,"moreover, we also discussed the time complexity of c-cnn and ds-cnn."
7,"the time complexity of c-cnn is:   2 2 timec−cnn ∼ o nout · nconv · cin · cout (9) where nout is the size of the output feature maps, nconv is the size of kernels, cin is the number of the input channels, and cout is the number of the output channels."
7,"the time complexity of ds-cnn is:   2 2 2 timeds−cnn ∼ o nout · nconv · cin + nout · cin · cout (10) from formulas (9) and (10), ds-cnn essentially converts continuous multiplication into continuous addition, so the redundancy of the network gets reduced."
7,"as a result, the computational efficiency of the network has been greatly improved."
7,network architecture figure 4 shows the network architecture of our proposed high-speed sar ship detection system.
7,"our network consists of a backbone network and a detection network, inspired from the experience of mobilenet [44], yolo [32], ssd [33], and densenet [47]."
7,"from figure 4, in our network, the images to be detected are resized into l × l and the number of channels is 3."
7,"in figure 4, in our model, to achieve aremote goodsens. tradeoff between 2019, 11, accuracy x for peer and speed, we set l = 160."
7,detailed research of the value of l will review 8 ofbe 38 introduced in section 5.4.1.
7,l l/8 d-conv2d_5 20×20×8 20×20×16 20×20×18 conv2d_10 conv2d_11 conv2d_12 kernel (3×3×256×1) kernel (1×1×264×8) kernel (3×3×8×16) kernel (1×1×16×18) (l×l×3) 160×160×3 20×20×256 c-cnn 20×20×264 conv2d_1 p-conv2d_5 detection network-3 (l/8 ×l/8 ×18) kernel (3×3×3×32) kernel (1×1×256×256) 20×20×256 (dropout 0.1%) 80×80×32 concatenate_2 ds-cnn 20×20×256 dropout d-conv2d_1 d-conv2d d-conv2d_6 20×20×8 kernel (3×3×32×1) kernel (3×3×256×1) 80×80×32 10×10×256 upsampling2d_2 p-conv2d_1 p-conv2d p-conv2d_6 10×10×8 kernel (1×1×32×64) kernel (1×1×256×512) conv2d_9 80×80×64 10×10×512 kernel (1×1×16×8) d-conv2d_2 d-conv2d_7~11 10×10×16 kernel (3×3×64×1) kernel (3×3×512×1) 10×10×16 10×10×32 10×10×18 conv2d_6 conv2d_7 conv2d_8 40×40×64 ×5 10×10×512 kernel (1×1×528×16) kernel (3×3×16×32) kernel (1×1×32×18) l/16 p-conv2d_2 p-conv2d_7~11 10×10×528 (l/16 ×l/16 ×18) kernel (1×1×64×128) kernel (1×1×512×512) 10×10×512 40×40×128 10×10×512 concatenate_1 d-conv2d_3 d-conv2d_12 10×10×16 kernel (3×3×128×1) kernel (3×3×512×1) detection network-2 40×40×128 5×5×512 upsampling2d_1 (dropout 0.1%) p-conv2d_3 p-conv2d_12 kernel (1×1×128×128) kernel (1×1×512×1024) 5×5×16 40×40×128 conv2d_5 5×5×1024 kernel (1×1×32×16) d-conv2d_4 d-conv2d_13 kernel (3×3×128×1) kernel (3×3×1024×1) 5×5×32 20×20×128 5×5×1024 p-conv2d_4 20×20×256 p-conv2d_13 5×5×1024 conv2d_2 5×5×32 conv2d_3 5×5×64 conv2d_4 5×5×18 kernel (1×1×128×256) kernel (1×1×1024×1024) kernel (1×1×1024×32) kernel (3×3×32×64) kernel (1×1×64×18) l/32 backbone network (dropout 0.1%) detection network-1 (dropout 0.1%) (l/32 ×l/32 ×18) figure network architecture.
7,network figure4.
7,architecture.
7,266 3.1.
7,backbone network 267 a backbone network is used to extract ships’ features and it is established by using ds-cnn 268 (marked in a red rectangular frame in figure 4) and c-cnn (marked in a blue rectangular frame
8,"remote sens. 2019, 11, 2483 8 of 37 in addition, the detection network is composed of three parts (detection network-1, detection network-2 and detection network-3), which means that our network will detect an input sar image under three different scales (l/32, l/16, and l/8), and then obtain the final ship detection results."
8,"next, we will introduce the backbone network and the detection network, respectively."
8,backbone network a backbone network is used to extract ships’ features and it is established by using ds-cnn (marked in a red rectangular frame in figure 4) and c-cnn (marked in a blue rectangular frame figure 4).
8,"here, in order to avoid loss of original image information (make full use of input image information), we intentionally only set the first layer (conv2d_1) as a c-cnn, inspired by the experience of mobilenet [44]."
8,"from figure 4, there are 13 ds-cnns in all in our backbone network, which have enough capability to fully extract ships’ features."
8,"as is shown in figure 4, a d-conv2d and a p-conv2d collectively constitute a basic ds-cnn, which has been introduced in section 2.2."
8,we made all d-conv2ds have the same 3 × 3 kernel size to facilitate the analysis of model parameters.
8,"moreover, certainly, the kernel size of all p-conv2ds must be 1 × 1 according to the fundamental conception of ds-cnn in figure 2 of section 2.2."
8,"figure 5a,b respectively shows the internal implementation of a c-cnn and a ds-cnn in detail."
8,"from figure 5a, the c-cnn only carries out a conv2d operation, then passes through batch normalization (bn) layer, and finally enters into the activation function leaky-relu layer to obtain the output of the c-cnn."
8,"from figure 5b, the ds-cnn carries out d-conv2d operation first, then passes through bn layer, and finally enters into the leaky-relu layer to obtain the output of the d-conv2d."
8,"after that, the output of the d-conv2d layer is inputted into the p-conv2d layer."
8,"p-conv2d carries out a pointwise convolution operation, then passes through batch normalization (bn) layer, and finally sens. the remote into enters 11, x for peer 2019,activation review function 9 of 38 leaky-relu layer to obtain the output of the p-conv2d."
8,the output of the p-conv2d is that of the ds-cnn. (a) c-cnn (b) ds-cnn conv2d kernel (3×3) batch normalization d-conv2d kernel (3×3) batch normalization { d-conv2d leaky-relu leaky-relu p-conv2d kernel (1×1) batch normalization { p-conv2d leaky-relu figure 5.
8,figure implementation. (a) internal implementation.
8,internal (a)c-cnn; (b) ds-cnn.
8,c-cnn;(b) ds-cnn.
8,"here, bn can accelerate deep network training by reducing internal covariate shift [48], which 287 here, bn can accelerate deep network training by reducing internal covariate shift [48], which can normalize the input data x to [0,1], and make the output data conform to the standard normal 288 can normalize the input data x to [0,1], and make the output data conform to the standard normal distribution."
8,"in addition, leaky-relu [49], an improved version of relu, is an activation function. 289 distribution."
8,"in addition, leaky-relu [49], an improved version of relu, is an activation function."
8,"its definition is as follows: 290 its definition is as follows: ( x, x ≥ 0 y= (11) x ,x < 0 αx , x  0  291 y = x (11)   , x  0 292 where α is a constant and α∈(1, + )."
8,"in our model, we set α = 5.5, inspired by the experience of 293 reference [49]."
8,"leaky-relu can reduce the possibility of gradient disappearance because it can 294 reduce the occurrence of dead neurons [49] for its nonzero derivative when x < 0, compared with 295 relu."
9,"remote sens. 2019, 11, 2483 9 of 37 where α is a constant and α ∈ (1, +∞)."
9,"in our model, we set α = 5.5, inspired by the experience of reference [49]."
9,"leaky-relu can reduce the possibility of gradient disappearance because it can reduce the occurrence of dead neurons [49] for its nonzero derivative when x < 0, compared with relu."
9,"finally, the ships’ features extracted by the backbone network are transmitted to the detection network for ship detection."
9,detection network a detection network is used to perform ship detection.
9,"from figure 4, in order to fully absorb the features extracted from the backbone network and improve detection accuracy, the detection network is established by using c-cnns."
9,"(in our experiments, if detection networks all use ds-cnns, the accuracy is far less than 90%, which cannot meet the practical requirements.)"
9,3.2.1.
9,"multi-scale detection mechanism considering the diversity of ship sizes in the ssdd dataset, we set three different detection networks (detection network-1, detection network-2, and detection network-3), inspired by the experience of yolo [32] and ssd [33], to detect ships under three different scales."
9,"detection network-1 is designed for detecting big size ships, detection network-2 is designed for detecting medium size ships, and detection network-3 is designed for detecting small size ships."
9,figure 6 is the diagram of multi-scale detection.
9,"as is shown in figure 6, for example, if the size of the input image is l × l, the size of the output feature maps are l/32 × l/32 in the detection network-1, l/16 × l/16 in the detection network-2, and l/8 × l/8 in the detection network-3."
9,"therefore, our network will generate predictive bounding boxes based on these feature maps."
9,"by using multi-scale mechanism, the detection remote accuracy sens. 2019, 11, x for can peerbe 10 ofbe greatly improved."
9,detailed research of multi-scale detection will review 38 introduced in section 5.4.3.
9,backbone network l detection network-1 detection network-2 l/2 detection network-3 l/4 l/8 l/8 l/16 l/16 l/32 figure 6.
9,multi-scale detection mechanism.
9,figure 6.
9,multi-scale detection mechanism.
9,3.2.2.
9,concatenation mechanism 315 3.2.2.
9,"concatenation mechanism in addition, considering that our method has faster detection speed, but may reduce accuracy."
9,"316 in addition, therefore, considering to improve that our the detection methodwe accuracy, faster hasalso detection adopted speed, but mechanism may reduce(two concatenation accuracy. blue 317 therefore, arrows to improve in figure the detection 4), inspired from theaccuracy, weofalso experience adopted densenet concatenation mechanism (two blue [47]. 318 arrows in figure figure inspired from 4), diagram 7 is the the experience of concatenation of densenetfrom [47]."
9,"figure 7, the two inputs of the mechanism."
9,319 figure 7 is the diagram of concatenation mechanism.
9,from concatenation (marked in a red rectangular frame) are a and b.
9,"we directly figure 7, the two a concatenated and bof inputs as the an 320 concatenation (marked in a red rectangular frame) are a and b."
9,we directly concatenated a and b as 321 an output c without any other operations.
9,"in addition, b is generated by some subsequent 322 convolution operations of a, which means the feature maps of a are shallow features and the 323 feature maps of b are deep features."
9,"therefore, the feature maps of c are the combination of shallow 324 features and deep features."
9,"in this way, our network can achieve feature fusions and improve 325 accuracy."
9,"especially, the sizes of the two input feature maps (a and b) must be the same, while the"
10,315 3.2.2.
10,"concatenation mechanism 316 in addition, considering that our method has faster detection speed, but may reduce accuracy."
10,"317 therefore, to improve the detection accuracy, we also adopted concatenation mechanism (two blue 318 arrows in figure 4), inspired from the experience of densenet [47]."
10,"remote sens. 2019, 11, 2483 10 of 37 319 figure 7 is the diagram of concatenation mechanism."
10,"from figure 7, the two inputs of the 320 concatenation (marked in a red rectangular frame) are a and b."
10,we directly concatenated a and b as 321 output an outputc without any other c without any operations.
10,"in addition, other operations."
10,"b is generated in addition, b is by some subsequent by some convolution generated subsequent 322 convolution operations of a, which means the feature maps of a are shallow features maps operations of a, which means the feature maps of a are shallow features and the feature and theof 323 b are deep feature maps features. of b aretherefore, the feature deep features. maps therefore, theoffeature c are maps of c are theof the combination shallow features combination and of shallow 324 deep features. features and deep way, ourinnetwork in thisfeatures. achieve can our this way, featurecan fusions network improvefusions andfeature achieve accuracy.andespecially, improve 325 the sizes ofespecially, accuracy. the two input feature the sizes maps of the and feature two(ainput b) mustmapsbe the(asame, while mustthe and b) benumber the same, of while channels the 326 need numbernot of equal."
10,theneed be channels number of the not be output thechannels equal. number(c) of is thethe sum ofchannels output the numbers (c) isofthe two inputs the sum of the 327 numbers (a channels b). inputs channels (a and b). andtwo of the concatenation c a b b a shallow features deep features feature fusion: shallow features + deep features .
10,concatenation mechanism.
10,figure 7.
10,"figure 328 in our experiments, we concatenated the outputs from different different depth layers, which can achieve 329 accuracy."
10,"as is shown in figure 4, we concatenated p-conv2d_11 layer feature fusions and improve accuracy. 330 and upsampling2d_1 and upsampling2d_1 for thefor layerlayer detection network-2, network-2, the detection and p-conv2d_5 and layer and upsampling2d_2 p-conv2d_5 layer and 331 layer layernetwork-3. for the detection upsampling2d_2 network-3. layer, for concatenate_1 for the detection the p-conv2d_11 for concatenate_1 layer layer, the (10 × 10 × 512) p-conv2d_11 332 layerthe and × 10 × 512) and thelayer (10 upsampling2d_1 (10 × 10 × layer upsampling2d_1 16) are × 10inputs, (10 its × 16) are theinputs, so its size of output so the size is of the 333 10 × 10 × 10 is output 528× (512 = 528)."
10,+ 16(512 10 × 528 + 16 =for 528).
10,"concatenate_2 for concatenate_2 the p-conv2d_9 layer,layer, the p-conv2d_9 layer × ×2020×× 256) (20(20 layer 334 and the upsampling2d_2 layer (20 ××20 20××8)8)are layer (20 areits itsinputs, inputs,sosothe thesize theoutput sizeofofthe 20 × 20 ×(256 outputisis20×20×264 264 335 (256 + 8 = 264)."
10,"in particular, the upsampling2d_1 layer is to ensure the + 8 = 264)."
10,"in particular, the upsampling2d_1 layer is to ensure the same size between same size between conv2d_5 layer (5 × 5 × 16) and p-conv2d_11 layer (10 × 10 × 512), whose up-sampling multiple is 2."
10,"similarly, the upsampling2d_2 layer is to ensure the same size between conv2d_9 (10 × 10 × 8) and p-conv2d_5 (20 × 20 × 256), whose up-sampling multiple is also 2."
10,detailed research of concatenation mechanism will be introduced in section 5.4.4.
10,"ship detection model in this section, firstly, we will introduce the ship detection process of our method."
10,"then, we will introduce the anchor box mechanism."
10,"finally, we will present the evaluation indexes of sar ship detection."
10,detection process we designed our high-speed sar ship detection system utilizing the basic idea of one-stage detectors [30–32] for their faster detection speed than two-stage detectors.
10,figure 8 shows the detection process of our proposed ds-cnn ship detection system.
10,the detailed ship detection process is as follows.
10,step 1: input sar images to be detected.
10,see figure 8a.
10,step 2: resize images into l × l.
10,see figure 8b.
10,"in order to make images of different sizes have the same feature dimension, we need to resize all the images into l × l by resampling."
10,"here, l is a variable that can be adjusted to make a tradeoff between accuracy and speed in our experiments."
10,"in particular, since our network can be regarded as 32 times down-sampling (input: l; detection network-1: l/32), l must be a multiple of 32."
10,detailed research of l will be introduced in section 5.4.1.
11,344 4.1.
11,detection process 345 we designed our high-speed sar ship detection system utilizing the basic idea of one-stage 346 detectors [30–32] for their faster detection speed than two-stage detectors.
11,"347 figure remote sens. shows 2019,811, 11 ofthe 2483 the detection process of our proposed ds-cnn ship detection system. 37 348 detailed ship detection process is as follows. (a) (b) ship 1 (c) ship 1 ship 2 ship 2 the grid cell responsible for detecting ship 1."
11,the grid cell responsible l×l s×s for detecting ship 2.
11,"original image resize images divide images (d) step 1 step 2 step 3 a b ship 1 p c ship 2 high-speed sar ship detection system step 4 de q f step 7 step 6 step 5 (x,y,w,h,score) locate ships' center (g) (f) (e) ship:0.92 (x,y) w ship 1 h ship 1 b bounding boxes for ship 1. (marked in blue) ship:0.90 ship 2 ship 2 b bounding boxes for ship 2. (marked in blue) coordinate mapping non-maximum suppression generate bounding boxes (detection results) figure8."
11,figure detection detection process process of high-speed of high-speed sar shipship sar detection detection system. system. (a) original (a) original images; images; (b) (b) resize resize (c) images; images; divide(d) divide(c)images; images; (d) locate locate ships’ ships’ center; center; (e) (e) generate generate bounding bounding boxes; boxes; (f) (f) non-maximum non-maximum suppression; suppression; (g) detection (g) detection results. results.
11,349 step3:1:divide step images input sar into sto×be s grids. images seesee detected.
11,figure 8c. 8a.
11,figure 350 we divided images into s × s grids inspired step 2: resize images into l × l.
11,see figure 8b. by the basic idea of yolo [30].
11,"here, s has three 351 different valuestofor in order threeimages make different of detection introduced scaleshave different sizes the same figure 6dimension, in feature of section 3.2.1: we need to resize 352 all the images into l × l by resampling."
11,"here, l is a variable that can be adjusted to make a tradeoff 353 s ∈ {l/32, l/16, between accuracy and speed in our experiments."
11,"l/8} in particular, since our network can be regarded (12)as 354 32 times down-sampling (input: l; detection network-1: l/32), l must be a multiple of 32."
11,"detailed for example, if l = 160, then s = 5, 10, 20."
11,"then, the images will be divided into 5 × 5 grids 355 research of l will be introduced in section 5.4.1. in the detection network-1, 10 × 10 grids in the detection network-2, and 20 × 20 grids in the 356 step 3: divide images into s × s grids."
11,see figure 8c. detection network-3.
11,357 we divided images into s × s grids inspired by the basic idea of yolo [30].
11,"here, s has three step 4: locate ships’ center."
11,"see figure 8d. 358 different values for three different detection scales introduced in figure 6 of section 3.2.1: when performing ship detection, our network can automatically locate the ships’ center."
11,"this ability 359 can be obtained by learning from the ground s  truth  l / 32,(real ships) l / 8in the training set."
11,"if the center of a ship l / 16,  falls into a grid cell, that grid cell is responsible for detecting this ship."
11,"for example, in figure 8d, cell p (12) is responsible for detecting ship 1, and cell q is responsible for detecting ship 2."
11,"besides, in order to calculate the loss function in section 5.2, we also defined the probability that cell i contains ships as follows: ( 1, cell i contains ships pcelli (ship) = (13) 0, cell i does not contain ships equation (13) means that as long as cell i contains a ship or a part of a ship, the probability that cell i contains a ship is 1, otherwise 0."
11,"for example, in figure 8d, cell a, b, c, d, e, f, p, and q all contain ships, so: pcella (ship) = 1, pcellb (ship) = 1, pcellc (ship) = 1, pcelld (ship) = 1 pcelle (ship) = 1, pcellf (ship) = 1, pcellp (ship) = 1, pcellq (ship) = 1 for other cells, the probability is zero."
12,"remote sens. 2019, 11, 2483 12 of 37 step 5: generate b bounding boxes."
12,see figure 8e.
12,each grid cell generates b bounding boxes and scores for these boxes.
12,"here, b is a variable that can be adjusted to make a tradeoff between accuracy and speed in experiments."
12,"besides, these b bounding boxes in figure 8e may have different sizes coming from the use of different features."
12,"for example, in figure 8e, if b = 3, grid cell p will generate 3 bounding boxes for ship 1, and grid cell q will generate 3 bounding boxes for ship 2 (marked in blue in figure 8e)."
12,"additionally, each bounding box contains five predictive parameters (x, y, w, h, score), where (x, y) is the coordinate of the top left vertex, w is the width, and h is the height (marked in figure 8f)."
12,"the score of each bounding box is defined by [30]: remote sens. 2019, 11, x for peer review score = pcelli (ship) · iou (14) 13 of 38 402 where drawbackiou isthat intersection theeach grid cellover union. detects only one ship, which may lead to more missed detection cases."
12,"403 next,iou defined weiswill by:example to illustrate. take an area(bp ∩ bg ) 404 for example, in figure 9a, aniou (bp , bgsar original ) = image contains densely distributed small ships. (15) area(bp ∪ bg ) 405 additionally, from figure 9b, cell p simultaneously contains ship a and ship b."
12,"however, from 406 figureb8, where theship p isour prediction and bg is box system detection the ground finally generatestruthonly box.one prediction box (the one with the 407 detailed maximum research score) for aof thecell. grid value of b will one therefore, be introduced of the ships in is section bound5.4.5. to be missed."
12,"in order to solve 408 step 6: non-maximum suppression this problem, the anchor box mechanism was proposed.("
12,nms) [50].
12,see figure 8f.
12,"409 in step 9b figure 5, there are b bounding is a diagram boxes for of the anchor boxeach ship."
12,"here, mechanism."
12,"we weset retained 3 anchortheboxes one with the maximum for each detection 410 score from these boxes, and the others b-1 bounding boxes from step 5 are suppressed. scale (detection network-1, detection network-2, and detection network-3)."
12,"for example, for one of 411 threestep 7: coordinate scales, in figure 9b, mapping (detection our system results). thesee will assign figure 8g. detection task of ship a to the anchor box n, 412 and from step 2, our the detection taskship detection of ship model b to the anchordetects box m."
12,"forinthe ships l ×anchor l images, box and l, itthese l ×deleted will be l images are finally 413 obtained by resampling the original images."
12,"however, the final prediction box should according to iou(bp, bg)."
12,"each detection scale has 3 anchor boxes, so our system has 9 anchor boxes in be drawn in the 414 all for 3 images. original detection therefore, scales, sowe to map needdetect it can to 9coordinates upthe ships in the ofsame the prediction grid cell. box obtained in this from way, the step 6 detection 415 to the original accuracy of small shipshere, images. withour system dense finally completed distribution the task gets improved. of ship9detection."
12,"besides, anchor boxes is sufficient 416 because in the ssdd dataset, each grid cell contains less than 9 ships."
12,"however, for other dataset, the 4.2."
12,anchor box mechanism 417 maximum number of anchor boxes may be different.
12,"detailed research of the number of anchor 418 boxes introduced will bebox anchor mechanism in section 5.4.5. was first proposed by ren et al. in faster r-cnn [28], which has been 419 in many used in our experiments, automatically we such object detectors, as yolo, obtained theour ssd, etc."
12,"in size of the model, anchor it is box used to by k-means address cluster the drawback 420 each as analysis that is shown grid in figure cell detects 9c."
12,"the only one ship,cluster may lead were whichcentroids significantly to more different missed detection thannext, cases. hand-picked we will 421 anchor take boxes."
12,"there an example were fewer short, wide boxes and taller, thin boxes [31]."
12,"additionally, if we use to illustrate. 422 standard k-means with for example, euclidean in figure 9a, an originallarger distance, imagegenerate sar boxes containsmore errordistributed than smaller densely boxesships. small [31]."
12,"423 however, what we additionally, from figure 9b, cell p simultaneously contains ship a and ship b."
12,"however, from figurethe really want are priors that lead to good iou scores, which are independent of 8, 424 sizeship our of the box."
12,"thus, detection systemfor our distance finally metric, generates we only useprediction [31]: one box (the one with the maximum score) for a grid cell."
12,"therefore, one of the ships is bound to be missed."
12,"in order to solve this problem, 425 d ( anchor box, cluster centroid ) = 1 − iou ( anchor box, cluster centroid ) (16) the anchor box mechanism was proposed. (a) (b) (c) p p anchor box l ship b anchor box m ship a anchor box n figure9.9."
12,anchor figure anchor box. (a) an box. (a) an original sar image image with original sar densely distributed with densely distributed small ships (ground small ships (ground truth truth isismarked marked in green.); (b) diagram of anchor box mechanism. (c) cluster results of ssdd.(
12,the in green.); (b) diagram of anchor box mechanism. (c) cluster results of ssdd.
12,(thexxaxis axisisis the thewidth widthof ofground andyyaxis groundtruth truthand theheight axisisisthe ofground heightof groundtruth.). truth.).
12,"426 table 1. shows the size of each anchor box for three scales (detection network-1, detection network-2, 427 and detection network-3)."
12,more detailed about anchor box can be found in reference [31].
13,"remote sens. 2019, 11, 2483 13 of 37 figure 9b is a diagram of the anchor box mechanism."
13,"we set 3 anchor boxes for each detection scale (detection network-1, detection network-2, and detection network-3)."
13,"for example, for one of three scales, in figure 9b, our system will assign the detection task of ship a to the anchor box n, and the detection task of ship b to the anchor box m."
13,"for the anchor box l, it will be deleted finally according to iou(bp , bg )."
13,"each detection scale has 3 anchor boxes, so our system has 9 anchor boxes in all for 3 detection scales, so it can detect up to 9 ships in the same grid cell."
13,"in this way, the detection accuracy of small ships with dense distribution gets improved."
13,"besides, 9 anchor boxes is sufficient because in the ssdd dataset, each grid cell contains less than 9 ships."
13,"however, for other dataset, the maximum number of anchor boxes may be different."
13,detailed research of the number of anchor boxes will be introduced in section 5.4.5.
13,"in our experiments, we automatically obtained the size of the anchor box by k-means cluster analysis as is shown in figure 9c."
13,the cluster centroids were significantly different than hand-picked anchor boxes.
13,"there were fewer short, wide boxes and taller, thin boxes [31]."
13,"additionally, if we use standard k-means with euclidean distance, larger boxes generate more error than smaller boxes [31]."
13,"however, what we really want are priors that lead to good iou scores, which are independent of the size of the box."
13,"thus, for our distance metric, we use [31]: d(anchor box, cluster centroid) = 1 − iou (anchor box, cluster centroid) (16) table 1 shows the size of each anchor box for three scales (detection network-1, detection network-2, and detection network-3)."
13,more detailed about anchor box can be found in reference [31].
13,table 1.
13,size of anchor box in different detection networks.
13,"name scale anchor boxes (width, height) detection network-1 l/32 (9,12), (12,25), (17,12) detection network-2 l/16 (21,45), (27,17), (36,64) detection network-3 l/8 (50,25), (59,115), (105,45) 4.3."
13,evaluation index 4.3.1.
13,detection accuracy precision is defined by: tp precision = (17) tp + fp where tp is the number of the true positive and fp is the number of the false positive.
13,"in addition, tp can be understood that real ships are correctly detected, and fp can be understood as missed detection."
13,recall is defined by: tp recall = (18) tp + fn where fn is the number of the false negative.
13,"in addition, fn can be understood as false alarm."
13,"mean average precision (map) is defined by: z 1 map = p(r)dr (19) 0 where p is precision, r is recall, and p(r) is precision-recall (p-r) curve."
13,"apparently, map is a comprehensive evaluation index, which combines precision and recall."
13,"therefore, in our work, we used map to represent detection accuracy."
14,"remote sens. 2019, 11, 2483 14 of 37 4.3.2."
14,detection speed the ship detection time of each sar image is expressed as: time (ms) (20) frames per second (fps) is defined by: 1s fps = (21) time fps means that object detectors can detect the number of images in one second (mathematical reciprocal of time).
14,"therefore, in our work, we used fps to represent detection speed."
14,"experiments in this section, firstly, we will introduce the ssdd dataset used in this paper."
14,"then, we will introduce the loss function."
14,"afterwards, we will introduce our training strategies."
14,"finally, we will introduce the establishment process of the sar ship detection model through five types of researches."
14,"our experimental hardware platform is a personal computer (pc) with the hardware configuration of intel(r) i9-9900k cpu @3.60ghz, nvidia rtx2080ti gpu, and 32g memory."
14,"our experiments are performed on pycharm [51] software platform, with python 3.5 language."
14,"our programs are written based on the keras framework [52], a python-based deep learning library with tensorflow [53] as backend."
14,"in addition, we call gpu through cuda 10.0 and cudnn 7.6 for training acceleration."
14,"in our experiments, to achieve better detection accuracy, we set iou = 0.5 and score = 0.5 as detection thresholds, which means that if the probability of a bounding box containing a ship is greater than or equal to 50%, it is retained."
14,"in fact, these two detection thresholds can be dynamically adjusted as a good tradeoff between false alarm and missed detection according to the actual detection results."
14,"dataset in the dl field, a dataset that has been correctly labeled is a momentous prerequisite for research."
14,"therefore, we chose an open ssdd dataset [35] to verify the correctness and feasibility of our proposed method."
14,table 2 shows the detailed descriptions of ssdd.
14,"from table 2, there are 1160 sar images in ssdd dataset coming from three different sensors and there are 2358 ships in these images, with 2.03 ships in one image on average."
14,"these 1160 sar images are labeled by li et al. [35] by using labelimg [54], an image annotation software."
14,"sar images in this dataset possess different satellite sensors, various polarization modes, multiple resolutions, different scenes, and abundant ship sizes, so it can verify the robustness of methods."
14,"therefore, many scholars [10,21,35,55–63] conducted research based on it for a better comparison."
14,table 2.
14,detailed descriptions of ssdd.
14,h: horizontal; v: vertical.
14,"sensors sentinel-1, radarsat-2, terrasar-x place visakhapatnam, india; yantai, china polarization hh, vv, hv, vh average size (pixel × pixel) 500 × 500 resolution 1 m–10 m scene inshore, offshore number of images 1160 number of ships 2358"
15,"remote sens. 2019, 11, 2483 15 of 37 5.2."
15,"loss function the task of ship detection is to provide five parameters (x, y, w, h, score) of a prediction box."
15,"therefore, our loss function is mainly composed of the errors of these five parameters."
15,"loss function of the predictive coordinates (x, y) is defined by: s2  b x x  2  2  loss(x,y) = pcell j (ship) · xi − x̂i,j + yi − ŷi, j (22) i=0 j=0 where (xi , yi ) is the coordinate of the i-th ship’s ground truth box, (x̂i,j , ŷi,j ) is the coordinate of the i-th ship’s prediction box of the j-th grid cell."
15,"loss function of the predictive width and height (w, h) is defined by: s2 b x "" 2 q 2 # √ x q p loss(w,h) = pcell j (ship) · wi − ŵi,j + hi − ĥi,j (23) i=0 j=0 where (wi , hi ) is the width and height of the i-th ship’s ground truth box, (ŵi,j , ĥi,j ) is the width and height of the i-th ship’s prediction box of the j-th grid cell."
15,"loss function of the predictive confidence (score) is defined by: b s 2 b x s h 2 1 xx h i x i loss(score) = · pcell j (ship) · ŝi,j − iou (bp , bg ) + 1 − pcell j (ship) · ŝi,j 2 (24) γ i=0 j=0 i=0 j=0 where ŝi,j is the score of the i-th ship’s prediction box of the j-th grid cell, and γ is a weight coefficient."
15,"therefore, the total loss function is: loss = α · loss(x,y) +β · loss(w,h) +γ · loss(score) (25) where α, β and γ are the weight coefficients, which indicate the weight of various types of loss in the total loss."
15,"in particular, here, γ is the same as that in equation (24), inspired from the experience of reference [30]."
15,"in our ship detection model, we set α = β = 5 and γ = 0.5, inspired by the experience of yolo [30]."
15,"by using equation (25), our model will have a good convergence in training process, which is a critical premise for our work."
15,the good convergence in training process will be presented in figure 10 of section 5.3.
15,"training strategies we randomly divided the dataset into a training set, a validation set and a test set according to the ratio of 7:2:1."
15,"the training set is used to train the model, the verification set is used to adjust the model to avoid over-fitting, and the test set is used to evaluate the performance of the model."
15,we used adaptive moment estimation (adam) algorithm [64] to realize the iteration of network parameters.
15,"we trained the network for 100 epochs with batch size = 8, which means that every eight samples complete a parameter update."
15,"for the first 50 epochs, the learning rate was set to 0.001; for the last 50 epochs, the learning rate was changed to 0.0001 in order to further reduce the loss."
15,"in the last 50 epochs, if the loss of verification set does not decrease for three consecutive times, the learning rate will automatically decrease."
15,"for the deeper layers of our backbone network, we adopted dropout mechanism [65] to accelerate the learning speed of the network and avoid over-fitting, by which each neuron loses its activity at a 0.1% probability."
15,"in addition, in order to further avoid over-fitting of the whole network, we also"
16,527 detection model was retained and others were deleted.
16,528 figure 10 shows the loss curves of the training set and the validation set.
16,"from figure 10, for 529 one thing, the loss can be reduced rapidly, which shows that the loss function we set in section 5.2 is 530 effective (only need 10 epochs from 3500 loss to 45 loss, a good convergence)."
16,"for another thing, the 531 remote gap sens. 2019, between the11,loss 2483 of the validation set and the training set is very narrow, which indicates16that of 37 532 there is not an over-fitting phenomenon in our network, so our training strategies are effective and 533 feasible."
16,"in particular, in figure 10, this network trained only 77 epochs due to the early stopping adopted early stopping mechanism [66], by which if the loss of the verification set does not decrease 534 mechanism."
16,"in fact, in our experiments, almost every training was forced to stop (<100 epochs), for 10 consecutive times, the network is forced to stop training. 535 which reflected the powerful and effective role of early stopping mechanism."
16,4000 validation set training set 3000 60 loss 50 40 2000 30 20 1000 10 0 10 20 30 40 50 60 70 80 0 0 10 20 30 40 50 60 70 80 epoch figure10.
16,figure losscurves 10.
16,loss curvesof ofthe thetraining trainingset andthe setand validationset. thevalidation set.
16,"especially, in order to reduce the number of iterations and avoid falling into local optimum, we first pre-trained our network on imagenet dataset [40]."
16,"in the dl field, imagenet pre-training [67] is a normal practice, which has adopted by many other object detectors."
16,"certainly, we can also start training from scratch [68], but it reduces the accuracy by 4% in our experiments."
16,detailed research of pre-training will be introduced in section 5.4.2.
16,"we also used tensorboard [53], a visualization tool of tensorflow, to facilitate the monitor of the training process."
16,"in our experiments, we saved the model only when the loss of the verification set of the current epoch is better than that of the previous epoch."
16,"finally, the optimal sar ship detection model was retained and others were deleted."
16,figure 10 shows the loss curves of the training set and the validation set.
16,"from figure 10, for one thing, the loss can be reduced rapidly, which shows that the loss function we set in section 5.2 is effective (only need 10 epochs from 3500 loss to 45 loss, a good convergence)."
16,"for another thing, the gap between the loss of the validation set and the training set is very narrow, which indicates that there is not an over-fitting phenomenon in our network, so our training strategies are effective and feasible."
16,"in particular, in figure 10, this network trained only 77 epochs due to the early stopping mechanism."
16,"in fact, in our experiments, almost every training was forced to stop (<100 epochs), which reflected the powerful and effective role of early stopping mechanism."
16,"establishment of model next, we will establish the most suitable high-speed sar ship detection model through the following five types of researches."
16,5.4.1.
16,"research on image size from figure 8, the images to be detected are resized into l × l by resampling."
16,"then, we need to determine the final exact value of l to obtain a better detection performance."
16,"therefore, we studied the influence of different values of l on detection performance."
16,"in particular, since our network can be regarded as 32 times down-sampling (input: l; detection network-1: l/32), l must be a multiple of 32."
16,"thus, we set: l = 32k, k = 1, 2, · · · , 10 (26) to conduct comparative experiments."
16,"in addition, these ten groups of experiments were conducted under the following same conditions:"
17,544 of 32.
17,"545 thus, we set: 546 l = 32k, k = 1,2, ,10 (26) 547 to conduct remote comparative sens. 2019, 11, 2483 experiments. 17 of 37 548 in addition, these ten groups of experiments were conducted under the following same 549 conditions: (a) pretraining; 550 (b) a)two pretraining; concatenations; 551 (c) b)threetwo concatenations; detection scales; 552 c) three detection (d) nine anchor boxes. scales; 553 d) nine anchor boxes."
17,table 3 shows the evaluation indexes of research on image size.
17,figure 11a shows the 554 table 3 shows precision-recall the evaluation (p-r) curves and figureindexes of bar 11b is the research onmap graph of image size.
17,figure (accuracy) 11a(speed). and fps shows the 555 precision-recall (p-r) curves and figure 11b is the bar graph of map (accuracy) and fps (speed).
17,table 3.
17,evaluation indexes of research on image size.
17,556 table 3.
17,evaluation indexes of research on image size.
17,l precision recall map time (ms) fps l precision recall map time (ms) fps 32 75.63% 49.45% 45.54% 8.06 124 32 75.63% 49.45% 45.54% 8.06 124 64 83.87% 85.71% 83.35% 8.55 117 96 83.87% 90.66% 64 80.49% 85.71% 83.35% 88.29% 8.55 8.70 117 115 128 80.49% 93.41% 96 87.63% 90.66% 88.29% 91.36% 8.708.90 115 112 160 87.63% 96.15% 12887.94% 93.41% 91.36% 94.13% 8.909.03 112 111 192 87.94% 95.05% 16088.72% 96.15% 94.13% 93.12% 9.039.37 111 107 224 88.72% 94.51% 19288.21% 95.05% 93.12% 92.55% 9.379.85 107 102 256 88.21% 95.05% 22488.72% 92.06% 94.51% 92.55% 9.8510.37 102 96 288 88.72% 94.51% 25685.57% 91.96% 95.05% 92.06% 10.3711.00 96 91 320 288 87.88% 85.57% 95.60% 92.54% 94.51% 91.96% 11.0011.76 91 85 320 87.88% 95.60% 92.54% 11.76 85 1.0 100% 150 map 91.36% 94.13% 93.12%92.55% 92.06% 91.96% 92.54% fps 88.29% 124 83.35% 125 0.8 80% 117 115 112 111 107 102 96 100 l = 32 91 precision fps 0.6 60% 85 l = 64 map l = 96 45.54% 75 l = 128 0.4 40% l = 160 50 l = 192 l = 224 0.2 20% l = 256 25 l = 288 l = 320 0.0 0% 0 0.0 0.2 0.4 32 64 96 128 160 192 224 256 288 320 recall 0.6 0.8 1.0 l (a) (b) figure 11.
17,research on image size. (a) p-r curves; (b) bar graph of map (accuracy) and fps (speed).
17,"from table 3 and figure 11, we can draw the following conclusions: (1) the detection accuracy becomes higher with the increase of l when l < 160 (marked in green in figure 11b)."
17,the detection accuracy is the highest when l = 160.
17,"the detection accuracy does not increase any more, but fluctuates slightly when l > 160; (2) the detection speed becomes lower with the increase of l (marked in pink in figure 11b)."
17,"this phenomenon is in line with common sense because large-size images have more pixels, leading to more computation."
17,"finally, we chose l = 160 as a tradeoff between accuracy and speed in our final model."
17,"for one thing, it has relatively high accuracy (94.13% map)."
17,"for another thing, its detection speed does not decrease too much, compared with l = 32 (from 124 fps to 111 fps)."
17,5.4.2.
17,"research on pretraining in the dl field, in recent years, a common practice is to pre-train the model on some large-scale datasets (such as imagenet dataset) and then fine-tune the model on other target tasks with less"
18,"565 finally, we chose l = 160 as a tradeoff between accuracy and speed in our final model."
18,"for one 566 thing, it has relatively high accuracy (94.13% map)."
18,"for another thing, its detection speed does not 567 decrease too much, compared with l = 32 (from 124 fps to 111 fps)."
18,568 research 5.4.2.
18,"remote pretraining on2483 2019, 11, 18 of 37 569 in the dl field, in recent years, a common practice is to pre-train the model on some large-scale 570 datasets data training (such[67]."
18,"compareddataset) as imagenet and with 1.26 then fine-tune million images inthe model on imagenet, theother numbertarget tasksimages of sar with less in 571 training ssdd data [67]. dataset compared is very with1160 small (only million images 1.26images). in imagenet, therefore, we studiedthethe number sar imageson effect of pretraining in 572 ssdd dataset is very small (only imagenet on detection performance. 1160 images)."
18,"therefore, we studied the effect of pretraining on 573 imagenet on detection we arranged performance. two groups of experiments, one for pretraining on imagenet and the other for 574 we arranged non-pretraining ontwo groups of imagenet, andexperiments, one for of these two groups pretraining experimentson imagenet and theunder other the were conducted for 575 non-pretraining on imagenet, following same conditions: and these two groups of experiments were conducted under the 576 following same conditions: (a) l = 160; 577 (b) a) = 160; two lconcatenations; 578 (c) b)threetwo concatenations; detection scales; 579 c) three detection (d) nine anchor boxes. scales; 580 d) nine anchor boxes."
18,table 4 shows the evaluation indexes of research on pretraining.
18,figure 12a shows the 581 table 4 shows precision-recall and figureindexes the evaluation (p-r) curves of bar 12b is the research graph on pretraining. of map (accuracy)figure 12a(speed). and fps shows the 582 precision-recall (p-r) curves and figure 12b is the bar graph of map (accuracy) and fps (speed).
18,table 4.
18,evaluation indexes of research on pretraining.
18,583 table 4.
18,evaluation indexes of research on pretraining.
18,pretraining?
18,precision recall map time (ms) fps pretraining?
18,precision recall map time (ms) fps × 84.58% 93.41% 90.14% 9.13 110 √ × 84.58% 93.41% 90.14% 9.13 110 87.94% 96.15% 94.13% 9.03 111 √ 87.94% 96.15% 94.13% 9.03 111 584 1.0 100% 150 94.13% 90.14% map fps 0.8 80% 110 111 100 precision 0.6 60% map fps 0.4 40% 50 0.2 20% pretraining non-pretraining 0.0 0% 0 0.0 0.2 0.4 recall 0.6 0.8 1.0 non-pretraining pretraining (a) (b) figure research on 12.
18,research figure 12. on pretraining. (a) p-r pretraining. (a) curves; (b) p-r curves; (b) bar graph of bar graph of map (accuracy) and map (accuracy) fps (speed). and fps (speed).
18,"585 from table from table44and figure12, andfigure wecan 12,we candraw drawthe followingconclusions: thefollowing conclusions: (1) the detection accuracy of pretraining on imagenet is superior to that of non-pretraining."
18,"this phenomenon shows that pretraining is an effective way to improve the detection accuracy; (2) the detection speed of pretraining and non-pretraining is similar (110 fps and 111 fps) because the pre-training is only carried out in the training process, it does not affect the subsequent actual detection."
18,"although pretraining on imagenet is time-consuming before establishing the model, it can improve the accuracy considerably."
18,"therefore, finally, we still chose pretraining on imagenet in our final model."
18,5.4.3.
18,"research on multi-scale detection mechanism in our network, we set three different detection networks (detection network-1, detection network-2, and detection network-3) to detect ships with different sizes (l/32, l/16, and l/8), as is shown in figure 6."
19,"592 although pretraining on imagenet is time-consuming before establishing the model, it can 593 improve the accuracy considerably."
19,"therefore, finally, we still chose pretraining on imagenet in our 594 final model."
19,"remote sens. 2019, 11, 2483 19 of 37 595 5.4.3."
19,"research on multi-scale detection mechanism 596 in our network, we set three different detection networks (detection network-1, detection 597 network-2,we therefore, andmade a further detection research to network-3) ondetect ships detection multi-scale to confirm with different sizes that l/16,indeed it can (l/32, improve and l/8), as is 598 the detection accuracy. shown in figure 6."
19,"therefore, we made a further research on multi-scale detection to confirm that it 599 we arranged can indeed seven improve comparative experiments under the following same conditions: groups of accuracy. the detection 600 we arranged seven groups of comparative experiments under the following same conditions: (a) l = 160; 601 pretraining; (b) a) l = 160; 602 two pretraining; (c) b) concatenations; 603 ninetwo (d) c) concatenations; anchor boxes. 604 d) nine anchor boxes."
19,table 5 shows the evaluation indexes of research on multi-scale detection mechanism.
19,figure 13a 605 showstable shows the evaluation the 5precision-recall indexes (p-r) curves and research of figure onismulti-scale 13b detection the bar graph mechanism. of map figure (accuracy) and 606 (speed).the precision-recall (p-r) curves and figure 13b is the bar graph of map (accuracy) and 13a shows fps 607 fps (speed).
19,table 5.
19,evaluation indexes of research on multi-scale detection mechanism.
19,608 table 5.
19,evaluation indexes of research on multi-scale detection mechanism.
19,"l/32 l/16 l/8 precision recall map time (ms) fps √ l/32 l/16 l/8 76.32% recall precision 47.80% map time (ms) fps 45.11% 7.15 140 √√ 47.80% 45.11% 76.32% 50.50% 76.67% 48.21% 7.15 7.56 140 132 √ √ 50.50% 48.21% 76.67% 46.15% 73.68% 43.39% 7.56 7.96 132 126 √ √ 73.68% 70.33% √ 85.47% 46.15% 43.39%67.21% 7.96 8.56 126 117 √ √ √ √ 79.59% 85.47% 85.71% 70.33% 82.75% 8.56 8.87 117 67.21% 113 √ √ √ √ 84.47% 79.59% 74.73% 85.71% 71.23% 8.87 8.91 113 82.75% 112 √ √ √ √ √ 87.94% 84.47% 96.15% 74.73% 71.23%94.13% 8.91 9.03 112 111 √ √ √ 87.94% 96.15% 94.13% 9.03 111 1.0 100% 150 140 map 94.13% 132 fps 126 82.75% 0.8 80% 117 113 112 111 71.23% 67.21% 100 60% precision 0.6 map fps 48.21% 45.11% 43.39% l/32 40% 0.4 l/16 50 l/8 l/32, l/16 20% 0.2 l/32, l/8 l/16, l/8 l/32, l/16, l/8 0% 0 l/3 2 l/1 6 l/8 /16 /8 /8 /8 0.0 2, l 2, l 6, l 6, l l/3 l/3 l/1 , l/1 0.0 0.2 0.4 recall 0.6 0.8 1.0 2 l/3 (a) (b) 13."
19,research figure 13. (a) p-r research on multi-scale detection mechanism. (a) curves; (b) p-r curves; bar graph of map (b) bar fps (speed). and fps (accuracy) and (speed).
19,"610 table 55 and from table and figure 13,we figure 13, candraw wecan drawthe thefollowing followingconclusions: conclusions: (1) the detection accuracy shows a upward trend with the increase of the number of detection scales."
19,"if using only one scale, the detection accuracy is the lowest."
19,"if using two scales, the detection accuracy is better than that of one scale."
19,"however, it is still too poor to meet the actual needs (map < 90%)."
19,"if using all three scales, the detection accuracy is the highest (map = 94.13%)."
19,this phenomenon shows that multi-scale detection mechanism can indeed improve detection accuracy; (2) the detection speed shows a downward trend with the increase of the number of detection scales.
19,this phenomenon is in line with common sense because the size of the network has increased.
19,"in addition, in the ssdd dataset, the smallest size of ship is 4 × 7 and the biggest size of ship is 211 × 298."
19,ships from the smallest to the biggest can all be detected successfully by these three
20,616 indeed improve detection accuracy; 617 2) the detection speed shows a downward trend with the increase of the number of detection 618 scales.
20,this phenomenon is in line with common sense because the size of the network has 619 increased.
20,"remote sens. 2019, 11, 2483 20 of 37 620 in addition, in the ssdd dataset, the smallest size of ship is 4×7 and the biggest size of ship is 621 211×298."
20,ships from the smallest to the biggest can all be detected successfully by these three 622 different detection different scales.
20,"in detection scales. otherwords, inother words,three detectionscales differentdetection threedifferent scalesare sufficient. aresufficient."
20,"certainly, certainly, if 623 if more detection scales are used, the detection accuracy may be higher while the detection more detection scales are used, the detection accuracy may be higher while the detection speed is speed is 624 bound to bound to decline. decline."
20,"625 therefore, finally, therefore, finally, we chose three we chose three detection scales in in our detection scales final model, our final model, which which can achieve the can achieve best the best 626 detection accuracy. detection accuracy."
20,5.4.4.
20,research on concatenation mechanism 627 5.4.4.
20,"research on concatenation mechanism in our detection network, we set two concatenations to achieve feature fusion, as is shown in 628 in our detection network, we set two concatenations to achieve feature fusion, as is shown in figure 4."
20,"therefore, we studied the effect of concatenation and non-concatenation."
20,629 figure 4.
20,"therefore, we studied the effect of concatenation and non-concatenation."
20,we arranged two groups of comparative experiments under the following same conditions: 630 we arranged two groups of comparative experiments under the following same conditions: (a) l = 160; 631 a) l = 160; (b) pretraining; 632 b) pretraining; (c) three detection scales; 633 c) three detection scales; 634 (d) d) anchor ninenine boxes. anchor boxes.
20,635 table 66shows table showsthe evaluation the indexes evaluation of research indexes on concatenation of research mechanism. on concatenation figurefigure 14a shows mechanism. 14a 636 shows the precision-recall (p-r) curves and figure 14b is the bar graph of map (accuracy) (speed). the precision-recall (p-r) curves and figure 14b is the bar graph of map (accuracy) and fps and fps 637 (speed).
20,table 6.
20,evaluation indexes of concatenation mechanism.
20,638 6.
20,evaluation indexes table precision concatenation?
20,recall of concatenation map mechanism.
20,time (ms) fps × 89.18% 95.05% 92.43% 8.93 fps 112 √concatenation?
20,precision recall map time (ms) × 87.94% 96.15% 89.18% 94.13% 95.05% 92.43% 9.03 8.93 112 111 √ 87.94% 96.15% 94.13% 9.03 111 1.0 100% 150 92.43% 94.13% map fps 0.8 80% 112 111 100 precision 0.6 60% map fps 0.4 40% 50 0.2 20% concatenation non-concatenation 0.0 0% 0 0.0 0.2 0.4 recall 0.6 0.8 1.0 non-concatanation concatanation (a) (b) figure 14.
20,figure 14.
20,research onconcatenation researchon mechanism.(a)(a) concatenationmechanism.
20,p-rp-r curves; curves; (b)(b) barbar graph graph of map of map (accuracy) (accuracy) and and fps (speed).
20,fps (speed).
20,"640 table 66 and from table and figure 14, we figure 14, we can drawthe thefollowing can draw conclusions: followingconclusions: (1) the detection accuracy of concatenation is superior to that of non-concatenation (94.13% map > 92.43% map)."
20,"this phenomenon shows that concatenation mechanism can indeed improve detection accuracy because the shallow features and deep features have been fully integrated; (2) the detection speed of concatenation and non-concatenation is almost equal (112 fps and 111 fps), because only two concatenations does not increase network parameters too much."
20,"finally, in order to obtain better detection accuracy, we adopted the concatenation mechanism in our final model."
21,"645 2) the detection speed of concatenation and non-concatenation is almost equal (112 fps and 646 111 fps), because only two concatenations does not increase network parameters too 647 much."
21,"648 finally, in order to obtain better detection accuracy, we adopted the concatenation mechanism 649 in oursens."
21,"remote final2019, 11, 2483 model. 21 of 37 650 5.4.5."
21,research on anchor box mechanism 5.4.5.
21,"research on anchor box mechanism 651 finally, we also studied the influence of the number of anchor boxes on the detection finally, we also studied the influence of the number of anchor boxes on the detection performance. 652 performance."
21,"therefore, we arranged three groups of experiments, and the number of their anchor therefore, we arranged three groups of experiments, and the number of their anchor boxes were 3, 6, 653 boxes were 3, 6, and 9, respectively."
21,"if there are 3 anchor boxes, then each detection scale generates and 9, respectively."
21,"if there are 3 anchor boxes, then each detection scale generates only one bounding 654 only one bounding box."
21,"if there are 6 anchor boxes, then each detection scale generates two box."
21,"if there are 6 anchor boxes, then each detection scale generates two bounding boxes."
21,if there are 655 bounding boxes.
21,"if there are 9 anchor boxes, then each detection scale generates three bounding 9 anchor boxes, then each detection scale generates three bounding boxes."
21,"besides, 9 anchor boxes 656 boxes."
21,"besides, 9 anchor boxes are sufficient, because, in the ssdd dataset, a grid cell contains less are sufficient, because, in the ssdd dataset, a grid cell contains less than 9 ships."
21,"however, for other 657 than 9 ships."
21,"however, for other dataset, the maximum number of anchor boxes may be different. dataset, the maximum number of anchor boxes may be different."
21,"658 in addition, the three groups of experiments were conducted under the following same in addition, the three groups of experiments were conducted under the following same conditions: 659 conditions: (a) l = 160; 660 a) l = 160; (b) pretraining; 661 b) pretraining; (c) two concatenations; 662 c) two concatenations; 663 (d) d) three detection three scales. detection scales."
21,664 table77shows table showsthe evaluationindexes theevaluation research ofof indexes anchor onon research boxbox anchor mechanism.
21,figure mechanism. 15a 15a figure shows the shows 665 precision-recall (p-r) curves and figure 15b is the bar graph of map (accuracy) and fps (speed). the precision-recall (p-r) curves and figure 15b is the bar graph of map (accuracy) and fps (speed).
21,table 7.
21,evaluation indexes of anchor box mechanism.
21,666 table 7.
21,evaluation indexes of anchor box mechanism.
21,number precision number precisionrecall recall (ms)(ms) map timetime map fps fps 3 379.90% 79.90%89.56% 89.56% 86.58% 86.58% 8.80 8.80 114 114 6 685.86% 85.86%93.41% 93.41% 90.92% 90.92% 8.95 8.95 112 112 9 987.94% 87.94%96.15% 96.15% 94.13% 94.13% 9.03 9.03 111 111 667 1.0 100% 150 94.13% map 90.92% 86.58% fps 0.8 80% 114 112 111 100 precision 0.6 60% map fps 0.4 40% 50 0.2 3 anchor boxes 20% 6 anchor boxes 9 anchor boxes 0.0 0.0 0.2 0.4 0% 0 recall 0.6 0.8 1.0 3 anchor boxes 6 anchor boxes 9 anchor boxes (a) (b) figure research on 15.
21,research figure 15. on anchor box mechanism. anchor box mechanism. (a) p-r curves; (b) bar graph of bar graph of map map (accuracy) (accuracy) and and fps (speed).
21,fps (speed).
21,"668 from table from table 77 and figure15, andfigure wecan 15,we candraw drawthe followingconclusions: thefollowing conclusions: (1) the detection accuracy becomes higher with the increase of the number of anchor boxes."
21,"one possible reason is that more anchor boxes can improve the detection accuracy of densely distributed small ships; (2) the detection speed is not greatly affected by the number of anchor boxes, because the number of the network parameters are invariable for these three cases and only a small amount of follow-up processing is added with the increase of the number of anchor boxes."
21,"in addition, for these three cases, we also compared the detection results of some small ships with the dense distribution."
21,figure 16 shows the ship detection results of a sar image with densely
22,"670 one possible reason is that more anchor boxes can improve the detection accuracy of 671 densely distributed small ships; 672 2) the detection speed is not greatly affected by the number of anchor boxes, because the 673 number of the network parameters are invariable for these three cases and only a small 674 amount of follow-up processing is added with the increase of the number of anchor boxes."
22,"remote sens. 2019, 11, 2483 22 of 37 675 in addition, for these three cases, we also compared the detection results of some small ships 676 smalldistribution. with the dense distributed figure ships."
22,"in figure 16there 16a, shows 27 ship arethe detection real ships in theresults a sar image.ofin image figure 16b, with densely for 3 anchor 677 distributed small ships."
22,"in figure 16a, there are 27 real ships in the image."
22,"in figure 16b, boxes, 15 ships are detected successfully and 12 ships are missed detected."
22,"in figure 16c, for 6 anchorfor 3 anchor 678 boxes, 15 ships are detected successfully and 12 ships are missed detected."
22,"in figure 16c, boxes, 21 ships are detected successfully and 6 ships are missed detected."
22,"in figure 16d, for 9 anchor for 6 anchor 679 boxes,25 boxes, 21ships aredetected shipsare detectedsuccessfully and16ship ships successfullyand is are missed missed detected. detected, false for in figurea 16d, meanwhile 9 anchor alarm case 680 boxes, 25 ships are detected successfully and 1 ship is missed detected, meanwhile a false appears."
22,"in brief, if there are more anchor boxes, our model has a lower missed detection rate for those alarm case 681 and in appears. small if there areships. brief, distributed densely more anchor boxes, our model has a lower missed detection rate for 682 those small and densely distributed ships. (a) (b) (c) (d) figure16."
22,figure 16.
22,ship detection shipdetection result result an an of of image image withwith densely densely distributed distributed smallsmall ships. ships. (a) ground (a) ground truth; truth; (b) (b) 3 anchor 3 anchor boxes; boxes; (c) (c) 6 anchor 6 anchor boxes; boxes; (d) (d) 9 anchor 9 anchor boxes. boxes.
22,"(ground(ground truth istruth is marked marked in green, in green, missed missed detection detection is markedisin marked in red, red, and falseand false alarm alarm isin is marked marked in yellow.). yellow.)."
22,"683 therefore,finally, therefore, we finally, chose we 9 anchor chose boxes 9 anchor in in boxes ourour final model, final which model, cancan which improve the the accuracy improve of accuracy 684 small ships. of small ships."
22,685 5.5.
22,visualization 5.5.
22,"featuremaps visualizationofoffeature maps 686 throughthe through theabove fivetypes abovefive typesofofresearches section5.4, researchesininsection 5.4,we wedetermined ourfinal determinedour finalhigh-speed high-speed 687 sar ship detection model, and the specific parameters are as follows: sar ship detection model, and the specific parameters are as follows: 688 (a) la)= 160; l = 160; 689 (b) pretraining; b) pretraining; 690 (c) c) concatenations; two two concatenations; 691 (d) three detection; scalesscales d) three detection; 692 (e) e) nine anchor boxes."
22,nine anchor boxes.
22,"693 afterbuilding after buildingthe model,ininorder themodel, ordertotomore morevividly vividlypresent presentthe processingofofthe theprocessing inputimage theinput imageby by 694 deepneural deep neuralnetworks, networks, wewevisualized visualizedsome some important important feature maps feature of our maps network of our network (input, output (input, of theof output 695 1st thelayer, of backbone outputoutput 1st layer, network,network, of backbone detection output ofoutput of network-1, output of detection detection network-1, output of network-2, detection 696 and output of network-2, detection and of detectionbesides, output network-3). the visualization network-3)."
22,"besides, the of feature maps visualization of isfeature also convenient maps is alsoto 697 convenientthe understand process of feature to understand extraction the process by deep of feature neural by extraction networks. deep neural networks."
22,698 figure17 figure showsthe 17shows visualizationof thevisualization ofsome someimportant importantfeature maps.
22,in featuremaps.
22,"figure17a, in figure theimage 17a,the imageisis 699 resized resizedinto into160 × 160, and 160×160, andthe numberofofchannels thenumber channelsisis 3."
22,"ininfigure figure17b, 17b,after after thethe convolution convolution operation of operation st 700 of 1thelayer the 1 layer st (conv2d_1 in figure (conv2d_1 4), the 4), in figure of all sizethe size feature of allmaps × 80 is is 80maps feature × 32, where 80where 80×80×32, is the 80 number is the 701 width pixels ofnumber of width andpixels heightandpixels andpixels height 32 is and the number 32 is theofnumber channels."
22,from figure of channels.
22,"from 17b, the features figure 17b, the 702 features from extracted deepfrom extracted networks deep are abstractare networks and difficultand abstract to explain difficult(maybe texture, to explain (maybe edge,texture, shape, etc.), edge, 703 shape,isetc.), which which isinathe a consensus dl field in consensus [20,69]. the dl even fieldthis, [20] [69]."
22,"evencan computers surprisingly this, computersaccurately detect can surprisingly 704 accurately objects based detect objects on these based abstract on these features, abstract which subvertsfeatures, which extraction the feature subverts the feature theory extraction of traditional 705 theory ofin methods."
22,"figure 17c, traditional after processing methods."
22,"in figureof17c, the after backbone network, processing of the the backbone outputs consist network, the feature of 1024 outputs 706 consist maps of the with 1024 of 5 ×maps feature size with the size 5 (p-conv2d_13 of 5×54). in figure (p-conv2d_13 then, these 1024 feature4). in figure then, maps arethese 1024 inputted into three detection networks for ship detection."
22,"in figure 17d, there are 18 feature maps with the size of 5 × 5 (l/32) for detecting big size ships."
22,"in figure 17e, there are 18 feature maps with the size of 10 × 10 (l/16) for detecting medium size ships."
22,"in figure 17f, there are 18 feature maps with the size of 20 × 20 (l/8) for detecting small size ships."
22,"finally, our model will generate prediction boxes based on the feature maps of figure 17d–f."
22,more details about the visualization of feature maps can be found in reference [69].
23,707 feature maps are inputted into three detection networks for ship detection.
23,"in figure 17d, there are 708 18 feature maps with the size of 5×5 (l/32) for detecting big size ships."
23,"in figure 17e, there are 18 709 feature maps with the size of 10×10 (l/16) for detecting medium size ships."
23,"in figure 17f, there are 710 18 feature maps with the size of 20×20 (l/8) for detecting small size ships."
23,"finally, our model will 711 remote sens. 2019, generate 11, 2483 boxes based on the feature maps of figure 17d-f."
23,more details about prediction 37 23 ofthe 712 visualization of feature maps can be found in reference [69].
23,an original image (input: 160×160×3) (a) output of the 1st layer output of backbone network (conv2d_1: 80×80×32) (p-conv2d_13: 5×5×1024) (b) (c) output of detection network-1 output of detection network-2 output of detection network-3 (conv2d_4: 5×5×18) (conv2d_8: 10×10×18) (conv2d_4: 20×20×18) (d) (e) (f) figure17.
23,figure visualization 17.
23,visualization of of feature feature maps. maps. (a) original (a) an an original image; image; (b) output (b) output of the 1of the 1st(c)layer; st layer; (c)of output output of backbone network; (d) output of detection network-1; (e) output of detection network-2; backbone network; (d) output of detection network-1; (e) output of detection network-2; (f) output of of detection network-3. (f) outputnetwork-3. detection 6.
23,results 713 6.
23,"results in this section, firstly, we will carry out actual ship detection on the test set of ssdd dataset."
23,"714 then,in verify to this the migration section, capability firstly, we of the will carry outmodel, actual we will ship also carry detection on out the actual ship test set detection of ssdd in a dataset. 715 wide-region large-size then, to verify sentinel-1 the migration sar image. capability finally, of the model, willwill we we make a performance also comparison carry out actual between ship detection in 716 our method and other methods on the ssdd dataset."
23,"in addition, we did not show a wide-region large-size sentinel-1 sar image."
23,"finally, we will make a performance comparison the performance 717 comparison between our results andsentinel-1 on the method sar image, other methods because on the ssdd of: (1) theinsimilar dataset. conclusions addition, we did tonotthat on the show the 718 ssdd (2) page limit. dataset;comparison performance results on the sentinel-1 sar image, because of: 1) the similar conclusions 719 to that on the ssdd dataset; 2) page limit."
23,results of ship detection on test set 720 figure of 6.1.
23,results 18ship detection shows the sar shipset on test detection results of some sample images in the ssdd dataset.
23,"721 figure 18, from figure real ships 18 shows theinsarvarious ship backgrounds can almost detection results of some be sample detectedimages correctly, which in the shows ssdd that dataset. 722 our fromship detection figure system 18, real shipshas strong robustness. in various backgrounds addition, in can almostthere are alsocorrectly, be detected some false alarmshows which and 723 missed detection cases."
23,"for the former, one possible reason is the high similarity that our ship detection system has strong robustness."
23,"in addition, there are also some false alarmbetween other port 724 facilities and missedand detection the latter, ships."
23,"forcases. oneformer, for the possible onereason is that possible theseisships reason are densely the high similaritydistributed and between other 725 portsmall, too which facilities ships.some andbring for theobstacles to the latter, one system possible for their reason lower is that theseiou scores. ships in a word, are densely from a distributed 726 subjective point of view in figure 18, our model achieves satisfactory ship detection and too small, which bring some obstacles to the system for their lower iou scores."
23,"in a word, from results. 727 objectively then, wepoint a subjective of viewevaluated in figure our model 18, our on the model test setsatisfactory achieves of ssdd."
23,table 8 shows the ship detection evaluation results. indexes of the test set of ssdd.
23,figure 19a shows the precision-recall (p-r) curves and figure 19b is the bar graph of map (accuracy) and fps (speed).
24,remote sens.
24,"remote 2019, sens. 11,11, 2019, 2483 x for peer review of 37 2438 24 of figure figure sar 18.18."
24,sar ship shipdetection detectionresults. truthisismarked results.(
24,"ground (groundtruth ingreen, markedin missed detection green, missed is marked detection is marked in red, and false alarm is marked in yellow). in red, and false alarm is marked in yellow)."
24,table 8.
24,evaluation indexes of the test set of ssdd.
24,"728 then, we objectively evaluated our model on the test set of ssdd."
24,"table 8 shows the evaluation remote sens. 2019, 11, x for peer review 25 of 38 729 indexes ground oftruth the test set fp figure tpof ssdd. shows the precision-recall fn 19a precision recall timeand (p-r) curves map figurefps (ms) 19b is 730 the bar graph of map (accuracy) and fps (speed)."
24,182 175 24 7 87.94% 96.15% 94.13% 9.03 111 733 731 table 8.
24,evaluation indexes of the test set of ssdd.
24,1.0 100% 150 94.13% map ground truth tp fp fn precision recall map time (ms) fps fps 182 175 24 7 87.94% 96.15% 94.13% 9.03 111 0.8 80% 1.0 100% 94.13% 111 150 map fps 100 precision precision 0.6 60% 0.8 80% 111 map fps 100 0.4 40% 0.6 60% 50 map fps 0.2 20% 0.4 40% 50 0.0 0% 0 0.2 20% 0.0 0.2 0.4 recall 0.6 0.8 1.0 our method (a) (b) 0.0 0% 0 figure 0.2 results 19.19.
24,0.0figure 0.4of results ofrecall test the test setofof0.8 0.6 set ssdd.(a) ssdd. (a) 1.0 p-r p-r curves; curves; (b) graph bar graph (b) bar ofour mapof map (accuracy) method (accuracy) and and fps fps (speed). (speed).
24,"734 from figure 19 and table 8, we can draw the following conclusions: 735 1) the detection accuracy of our method is 94.13% map, which can meet the requirement of 736 sar ship detection; 737 2) the detection speed of our method is 111 fps, which can detect 111 sar images in the 738 ssdd dataset per second."
24,the number of sar images in the test set of the ssdd dataset is
25,"remote sens. 2019, 11, x for peer review 25 of 38 (a) (b) sens."
25,"2019, remotefigure 19. 11, 25 of 37 2483 of the test set of ssdd. (a) p-r curves; (b) bar graph of map (accuracy) and fps results (speed)."
25,"from figure 19 and table 8, we can draw the following conclusions: 733 from figure 19 and table 8, we can draw the following conclusions: (1) the detection accuracy of our method is 94.13% map, which can meet the requirement of sar 734 1) the detection accuracy of our method is 94.13% map, which can meet the requirement of ship detection; 735 sar ship detection; (2) the detection speed of our method is 111 fps, which can detect 111 sar images in the ssdd 736 2) the detection speed of our method is 111 fps, which can detect 111 sar images in the dataset per second."
25,the number of sar images in the test set of the ssdd dataset is 116 (10% 737 ssdd dataset per second.
25,"the number of sar images in the test set of the ssdd dataset is of 1160), and ds-cnn averagely takes 9.03 ms to complete the ship detection of one image. 738 116 (10% of 1160), and ds-cnn averagely takes 9.03 ms to complete the ship detection of therefore, ds-cnn takes 1047.48 ms (116 × 9.03 ms) to complete the ship detection of the whole 739 one image."
25,"therefore, ds-cnn takes 1047.48 ms (116×9.03 ms) to complete the ship test set."
25,"moreover, there are 182 ships in the test set of the ssdd dataset, then the detection 740 detection of the whole test set."
25,"moreover, there are 182 ships in the test set of the ssdd average speed is 5.76 ms/ship (1047.48 ms/182). 741 dataset, then the detection average speed is 5.76 ms/ship (1047.48 ms/182)."
25,"finally, we also compared the detection results of images with severe speckle noise and clear 742 finally, we also compared the detection results of images with severe speckle noise and clear images."
25,table 9 shows the ship detection evaluation indexes of images with severe speckle noise and 743 images.
25,table 9 shows the ship detection evaluation indexes of images with severe speckle noise clear images.
25,figure 20a shows the precision-recall (p-r) curves and figure 20b is the bar graph of 744 and clear images.
25,figure 20a shows the precision-recall (p-r) curves and figure 20b is the bar graph map (accuracy) and fps (speed).
25,figure 21 shows the ship detection results of images with severe 745 of map (accuracy) and fps (speed).
25,figure 21 shows the ship detection results of images with speckle noise and clear images. 746 severe speckle noise and clear images.
25,table 9.
25,ship detection evaluation indexes of images with severe speckle noise and clear images.
25,747 table 9.
25,ship detection evaluation indexes of images with severe speckle noise and clear images.
25,type precision typeprecision recall recall map (ms) (ms) map timetime fps fps clear 88.68% clear 88.68% 95.92% 95.92% 93.63% 93.63% 9.23 9.23 108 108 noisy 85.00% noisy 85.00% 97.14% 97.14% 96.91% 9.05 9.05 110 96.91% 110 748 1.0 100% 96.91% 150 93.63% map fps 0.8 80% 108 110 100 precision 0.6 60% map fps 0.4 40% 50 0.2 20% clear noisy 0.0 0% 0 0.0 0.2 0.4 recall 0.6 0.8 1.0 clear noisy (a) (b) 20.
25,results of images of with figure 20. with severe severe speckle speckle noise noise and clear images. and clear images. (a) p-r curves; (b) bar and fps (accuracy) and graph of map (accuracy) (speed).
25,fps (speed).
25,"from table 9, figures 20 and 21, we can draw the following conclusions: (1) the detection accuracy of the nosiy images is superior to that of the clear images (96.91% map > 93.63% map)."
25,"this phenomenon seems to be out of line with common sense, because usually, clear images should have higher accuracy than noisy ones."
25,"in fact, the possible reasons for this phenomenon are that (1) in the test set of the ssdd dataset (1160 × 10% = 116), the number of moisy images is less than that of clear images (28 of 116 < 88 of 116), which may lead to greater accuracy randomness; (2) noisy images have a single background and contain offshore ships (see figure 21a) while clear images have more complex backgrounds and contain onshore ships (see figure 21b), and evidently, it is more difficulty to detect the onshore ships than the offshore ships; (2) the detection speed of the clear images is similar to that of the nosiy images, because the size of images is the same."
26,"remote sens. 2019, 11, 2483 26 of 37 remote sens. 2019, 11, x for peer review 26 of 38 (a) (b) sarship 21."
26,"sar figure 21. shipdetection results detection of images results of with of images of severe specklespeckle with severe noise and clear noise images. clear (ground and images. truth is marked (ground green, missed truth isinmarked detection in green, marked in missed isdetection is marked red,alarm false red, and in is marked and false alarminisyellow). marked in yellow)."
26,"results of ship detection on sentinel-1 749 in order to verify that our ship detection model has strong migration ability, we carried out actual 750 ship from table detection on9,afigure 20-21, we wide-region can draw large-size the following sentinel-1 conclusions: sar image with multiple targets and complex backgrounds."
26,"we obtained this sar image from the website of reference [70], and obtained the ground 751 1) the detection accuracy of the nosiy images is superior to that of the clear images (96.91% truth information from the association for information systems (ais) [71,72]."
26,table 10 shows the 752 map > 93.63% map).
26,"this phenomenon seems to be out of line with common sense, descriptions of the sentinel-1 sar image to be detected. 753 because usually, clear images should have higher accuracy than noisy ones."
26,"in fact, the 754 possible reasons for this phenomenon are that 1) in the test set of the ssdd dataset table 10."
26,descriptions of the sentinel-1 sar image.
26,az.: azimuth; rg.: range.
26,"h: horizontal; 755 (1160×10% = 116), the number of moisy images is less than that of clear images (28 of 116 < v: vertical. 756 88 of 116), which may lead to greater accuracy randomness; 2) noisy images have a single 757 background resolution and contain imaging offshoreincident ships (seeimage size 21a) while clear images have more figure cover area band time polarization az."
26,"mode angle pixel × pixel 758 complex backgrounds and contain onshore ships (see figure 21b), and evidently, it is more zhe jiang, china 10 m × 10 m iw 1 34~46◦ 6333 × 4185 c 8 july 2015 vv, vh 759 difficulty to detect the onshore ships than the offshore ships; 1 the interferometric wide (iw) swath mode is the main acquisition mode over land and satisfies the majority of 760 2) the detection speed of the clear images is similar to that of the nosiy images, because the service requirements [72]."
26,it acquires data with a 250 km swath at 5 m by 20 m spatial resolution (single look) [72]. 761 of images sizecaptures iw mode three is the same. sub-swaths using terrain observation with progressive scans sar (topsar) [73].
26,762 6.2.
26,"results detectionthe of ship divided we directly on sentinel-1 original wide-region large-size sar image (6333 × 4185) into 900 763 sub-images in order average, onto verify that 211 × with our 140detection ship size for each sub-image, model and migration has strong then resized these we 900 carried ability, sub-images out 764 into 160 × 160."
26,"finally, these sub-images were inputted into our high-speed sar ship actual ship detection on a wide-region large-size sentinel-1 sar image with multiple targets and detection system 765 to carry out complex the actual ship backgrounds."
26,"wedetection. obtained this sar image from the website of reference [70], and 766 figure obtained the22ground showstruth the ship results detectionfrom information the of the sentinel-1 association sar image.systems for information from figure 22, our (ais) [71,72]. 767 method can detect almost all ships (marked in blue)."
26,"there are some table 10 shows the descriptions of the sentinel-1 sar image to be detected. false alarm cases on land (marked in yellow), which is due to the high similarity between these land facilities and real ships."
26,"besides, 768 are some theretable missed detection 10."
26,"descriptions of the cases sar in (marked sentinel-1 which red), az.: image. may berg.: azimuth; duerange. to the h: dense distribution horizontal; v: or 769 parallel parking of ships, causing certain difficulties for detection."
26,"in short, our method has strong vertical. migration ability which can be applied in practical sar ship detection."
26,"cover resolution imaging incident image size then, we evaluated our model on the sentinel-1 sar image."
26,bandtabletime11 showspolarization the evaluation area az.
26,"mode angle pixel × pixel indexes of the sentinel-1 sar image.1 figure 23a shows the precision-recall (p-r) curves and figure 23b zhe jiang, china 10m×10m iw 34~46° 6333×4185 c 8 july 2015 vv, vh is the1 bar graph of map (accuracy) and fps (speed)."
26,770 the interferometric wide (iw) swath mode is the main acquisition mode over land and 771 satisfies the majority of service requirements [72].
26,it acquires data with a 250 km swath at 5 m by 20 772 m spatial resolution (single look) [72].
26,iw mode captures three sub-swaths using terrain 773 observation with progressive scans sar (topsar) [73].
26,"774 we directly divided the original wide-region large-size sar image (6333 × 4185) into 900 775 sub-images on average, with 211×140 size for each sub-image, and then resized these 900"
27,778 figure 22 shows the ship detection results of the sentinel-1 sar image.
27,"from figure 22, our 783 779 method methodhas detectmigration canstrong shipswhich ability almost all can be (marked in applied in practical blue)."
27,"there sarfalse are some ship detection. alarm cases on land 780 (marked in yellow), which is due to the high similarity between these land facilities and real ships."
27,"781 besides, there are some missed detection cases (marked in red), which may be due to the dense 782 distribution or parallel parking of ships, causing certain difficulties for detection."
27,"in short, our remote sens. 2019, 11, 2483 27 of 37 783 method has strong migration ability which can be applied in practical sar ship detection."
27,figure 22.
27,ship detection results of the sentinel-1 sar image.
27,"(detection results are marked in blue, missed detection is marked in red, and false alarm is marked in yellow)."
27,"784 then, we evaluated our model on the sentinel-1 sar image."
27,table 11 shows the evaluation 785 indexes of the sentinel-1 sar image.
27,figure 23a shows the precision-recall (p-r) curves and figure 22.
27,ship figure22. detectionresults shipdetection ofthe resultsof thesentinel-1 sentinel-1sar sarimage. image.
27,"(detection results are (detection results marked in are marked blue, in blue, 786 23b isfigure the bar graph of map (accuracy) and fps (speed). misseddetection missed detectionisismarked red,and markedininred, falsealarm andfalse markedininyellow). alarmisismarked yellow)."
27,"787 784 then, table we evaluated table 11."
27,evaluation indexes of the ship detection results of the sentinel-1 sar image.
27,evaluation model of our indexes onthethe ship detection results sentinel-1 of the sentinel-1 sar image.
27,table 11sar image. shows the evaluation 785 indexes of the sentinel-1 sar image.timefigureof 23a shows time of timetheofprecision-recall time of fps of (p-r) curves fps of time ofand figure time of whole precision precision recall recall map map 786 23b is the bar graph of map preprocessing (accuracy) and fps preprocessing (s)(s) sub-image(s)(s) sub-image sub-image (speed).
27,sub-image wholeimage image(s)(s) 91.15% 89.63% 90.36% 54.85×10-3 −3 8.92×10-3 −3 116 8.09 91.15% 89.63% 90.36% 54.85 × 10 8.92 × 10 116 8.09 787 788 table 11.
27,evaluation indexes of the ship detection results of the sentinel-1 sar image.
27,1.0 time of 100% time of fps of time of 150 precision recall map map preprocessing (s) sub-image (s)90.36% sub-image whole image (s) fps 0.8 91.15% 89.63% 90.36% 54.85×10-3 80% 8.92×10-3 116 116 8.09 788 100 precision 1.0 0.6 100% 60% 150 90.36% map map fps fps 0.4 0.8 80% 40% 116 50 100 precision 0.2 0.6 60% 20% map fps 0.4 0.0 40% 0% 0 0.0 0.2 0.4 recall 0.6 0.8 1.0 our method 50 (a) (b) 0.2 20% resultsofofthe figure 23.
27,results sentinel-1image. thesentinel-1 (a)(a) image. curves; p-rp-r bar bar (b) (b) curves; graph of map graph (accuracy) of map and fps (accuracy) and 0.0 (speed).
27,fps (speed).
27,"0% 0 0.0 0.2 0.4 recall 0.6 0.8 1.0 our method from table 11 and (a)figure 23, our method has high accuracy with(b) 90.36% map, which can meet the practical application requirements."
27,the time to divide the wide-region large-size sar figure 23.
27,results of the sentinel-1 image. (a) p-r curves; (b) bar graph of map (accuracy) and fps image into small-size sub-images (time of preprocessing) is 54.85 × 10−3 s by using python imaging (speed).
27,"library (pil), an image preprocessing tool."
27,"additionally, our method takes only 8.92 × 10−3 s for one sub-image (160 × 160) similar to the detection time in ssdd dataset (9.03 × 10−3 s), and takes only 8.09 s (8.92 × 10−3 s × 900 + 54.85 × 10−3 s = 8.09 s) to complete the whole wide-region large-size sar image detection (6333 × 4185)."
27,"therefore, our method is of value in practical application from the above detection results."
28,790 the practical application requirements.
28,"the time to divide the wide-region large-size sar image 791 into small-size sub-images (time of preprocessing) is 54.85×10-3 s by using python imaging library 792 (pil), an image preprocessing tool."
28,"additionally, our method takes only 8.92×10-3 s for one 793 sub-image (160×160) similar to the detection time in ssdd dataset (9.03×10-3 s), and takes only 8.09 s 794 (8.92×10-3 s×900 + 54.85×10-3 s =8.09s) to complete the whole wide-region large-size sar image remote sens. 2019, 11, 2483 28 of 37 795 detection (6333 × 4185)."
28,"therefore, our method is of value in practical application from the above 796 detection results."
28,compared with c-cnn 797 6.3.
28,"compared with c-cnn from figure 4, there are 13 ds-cnns in our backbone network, which are used to extract ships’ 798 from features."
28,"figure that to confirm 4, there are 13 ds-cnn can ds-cnns in our really speed backbone up ship network, detection which little and sacrifice are used to extract accuracy, we 799 ships’ features."
28,"to confirm that ds-cnn replaced these 13 ds-cnns in figure 4 with 13 c-cnns to carry out further research, as is shown little can really speed up ship detection and sacrifice in 800 24. we replaced these 13 ds-cnns in figure 4 with 13 c-cnns to carry out further research, accuracy, figure 801 as is shown in figure 24."
28,ds-cnn 80×80×32 c-cnn d-conv2d_n dethwise 80×80×32 kernel (3×3×32×1) 80×80×32 conv2d_n kernel (3×3×32×64) p-conv2d_npointwise 80×80×64 kernel (1×1×32×64) 80×80×64 figure 24.
28,replace ds-cnn in figure 4 with c-cnn.
28,figure 24.
28,replace ds-cnn in figure 4 with c-cnn.
28,"in addition, in order to ensure the rationality of the study, we arranged the following two groups 802 of experiments in addition, in order to ensure the rationality of the study, we arranged the following two under the following same conditions: 803 groups of experiments under the following same conditions: (a) l = 160; 804 (b) a) l = 160; pretraining; 805 (c) pretraining; b) concatenations; two 806 (d) c) two concatenations; three scales detection; 807 d) three scales detection; (e) nine anchor boxes. 808 e) nine anchor boxes."
28,"in particular, we only compared the detection speed of small-size sub-images in the test set of the 809 in particular, ssdd dataset, because only wethe compared faster detectionthespeed detection speed ofsub-images of small-size small-size sub-images the test in bring will naturally set of about 810 the ssdd dataset, because the faster detection speed of the increase of detection speed of wide-region large-size images. small-size sub-images will naturally bring 811 about tablethe12 showsofthe increase detection of wide-region speedindexes evaluation of c-cnnlarge-size and images."
28,ds-cnn.
28,figure 25a shows the 812 table 12 shows the evaluation indexes of c-cnn precision-recall (p-r) curves and figure 25b is the bar graph of map (accuracy) figure and ds-cnn. and fps25a shows the (speed). 813 precision-recall (p-r) curves and figure 25b is the bar graph of map (accuracy) and fps (speed).
28,"814 figure from table 12 andtable 12.25, we can draw evaluation the indexes offollowing c-cnn and conclusions: ds-cnn."
28,815 the name1) precisionof ds-cnn detection accuracy recall is slightly maplower than time c-cnn (ms) (94.13% fps map < 94.39% 816 map).
28,"c-cnn this phenomenon 89.29% shows that 96.15% using ds-cnn 94.39% to build 24.17 a network41 is feasible and 817 does ds-cnnnot decline 87.94% accuracy too much; the detection 96.15% remote sens. 2019, 11, x for peer review 94.13% 9.03 111 29 of 38 818 2) the detection speed of ds-cnn is 2.7 times than c-cnn (111 fps > 41 fps)."
28,this 819 1.0 phenomenon shows that ds-cnn can 100%indeed improve the detection speed by using 94.39% 94.13% 150 map 820 d-conv2d and p-conv2d to replace c-cnn.
28,fps 0.8 80% 111 821 table 12.
28,evaluation indexes of c-cnn and ds-cnn.
28,100 precision 0.6 name precision 60% map recall time (ms) fps map fps c-cnn 89.29% 96.15% 94.39% 24.17 41 0.4 ds-cnn 87.94% 96.15% 40% 94.13% 9.03 111 822 41 50 0.2 20% c-cnn ds-cnn 0.0 0% 0 0.0 0.2 0.4 recall 0.6 0.8 1.0 c-cnn ds-cnn (a) (b) figure 25.
28,figure 25.
28,results ofc-cnn results of andds-cnn.
28,c-cnnand ds-cnn.(a)(a)p-r p-rcurves; (b)(b) curves; graph barbar of map graph (accuracy) of map andand (accuracy) fps (speed).
28,fps (speed).
28,"823 especially, it is meaningful to increase the detection speed from 41 fps to 111 fps for 160×160 824 images."
28,"for example, ds-cnn only takes 10.45 s to complete the ship detection task of 1160 sar 825 images in the ssdd dataset, while c-cnn needs 28.2 s."
28,"in general, even millisecond level time 826 saving is meaningful and valuable in the field of image processing."
28,"in addition, for the wide-region 827 large-size sar image (6333×4185) in figure 22, the detection time of ds-cnn is 8.09 s, while that of"
29,"remote sens. 2019, 11, 2483 29 of 37 from table 12 and figure 25, we can draw the following conclusions: (1) the detection accuracy of ds-cnn is slightly lower than c-cnn (94.13% map < 94.39% map)."
29,this phenomenon shows that using ds-cnn to build a network is feasible and does not decline the detection accuracy too much; (2) the detection speed of ds-cnn is 2.7 times than c-cnn (111 fps > 41 fps).
29,this phenomenon shows that ds-cnn can indeed improve the detection speed by using d-conv2d and p-conv2d to replace c-cnn.
29,"especially, it is meaningful to increase the detection speed from 41 fps to 111 fps for 160 × 160 images."
29,"for example, ds-cnn only takes 10.45 s to complete the ship detection task of 1160 sar images in the ssdd dataset, while c-cnn needs 28.2 s."
29,"in general, even millisecond level time saving is meaningful and valuable in the field of image processing."
29,"in addition, for the wide-region large-size sar image (6333 × 4185) in figure 22, the detection time of ds-cnn is 8.09 s, while that of c-cnn is 21.8 s."
29,"from the perspective of image processing, such much time saving is remarkable, which can facilitate subsequent reprocessing (such as ship classification) and improve efficiency of full-link sar application."
29,"most noteworthy, generally, the size of sar images acquired from spaceborne sar satellites is often larger (tens of thousands of pixels × tens of thousands of pixels), so in this case, the advantages of ds-cnn will be better reflected."
29,we also compared the network sizes of c-cnn and ds-cnn.
29,"in the dl field, the number of network parameters, model file size, and weight file size are often used to measure the size of the network."
29,"therefore, we made a comparison from the above three perspectives to illustrate the lightweight nature of our network."
29,table 13 shows the network sizes of c-cnn and ds-cnn.
29,table 13.
29,network sizes of c-cnn and ds-cnn.
29,"method number of parameters model file size (kbyte) weight file size (kbyte) c-cnn 28,352,054 334,783 119,986 ds-cnn 3,299,862 38,965 13,159 from table 13, the number of network parameters of our proposed ds-cnn is only 3,299,862, which is about one-ninth of the c-cnn’s (28,352,054)."
29,"similarly, the model file size and weight file size are nearly one-tenth of the c-cnn’s."
29,"therefore, our network is lighter, leading to higher training efficiency and faster detection speed, which accords with the theoretical analysis before in section 2.3."
29,figure 26 shows the ship detection results of c-cnn and ds-cnn.
29,"from figure 26, both c-cnn and ds-cnn can detect almost all ships successfully, with similar accuracy."
29,"in addition, for the second image in figure 26, ds-cnn generates more missed detection cases (the number of missed detection is (3) than c-cnn (the number of missed detection is (1)."
29,one possible reason is that ds-cnn does not adequately extract features from the inshore ships.
29,"therefore, our method is effective and brilliant."
29,"despite of the slight sacrifice of accuracy for the inshore ships, the detection speed has got largely improved."
29,"compared with other object detectors in order to prove that our method can authentically achieve high-speed ship detection, we also compared the performance with other object detectors, such as faster r-cnn, retinanet, ssd, yolov1, yolov2, yolov2-tiny, yolov3, and yolov3-tiny."
29,"here, yolov2-tiny and yolov3-tiny are two lightweight networks that improve yolov2 and yolov3 respectively, proposed by redmon et al."
29,"[31,32]."
29,"moreover, in order to ensure the rationality of experiments, these other object detectors are implemented on the same platform, and the training mechanism is basically consistent."
30,"remote sens. 2019, 11, 2483 30 of 37 remote sens. 2019, 11, x for peer review 30 of 38 (a) (b) (c) ds-cnn (a) ground figure 26."
30,"ship detection results of c-cnn and ds-cnn truth; (b) ground truth; c-cnn; (c) ds-cnn. (b) c-cnn; ds-cnn. marked in (ground truth is marked green,missed in green, detectionisismarked misseddetection red,and markedininred, false and alarm false is marked alarm in is marked yellow.). in yellow.)."
30,"849 therefore, the especially, ourdetection method is speed traditional of theand effective brilliant.feature despiteextraction methods of the slight is quite sacrifice slow, whose of accuracy for 850 detection the inshoretime reaches ships, several seconds the detection or more speed has per image, got largely while the modern dl methods only need improved. tens or hundreds of milliseconds per image."
30,"therefore, we ignored the comparison with the traditional 851 feature withmethods. extraction 6.4."
30,compared other object detectors table 14 shows the evaluation indexes of different object detectors.
30,"figure 27 shows the 852 in order to prove that our method can authentically achieve high-speed ship detection, we also precision-recall (p-r) curves and figure 28 is the bar graph of map (accuracy) and fps (speed). 853 compared the performance with other object detectors, such as faster r-cnn, retinanet, ssd, 854 yolov1, yolov2, yolov2-tiny, yolov3, table 14."
30,"evaluation and indexes of yolov3-tiny. different here, yolov2-tiny and object detectors."
30,"855 yolov3-tiny are two lightweight networks that improve yolov2 and yolov3 respectively, 856 redmon. et.alprecision proposed by method recallin ordermap [31,32]."
30,"moreover, to ensure time (ms) the rationality fps of experiments, 857 faster r-cnn 81.15% 85.16% 82.66% 327.48 these other object detectors are implemented on the same platform, and the training3 mechanism is 858 retinanet basically consistent."
30,"93.12% 96.70% 95.68% 314.43 3 ssd 85.15% 94.51% 92.67% 48.86 20 859 especially, speed of the the detection93.62% yolov3 traditional 95.34% 96.70% feature extraction 22.30 methods45is quite slow, 860 whose detection several seconds time reaches77.58% yolov3-tiny 70.33% per image, while or more 64.64% 10.25the modern 98 dl methods 861 only need tens or hundreds of yolov2 milliseconds 84.92% per image."
30,"therefore, 92.86% 90.09% we 19.01 ignored the comparison 53 with 862 yolov2-tiny the traditional 73.73% feature extraction methods."
30,47.80% 44.40% 9.43 107 863 table 14yolov1 84.53% shows the evaluation 84.07% indexes 81.24% of different 21.95 object detectors.
30,figure4627 shows the our method 87.94% 96.15% 94.13% 9.03 111 864 precision-recall (p-r) curves and figure 28 is the bar graph of map (accuracy) and fps (speed).
30,"865 table 14.27evaluation from table 14 and figures and 28, we can draw indexes the following of different conclusions: object detectors."
30,(1) the detection accuracy method of our method recall precision is slightly map lowertime than(ms)retinanet fps (94.13% map < 95.68% map).
30,"however, 81.15% speed our detection faster r-cnn is almost 85.16% faster than3 retinanet."
30,"therefore, 37 times327.48 82.66% it is of far-reaching significance retinanet that a 93.12%little sacrifice of 96.70% 95.68% accuracy 314.43 about brings 3 a multiplier increase in speed; ssd 85.15% 94.51% 92.67% 48.86 20 (2) yolov3of our 93.62% the detection accuracy method is96.70% 95.34%lower22.30 also slightly 45 than yolov3 (94.13% map < yolov3-tiny 95.34% map)."
30,"however, 77.58% speed our detection 70.33% 64.64% is 2.47 10.25 times faster 98 than yolov3."
30,"therefore, our yolov2 84.92% 92.86% 90.09% 19.01 approach is still excellent because we traded 2.1% accuracy for a 147% speed increase;53 yolov2-tiny 73.73% 47.80% 44.40% 9.43 107 yolov1 84.53% 84.07% 81.24% 21.95 46"
31,"remote sens. 2019, 11, 2483 31 of 37 (3) the detection accuracy and speed of our method are all superior to faster r-cnn, ssd, yolov1, yolov2, and yolov3-tiny; (4) the detection speed of our method is slightly faster than yolov2-tiny (111 fps > 107 fps)."
31,"however, the detection accuracy of yolov2-tiny is too poor to meet the actual detection requirements at all (44.40% map), which is far lower than that of our method (94.13% map); (5) the detection speed of our method is superior to all other methods (9.03 ms per image and 111 fps); (6) our method has not only the fastest detection speed but also satisfactory accuracy (both map sens.2019, remotesens."
31,"2019,11, remote and fps forpeer xxfor are11,high inpeer review review figure 31 of 31 28), while other methods cannot maintain a good balance between 38 of 38 accuracy and speed. method ourmethod our 87.94% 87.94% 96.15% 94.13% 96.15% 94.13% 9.03 9.03 111 111 866 866 1.0 1.0 0.8 0.8 precision fasterr-cnn r-cnn precision 0.6 0.6 faster retinanet retinanet ssd ssd 0.4 0.4 yolov3 yolov3 yolov3-tiny yolov3-tiny yolov2 yolov2 0.2 0.2 yolov2-tiny yolov2-tiny yolov1 yolov1 ourmethod our method 0.0 0.0 0.0 0.0 0.2 0.2 0.4 recall 0.6 0.4 0.6 0.8 0.8 1.0 1.0 recall figure27."
31,figure figure 27. curvesof p-rcurves 27.
31,p-r p-r curves differentobject ofdifferent of different objectdetectors. object detectors. detectors.
31,100% 100% 120 120 95.68% 95.68% 92.67% 95.34% 95.34% 94.13% map94.13% 111 92.67% 90.09% map 111 90.09% 107 107 fps 82.66% fps 82.66% 98 98 81.24% 81.24% 100 80% 100 80% 64.64% 64.64% 80 80 60% 60% map fps map fps 60 60 44.40% 53 44.40% 53 49 49 46 46 40% 40% 45 45 40 40 20% 20% 20 20 20 20 33 33 0% 0% 00 t 3 2 1 nnn neet sdd ovv3 inyy ovv2 inyy ovv1 odd -cn nan sss ollo 3-t-itn ollo 2-t-itn ollo eththo errrr-c e retitni a yyo o lovv3 yyo o lovv2 yyo u rrmme astset r ol ol oou ffa yyo yyo figure 28.
31,figure28.
31,bar 28.
31,bar graph bargraph of graphof map ofmap (accuracy) (accuracy)and map(accuracy) and fps andfps (speed) fps(speed) of (speed)of different object differentobject ofdifferent detectors. detectors. objectdetectors.
31,figure figure 29 shows the ship detection results of some sample images of different object detectors.
31,"867 867 fromtable from table1414and figure27–28, andfigure 27–28,wewecan candraw drawthethefollowing conclusions: followingconclusions: from figure 29, we can draw the following conclusions: 868 868 1) the 1) detection accuracy the detection accuracy of our method of our slightly lower method isis slightly lower than retinanet (94.13% than retinanet map << (94.13% map (1) all methods can successfully detect ships in simple sample images, except yolov3-tiny 869 869 95.68% map)."
31,95.68% map).
31,"however, however, our detection speed our detection speed isis almost almost 37 times faster 37 times than retinanet. faster than retinanet. and yolov2-tiny; 870 870 therefore, it is of far-reaching significance that a little sacrifice of accuracy therefore, it is of far-reaching significance that a little sacrifice of accuracy brings aboutbrings about aa 871 (2) yolov3-tiny and yolov2-tiny multiplierincrease increasein speed; inspeed; generate more missed detection cases; 871 multiplier 872 872 2) the 2) detection accuracy the detection accuracy of our method of our also slightly method isis also slightly lower lower than than yolov3 (94.13% map yolov3 (94.13% map << 873 873 95.34% map)."
31,"however, our detection speed is 2.47 times faster than 95.34% map)."
31,"however, our detection speed is 2.47 times faster than yolov3."
31,"therefore,yolov3."
31,"therefore, 874 874 ourapproach our approachisisstill stillexcellent becausewe excellentbecause traded2.1% wetraded 2.1%accuracy accuracyfor 147%speed foraa147% increase; speedincrease; 875 875 3) the detection accuracy and speed of our method are all superior to 3) the detection accuracy and speed of our method are all superior to faster r-cnn, ssd, faster r-cnn, ssd, 876 876 yolov1, yolov2, and yolov3-tiny; yolov1, yolov2, and yolov3-tiny;"
32,"remote sens. 2019, 11, 2483 32 of 37 (3) retinanet and yolov2 have a better detection performance for small ships with dense distribution, with only one missed detection case; remotecompared (4) sens. 2019, 11,with retinanet x for and yolov2, yolov3 have similar detection performance for33small peer review of 38 ships with dense distribution, with only one missed detection case and only one false alarm case; (5) compared with all other methods, from a subjective point of view in figure 29, our method has 899 obtained satisfactory ship detection results, although not the best. (a) (b) (c) (d) (e) (f) (g) (h) (i) (a) faster figure 29."
32,"sar ship detection results of different object detectors. (a) r-cnn; (b) faster r-cnn; retinanet; (b) retinanet; yolov2-tiny; (h) (c) ssd; (d) yolov3; (e) yolov3-tiny; (f) yolov2; (g) yolov2-tiny; yolov1; (i) (h) yolov1; our method. (i) our method. false alarm (missed detection is marked in red, and false is marked alarm is in yellow). marked in yellow). 900 alsocompared we also we comparedthe network the networksizes of other sizes object of other detectors. object table table detectors. 15 shows the network 15 shows sizes of the network 901 different object detectors."
32,"from table 15, our model has the fewest network parameters sizes of different object detectors."
32,"from table 15, our model has the fewest network parameters only 3,299,862, 902 while the numbers only 3,299,862, network ofthe while numbersparameters of network of other methods parameters are several of other methodsor are even tens ofortimes even than several tens 903 that of our method."
32,"moreover, the model file size and the weight file size are also minimal of times than that of our method."
32,"moreover, the model file size and the weight file size are also compared 904 minimal compared with other methods."
32,"therefore, our network is lighter, leading to higher training 905 efficiency and faster detection speed, which also accords with the theoretical analysis before in 906 section 2.3."
32,"907 in summary, by comparing the actual network size, it further more forcefully indicated that 908 our method can achieve high-speed sar ship detection."
33,"remote sens. 2019, 11, 2483 33 of 37 with other methods."
33,"therefore, our network is lighter, leading to higher training efficiency and faster detection speed, which also accords with the theoretical analysis before in section 2.3."
33,table 15.
33,network sizes of different object detectors.
33,"method number of parameters model file size (kbyte) weight file size (kbyte) yolov1 272,746,867 2,308,877 770,814 yolov3 61,576,342 722,131 241,082 yolov2 50,578,686 592,777 197,668 retinanet 36,382,957 426,195 142,573 faster r-cnn 28,342,195 310,112 111,144 ssd 23,745,908 278,550 92,904 yolov2-tiny 15,770,510 184,858 61,646 yolov3-tiny 8,676,244 101,799 33,990 our method 3,299,862 38,965 13,159 in summary, by comparing the actual network size, it further more forcefully indicated that our method can achieve high-speed sar ship detection."
33,compared with references we also compared our methods with some previous open studies which use the same ssdd dataset.
33,"to be clear, here, we replaced nvidia rtx2080ti gpu with nvidia gtx1080 gpu to keep the hardware environment basically the same as previous other open studies (references [10,21,35,59–62] used nvidia gtx1080 gpu.)."
33,"for different performances of different types of gpus, we have to make such a replacement in order to consider the rationality of the comparison experiment."
33,"remotefigure shows 3011, sens. 2019, x forthe reviewresults of ship detection speed (fps, frames per second) between contrast peer 34 of 38 our method and previous other open studies."
33,60 fps fps 53 50 48 40 40 fps 30 23 20 10 10 5 6 3 0 59] 60] 35] 21] 61] 62] 10] od nc e[ nce[ nc e[ nc e[ nc e[ nc e[ nce[ meth e e e e e e e r fer fer fer fer fer fer fer ou re re re re re re re 30.
33,detection figure 30.
33,figure detectionspeed speedcomparison between comparison our method between and references our method under the and references similar under thehardware similar environment with nvidia gtx1080 gpu. hardware environment with nvidia gtx1080 gpu.
33,"917 from figure from figure 30, ship detection 30, the the ship speed of detection speed our method of our method is is 53 53 fps fps under the nvidia under the gtx1080 nvidia gtx1080 918 gpu, which is the half of that under the nvidia rtx2080ti gpu (111 fps introduced gpu, which is the half of that under the nvidia rtx2080ti gpu (111 fps introduced before). before)."
33,"this 919 phenomenon is in line with common sense, because the performance of rtx2080ti gpu is superior 920 to that of gtx1080 gpu."
33,"in figure 30, our method has the fastest detection speed compared with 921 the other references under the similar hardware environment with nvidia gtx1080 gpu."
33,"most 922 noteworthy, the detection speed of reference [10] is 48 fps, which is close to that of the method in 923 this paper (53 fps)."
33,"however, the accuracy of reference [10] is lower than ours (reference [10]:"
34,"remote sens. 2019, 11, 2483 34 of 37 this phenomenon is in line with common sense, because the performance of rtx2080ti gpu is superior to that of gtx1080 gpu."
34,"in figure 30, our method has the fastest detection speed compared with the other references under the similar hardware environment with nvidia gtx1080 gpu."
34,"most noteworthy, the detection speed of reference [10] is 48 fps, which is close to that of the method in this paper (53 fps)."
34,"however, the accuracy of reference [10] is lower than ours (reference [10]: 90.16% map; ours: 94.13% map)."
34,"therefore, our method can reliably realize high-speed sar ship detection."
34,"in addition, due to the lacking of specific type gpus, we cannot make full comparisons between our method and the remaining studies using the same ssdd dataset."
34,"conclusions aiming at the problem that the detection speed of sar ship is often neglected at present, in this paper, a brand-new light-weight network was established with fewer network parameters by mainly using ds-cnn to achieve high-speed sar ship detection."
34,"ds-cnn is composed of d-conv2d and p-conv2d, which substitute for the c-cnn, by which the number of network parameters get greatly decreased."
34,"consequently, the detection speed gets dramatically improved."
34,"in addition, multi-scale detection mechanism, concatenation mechanism and anchor box mechanism are integrated into our network to improve the detection accuracy."
34,we verified the correctness and effectiveness of the proposed method on an open ssdd dataset.
34,"the experimental results indicated that our method has the faster ship detection speed than other object detectors, under the same hardware platform, with 9.03 ms per image and 111 fps, meanwhile the detection accuracy is only lightly sacrificed compared with the state-of-art object detectors."
34,"besides, we also discussed the reasons why the proposed method can achieve high-speed ship detection via theoretical complexity analysis and calculation of network size."
34,"our method can achieve high-speed and accurate ship detection simultaneously (111 fps and 94.13% map) compared with other methods, which has great application value in real-time maritime disaster rescue and emergency military planning."
34,"author contributions: conceptualization, t.z.; methodology, t.z.; software, t.z.; validation, x.z.; formal analysis, x.z.; investigation, t.z.; resources, t.z.; data curation, x.z.; writing—original draft preparation, t.z.; writing—review & editing, t.z. and x.z.; visualization, t.z.; supervision, j.s. and s.w.; project administration, x.z.; funding acquisition, x.z."
34,"funding: this work was supported in part by the national key r&d program of china under grant 2017yfb0502700 and in part by the national natural science foundation of china under grants 61571099, 61501098, and 61671113."
34,acknowledgments: we thank anonymous reviewers for their comments towards improving this manuscript.
34,the authors would also like to thank durga kumar for his linguistic assistance during the preparation of this manuscript.
34,conflicts of interest: the authors declare no conflict of interest.
34,references 1.
34,"born, g.h."
34,"; dunne, j.a."
34,"; lame, d.b."
34,seasat mission overview.
34,"science 1979, 204, 1405–1406."
34,[crossref] [pubmed] 2.
34,"petit, m."
34,"; stretta, j.m."
34,"; farrugio, h."
34,"; wadsworth, a."
34,synthetic aperture radar imaging of sea surface life and fishing activities.
34,ieee trans.
34,geosci.
34,remote sens.
34,"1992, 30, 1085–1089."
34,[crossref] 3.
34,"cerutti-maori, d."
34,"; klare, j."
34,"; brenner, a.r."
34,"; ender, j.h.g."
34,wide-area traffic monitoring with the sar/gmti system pamir.
34,ieee trans.
34,geosci.
34,remote sens.
34,"2008, 46, 3019–3030."
34,[crossref] 4.
34,"dierking, w."
34,"; busche, t."
34,sea ice monitoring by l-band sar: an assessment based on literature and comparisons of jers-1 and ers-1 imagery.
34,ieee trans.
34,geosci.
34,remote sens.
34,"2006, 44, 957–970."
34,[crossref] 5.
34,"solberg, a.h.s."
34,"; storvik, g."
34,"; solberg, r."
34,"; volden, e."
34,automatic detection of oil spills in ers sar images.
34,ieee trans.
34,geosci.
34,remote sens.
34,"1999, 37, 1916–1924."
34,[crossref] 6.
34,"brusch, s."
34,"; lehner, s."
34,"; fritz, t."
34,"; soccorsi, m."
34,"; soloviev, a."
34,"; van schie, b."
34,ship surveillance with terrasar-x.
34,ieee trans.
34,geosci.
34,remote sens.
34,"2011, 49, 1092–1103."
34,[crossref]
35,"remote sens. 2019, 11, 2483 35 of 37 7."
35,"zhao, z."
35,"; ji, k."
35,"; xing, x."
35,"; zou, h."
35,"; zhou, s."
35,ship surveillance by integration of space-borne sar and ais—review of current research.
35,navig.
35,"2014, 67, 177–189."
35,[crossref] 8.
35,"vachon, p.w."
35,"; thomas, s.j."
35,"; cranton, j."
35,"; edel, h.r."
35,"; henschel, m.d."
35,validation of ship detection by the radarsat synthetic aperture radar and the ocean monitoring workstation.
35,remote sens.
35,"2000, 26, 200–212."
35,[crossref] 9.
35,"wackerman, c.c."
35,"; friedman, k.s."
35,"; pichel, w.g."
35,"; clemente-colón, p."
35,"; li, x."
35,automatic detection of ships in radarsat-1 sar imagery.
35,remote sens.
35,"2001, 27, 568–577."
35,[crossref] 10.
35,"zhang, t."
35,"; zhang, x."
35,high-speed ship detection in sar images based on a grid convolutional neural network.
35,remote sens.
35,"2019, 11, 1206."
35,[crossref] 11.
35,"zhou, d."
35,"; zeng, l."
35,"; zhang, k."
35,a novel sar target detection algorithm via multi-scale sift features.
35,northwest.
35,polytech.
35,"2015, 33, 867–873."
35,"schwegmann, c.p."
35,"; kleynhans, w."
35,"; salmon, b.p."
35,"ship detection in south african oceans using sar, cfar and a haar-like feature classifier."
35,"in proceedings of the 2014 ieee geoscience and remote sensing symposium, quebec city, qc, canada, 13–18 july 2014."
35,"xu, f."
35,"; liu, j.h."
35,ship detection and extraction using visual saliency and bar graph of oriented gradient.
35,optoelectron.
35,"2016, 12, 473–477."
35,[crossref] 14.
35,"raj, n."
35,"; sethunadh, r."
35,"; aparna, p.r."
35,object detection in sar image based on bandlet transform.
35,commun.
35,image represent.
35,"2016, 40, 376–383."
35,[crossref] 15.
35,"xu, x."
35,"; zheng, r."
35,"; chen, g."
35,"; blasch, e."
35,performance analysis of order statistic constant false alarm rate (cfar) detectors in generalized rayleigh environment.
35,"in proceedings of spie; the international society for optical engineering: bellingham, wa, usa, 2007."
35,"watts, s."
35,radar detection prediction in sea clutter using the compound k-distribution model.
35,ieee proc.
35,commun.
35,radar signal process.
35,"1985, 132, 613."
35,[crossref] 17.
35,"anastassopoulos, v."
35,"; lampropoulos, g.a."
35,optimal cfar detection in weibull clutter.
35,ieee trans.
35,aerosp.
35,electron.
35,"1995, 31, 52–64."
35,[crossref] 18.
35,"wang, c."
35,"; bi, f."
35,"; chen, l."
35,"; chen, j."
35,a novel threshold template algorithm for ship detection in high-resolution sar images.
35,"in proceedings of the 2016 ieee geoscience & remote sensing symposium, beijing, china, 10–15 july 2016."
35,"zhu, j."
35,"; qiu, x."
35,"; pan, z."
35,"; zhang, y."
35,"; lei, b."
35,projection shape template-based ship target recognition in terrasar-x images.
35,ieee geosci.
35,remote sens.
35,"2017, 14, 222–226."
35,[crossref] 20.
35,"lecun, y."
35,"; bengio, y."
35,"; hinton, g."
35,deep learning.
35,"nature 2015, 521, 436."
35,[crossref] 21.
35,"gui, y."
35,"; li, x."
35,"; xue, l."
35,a multilayer fusion light-head detector for sar ship detection.
35,"sensors 2019, 19, 1124."
35,[crossref] 22.
35,"girshick, r."
35,"; donahue, j."
35,"; darrell, t."
35,"; malik, j."
35,rich feature hierarchies for accurate object detection and semantic segmentation.
35,"in proceedings of the ieee conference on computer vision and pattern recognition, columbus, oh, usa, 23–28 june 2014; pp."
35,580–587.
35,"uijlings, j.r.r."
35,"; sande, k.e.a.v.d."
35,"; gevers, t."
35,"; smeulders, a.w.m."
35,selective search for object recognition.
35,comput.
35,"2013, 104, 154–171."
35,[crossref] 24.
35,"adankon, m.m."
35,"; cheriet, m."
35,support vector machine.
35,comput.
35,"2002, 1, 1–28."
35,"everingham, m."
35,"; eslami, s.a."
35,"; van gool, l."
35,"; williams, c.k."
35,"; winn, j."
35,"; zisserman, a."
35,the pascal visual object classes challenge: a retrospective.
35,comput.
35,"2015, 111, 98–136."
35,[crossref] 26.
35,"girshick, r."
35,fast r-cnn.
35,"in proceedings of the 2015 ieee international conference on computer vision (iccv), santiago, chile, 7–13 december 2015; pp."
35,1440–1448.
35,"he, k."
35,"; zhang, x."
35,"; ren, s."
35,"; sun, j."
35,spatial pyramid pooling in deep convolutional networks for visual recognition.
35,ieee trans.
35,pattern anal.
35,intell.
35,"2014, 37, 1904–1916."
35,[crossref] [pubmed] 28.
35,"ren, s."
35,"; he, k."
35,"; girshick, r."
35,"; sun, j."
35,faster r-cnn: towards real-time object detection with region proposal networks.
35,ieee trans.
35,pattern anal.
35,intell.
35,"2017, 39, 1137–1149."
35,[crossref] [pubmed] 29.
35,"wang, z."
35,"; liu, j."
35,a review of object detection based on convolutional neural network.
35,"in proceedings of the ieee 2017 36th chinese control conference (ccc), dalian, china, 26–28 july 2017."
35,"redmon, j."
35,"; divvala, s."
35,"; girshick, r."
35,"; farhadi, a."
35,"you only look once: unified, real-time object detection."
35,"in proceedings of the 2016 ieee conference on computer vision and pattern recognition (cvpr), las vegas, nv, usa, 27–30 june 2016."
36,"remote sens. 2019, 11, 2483 36 of 37 31."
36,"redmon, j."
36,"; farhadi, a."
36,"yolo9000: better, faster, stronger."
36,"in proceedings of the 2017 ieee conference on computer vision and pattern recognition (cvpr), honolulu, hi, usa, 21–26 july 2017; pp."
36,6517–6525.
36,"redmon, j."
36,"; farhadi, a."
36,"yolov3: an incremental improvement. arxiv 2018, arxiv:1804.02767."
36,"liu, w."
36,"; anguelov, d."
36,"; erhan, d."
36,"; szegedy, c."
36,"; reed, s.e."
36,"; fu, c.-y."
36,"; berg, a.c."
36,ssd: single shot multibox detector.
36,"in proceedings of the european conference on computer vision (eccv), amsterdam, the netherlands, 8–16 october 2016; pp."
36,21–37.
36,"lin, t.y."
36,"; goyal, p."
36,"; girshick, r."
36,"; he, k."
36,"; dollár, p."
36,focal loss for dense object detection.
36,ieee trans.
36,pattern anal.
36,intell.
36,"2017, 2999–3007."
36,[crossref] 35.
36,"li, j."
36,"; qu, c."
36,"; shao, j."
36,ship detection in sar images based on an improved faster r-cnn.
36,"in proceedings of the 2017 sar in big data era: models, methods and applications (bigsardata), beijing, china, 13–14 november 2017."
36,"lecun, y."
36,"; bottou, l."
36,"; bengio, y."
36,"; haffner, p."
36,gradient-based learning applied to document recognition.
36,"ieee 1998, 86, 2278–2324."
36,[crossref] 37.
36,"zhang, q.q."
36,"; liu, y."
36,"; pan, j.l."
36,"; yan, y.h."
36,continuous speech recognition by convolutional neural networks.
36,"2015, 5, 85–95."
36,"krizhevsky, a."
36,"; sutskever, i."
36,"; hinton, g."
36,imagenet classification with deep convolutional neural networks; nips (vol.
36,"25); curran associates inc.: new york, ny, usa, 2012."
36,"li, h."
36,deep learning for natural language processing: advantages and challenges.
36,"2018, 5, 24–26."
36,[crossref] 40.
36,"russakovsky, o."
36,"; deng, j."
36,"; su, h."
36,"; krause, j."
36,"; satheesh, s."
36,"; ma, s."
36,"; huang, z."
36,"; karpathy, a."
36,imagenet large scale visual recognition challenge.
36,comput.
36,"2015, 115, 211–252."
36,[crossref] 41.
36,"hubel, d.h."
36,"; wiesel, t.n."
36,receptive fields of single neurones in the cat’s striate cortex.
36,physiol.
36,"1959, 148, 574."
36,[crossref] 42.
36,"simonyan, k."
36,"; zisserman, a."
36,"very deep convolutional networks for large-scale image recognition. arxiv 2014, arxiv:1409.1556."
36,"szegedy, c."
36,"; liu, w."
36,"; jia, y."
36,"; sermanet, p."
36,"; reed, s."
36,"; anguelov, d."
36,"; erhan, d."
36,"; vanhoucke, v."
36,"; rabinovich, a."
36,going deeper with convolutions.
36,"in proceedings of the 2015 ieee conference on computer vision and pattern recognition (cvpr), boston, ma, usa, 7–12 june 2015."
36,"howard, a.g."
36,"; zhu, m."
36,"; chen, b."
36,"; kalenichenko, d."
36,"; wang, w."
36,"; weyand, t."
36,"; andreetto, m."
36,"; adam, h."
36,"mobilenets: efficient convolutional neural networks for mobile vision applications. arxiv 2017, arxiv:1704.04861."
36,"sifre, l."
36,rigid-motion scattering for image classification.
36,"thesis, ecole polytechnique, palaiseau, france, 2014."
36,"chollet, f."
36,"xception: deep learning with depthwise separable convolutions. arxiv 2016, arxiv:1610.02357v2."
36,"huang, g."
36,"; liu, z."
36,"; laurens, v.d.m."
36,"; weinberger, k.q."
36,"densely connected convolutional networks. arxiv 2016, arxiv:1608.06993."
36,"ioffe, s."
36,"; szegedy, c."
36,batch normalization: accelerating deep network training by reducing internal covariate shift.
36,"in proceedings of the international conference on international conference on machine learning, lille, france, 6–11 july 2015."
36,"xu, b."
36,"; wang, n."
36,"; chen, t."
36,"; li, m."
36,"empirical evaluation of rectified activations in convolutional network. arxiv 2018, arxiv:1505.00853."
36,"hosang, j."
36,"; benenson, r."
36,"; schiele, b."
36,learning non-maximum suppression.
36,"in proceedings of the 2017 ieee conference on computer vision and pattern recognition, honolulu, hi, usa, 21–26 july 2017; pp."
36,6469–6477.
36,pycharm.
36,available online: http://www.jetbrains.com/pycharm/ (accessed on 6 september 2019).
36,"manaswi, n.k."
36,understanding and working with keras.
36,"in deep learning with applications using python; apress: berkeley, ca, usa, 2018."
36,"abadi, m."
36,"; agarwal, a."
36,"; barham, p."
36,"; brevdo, e."
36,"; chen, z."
36,"; citro, c."
36,"; corrado, g.s."
36,"; davis, a."
36,"; dean, j."
36,"; devin, m.; et al."
36,tensorflow: large-scale machine learning on heterogeneous systems.
36,available online: https://arxiv.org/abs/1603.04467;https://www.tensorflow.org (accessed on 24 october 2019).
36,labelimg.
36,available online: https://github.com/tzutalin/labelimg. (accessed on 6 september 2019).
36,"cui, z."
36,"; li, q."
36,"; cao, z."
36,"; liu, n."
36,dense attention pyramid networks for multi-scale ship detection in sar images.
36,ieee trans.
36,geosci.
36,remote sens.
36,[crossref]
37,"remote sens. 2019, 11, 2483 37 of 37 56."
37,"jiao, j."
37,"; yue, z."
37,"; hao, s."
37,"; xue, y."
37,"; xun, g."
37,"; wen, h."
37,"; fu, k."
37,"; sun, x."
37,a densely connected end-to-end neural network for multiscale and multiscene sar ship detection.
37,"ieee access 2018, 6, 20881–20892."
37,[crossref] 57.
37,"liu, n."
37,"; cao, z."
37,"; cui, z."
37,"; pi, y."
37,"; dang, s."
37,multi-scale proposal generation for ship detection in sar images.
37,remote sens.
37,"2019, 11, 526."
37,[crossref] 58.
37,"chang, y.-l."
37,"; anagaw, a."
37,"; chang, l."
37,"; wang, y.c."
37,"; hsiao, c.-y."
37,"; lee, w.-h."
37,ship detection based on yolov2 for sar imagery.
37,remote sens.
37,"2019, 11, 786."
37,[crossref] 59.
37,"li, j."
37,"; qu, c."
37,"; peng, s."
37,"; jiang, y."
37,ship detection in sar images based on generative adversarial network and online hard examples mining.
37,electron.
37,technol.
37,"2019, 41, 143–149."
37,"li, j."
37,"; qu, c."
37,"; peng, s."
37,"; deng, b."
37,ship detection in sar images based on convolutional neural network.
37,electron.
37,"2018, 40, 1953–1959."
37,"wang, y."
37,"; chao, w."
37,"; hong, z."
37,combining a single shot multibox detector with transfer learning for ship detection using sentinel-1 sar images.
37,remote sens.
37,"2018, 9, 780–788."
37,[crossref] 62.
37,"wang, j."
37,"; lu, c."
37,"; jiang, w."
37,simultaneous ship detection and orientation estimation in sar images based on attention module and angle regression.
37,"sensors 2018, 18, 2851."
37,[crossref] [pubmed] 63.
37,"chen, c."
37,"; he, c."
37,"; hu, c."
37,"; pei, h."
37,"; jiao, l."
37,a deep neural network based on an attention mechanism for sar ship detection in multiscale and complex scenarios.
37,"ieee access 2019, 7, 104848–104863."
37,[crossref] 64.
37,"kingma, d."
37,"; ba, j."
37,"adam: a method for stochastic optimization. arxiv 2014, arxiv:1412.6980."
37,"hinton, g.e."
37,"; srivastava, n."
37,"; krizhevsky, a."
37,"; sutskever, i."
37,"; salakhutdinov, r.r."
37,"improving neural networks by preventing co-adaptation of feature detectors. arxiv 2012, arxiv:1207.0580."
37,"yuan, y."
37,"; rosasco, l."
37,"; caponnetto, a."
37,on early stopping in gradient descent learning.
37,constr.
37,approx.
37,"2007, 26, 289–315."
37,"he, k."
37,"; girshick, r."
37,"; dollár, p."
37,"rethinking imagenet pre-training. arxiv 2018, arxiv:1811.08883."
37,"deng, z."
37,"; sun, h."
37,"; zhou, s."
37,"; zhao, j."
37,learning deep ship detector in sar images from scratch.
37,ieee trans.
37,geosci.
37,remote sens.
37,"2019, 57, 4021–4039."
37,[crossref] 69.
37,"zeiler, m.d."
37,"; fergus, r."
37,"visualizing and understanding convolutional networks. arxiv 2013, arxiv:1311.2901v3."
37,opensar.
37,available online: http://opensar.sjtu.edu.cn/ (accessed on 6 september 2019).
37,"huang, l."
37,"; liu, b."
37,"; li, b."
37,"; guo, w."
37,"; yu, w."
37,opensarship: a dataset dedicated to sentinel-1 ship interpretation.
37,ieee j.
37,earth obs.
37,remote sens.
37,"2017, 11, 195–208."
37,[crossref] 72.
37,copernicus open access hub.
37,available online: https://scihub.copernicus.eu (accessed on 6 september 2019).
37,"de zan, f."
37,"; guarnieri, a.m."
37,topsar: terrain observation by progressive scans.
37,ieee trans.
37,geosci.
37,remote sens.
37,"2006, 44, 2352–2360."
37,[crossref] © 2019 by the authors.
37,"licensee mdpi, basel, switzerland."
37,this article is an open access article distributed under the terms and conditions of the creative commons attribution (cc by) license (http://creativecommons.org/licenses/by/4.0/).
