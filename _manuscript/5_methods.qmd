Our analysis of the literature consisted of five major phases (described in detail below): 1) developing a search string; 2) applying the search string to academic databases to acquire a set of citations; 3) screening citations by the title and abstract; 4) screening the full text of papers that passed the title and abstract screening stage using natural language processing and language models; and 5) coding the papers to identify ESI data source, valuation method, societal benefit area, and value domain.

To develop a search string (see SI Methods), we focused on three key domains: 1) application of ESI, 2) a decision context or analysis framework in which the ESI is applied, and 3) an expected or observed change in societal benefits based on decision outcome due to use of ESI. The research team collected (via Google Scholar searches) and solicited (via professional networks) a preliminary set of 72 candidate documents, which were screened based on these three domains. Of these 72 candidate documents, 14 were identified as a benchmark set that the research team felt exemplified valuation of ESI. From this benchmark set, we developed a preliminary search string combining the three domains: ESI (e.g., "remote sensing", "satellite", "Sentinel", "Landsat"), decision context (e.g., "management", "policy", "cost-benefit", "contingent valuation"), and societal benefit (e.g., "value", "benefit", or "utility" combined with terms such as "societal", "cultural", "environmental", "ecosystem service", or terms related to GEOSS societal benefit areas). The preliminary set of terms was used to collect citations (title, abstract, authors, metadata) from Web of Science (n = 1,158). We applied the functionality of the `litsearchr` package in R (REF) to this preliminary citation set, using text mining and keyword co-occurrence networks to identify additional terms to increase the inclusion of our search string [@grame_automatedapproach_2019]. The final search string (see SI XXX) was used to collect citations from Web of Science (January 26, 2024, n = 9,488) and Scopus (February 4, 2024, n = 18,585), including all 14 benchmark papers. In addition to these two citation sets we included a curated set of citations from the USGS Joint Societal Benefits of Earth Observation Digital Library (!!!REF USGS 2024) (SBL, n = 258). See Fig. S2 for PRISMA flow diagram.

The results of the search (Web of Science and Scopus) were then cleaned. Citations noted as conference abstracts or proceedings (n = 1,030 and n = 4,109 respectively) were dropped. Then, citations with missing title, author, abstract, or digital object identifier (DOI) field (n = 319 and n = 1,226 respectively) were dropped. After resolving minor differences among titles, author names, and DOI fields, 6,840 duplicate citations were removed from the combined citation set. The resulting set of 14,807 distinct citations were subjected to a preliminary screening to remove known spurious matches (n = 984), leaving n = 13,823 citations for screening and analysis (Fig. 1).

Screening was performed in two stages, to enable implementation of a classification algorithm to more efficiently screen papers (Fig. 2). In the first stage, the citations from the SBL and a random sample of ~1000 citations from the Web of Science/Scopus corpus were subjected to title/abstract screening, and then full-text screening on the title/abstract "include" papers, based on a set of inclusion criteria (See Table XXX for screening criteria). All title/abstract screening was performed using the Colandr web-based screening application [@cheng_usingmachine_2018], which uses machine learning and natural language processing to continually predict and sort citations in order of predicted relevance based on user screening decisions.

The resulting set from this first stage was then used to train a classification model based on the XLNet generalized autoregressive pretraining algorithm, which considers all permutations of dependencies between sets of words in the citation titles and abstracts to "understand" the context [@yang_xlnetgeneralized_2019], to classify citations in the remainder of the corpus as either "include" or "exclude". The predicted "include" citations were then title/abstract screened (using Colandr) and those that passed were full-text screened. The include/exclude classification model showed a low false negative rate (1.2%, sensitivity 92.3%) on the training data, but to ensure this held true of the larger document set, a random sample of 1000 predicted "excludes" was uploaded to Colandr. After screening 200 of these documents and finding no relevant matches to our screening criteria despite Colandr’s ability to prioritize relevant articles, this screening phase was stopped early. While the classifier’s false positive rate was higher (27.1%, specificity 71.2%), these false positives were subject to title/abstract screening so were not a concern. Of the 13,823 unique citations retrieved from Scopus, Web of Science, and the SBL, our screening process resulted in only n = 170 documents that met all screening criteria for inclusion (see Table XXX) in our corpus, for a final inclusion rate of 1.2%.


Documents included in the final corpus were screened and coded to identify valuation methods, societal benefit areas, and value types according to <XXX typology in SI, or tables here in main text?>.
